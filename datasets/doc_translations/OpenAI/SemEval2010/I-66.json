{
    "id": "I-66",
    "original_text": "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains. Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions. Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality. A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost. This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time. Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms. Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1. INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13]. The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable. Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete. Researchers have attempted two different types of approaches towards solving these models. The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11]. The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution. In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10]. Though these approaches obtain optimal solutions, they typically consider only two agents. Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents. To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents. We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm. There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree. We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution. The first enhancement uses abstractions for speedup, but does not sacrifice solution quality. In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies. The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution. The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal. We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments. In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents. Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions). We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work. This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10]. Figure 1 shows a specific problem instance within this type consisting of three sensors. Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1). To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously. In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously. Thus, sensors have to act in a coordinated fashion. We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents. Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives. The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions. Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off. Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models. Figure 1: A 3-chain sensor configuration 3. BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2. It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states. Si refers to the set of local states of agent i and Su is the set of unaffectable states. Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control. A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state. Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state. The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|. Based on the reward function, an interaction hypergraph is constructed. A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges. The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively. The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b. An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi. The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10]. We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents. GOA borrows from a global optimal DCOP algorithm called DPOP[12]. GOAs message passing follows that of DPOP. The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root. Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy. Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies. This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies. In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves. GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree). Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4. SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward. The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies. The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents. Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure. Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children. We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed. MCN heuristic tries to place agents with more number of constraints at the top of the tree. This tree governs how the search for the optimal joint polThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER. The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees. SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs. In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i). Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child. Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies. Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain. Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree. SPIDER exploits the structure of this DFS tree while engaging in its search. Note that in our example figure, each agent is assigned a policy with T=2. Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy. Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle. If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided. SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree). Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies. Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents). The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1. Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies. An MDP based heuristic is used to compute these upper bounds on the expected values. Detailed description about this MDP heuristic is provided in Section 4.2. All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order. Exploration of these policies (in step 2 below) are performed in this descending order. As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy. The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2. Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− . This is performed by iterating through all policies of agent i i.e. Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors. Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ]. The policy with the highest expected value is the best response policy. Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ]. Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold. A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold. This is because the expected value for the best joint policy attainable for that policy will be less than the threshold. On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− . This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy. In Figure 2, SPIDER assigns best response policies to leaf agents at level 3. The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step. These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy. Algorithm 1 provides the pseudo code for SPIDER. This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i). Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i). This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23. Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies. Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children. Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children. The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i). The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP. To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] . We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i). Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together. We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption. This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)). Thus, the equation for the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above. While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended. We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality). The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies. In this paper, we propose two types of abstraction: 1. Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy. It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy. In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step. For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy. This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon. This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action. Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2. Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree. Unlike in HBA, this implies multiple levels of abstraction. This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP. These incomplete T=2 policies are abstractions for T=2 complete policies. Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases. For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes. In such cases, the immediate reward is taken as Rmax (maximum reward for any action). We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS. Algorithm 5 provides the algorithm for this abstraction technique. For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17). The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24). Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30). However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32). EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them. If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1. Otherwise, πi.absNodes is increased by 1. Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes. Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33). Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX. The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality. This approximation parameter is used at each agent for pruning out joint policies. The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value. However, the 826 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ]. Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS. In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs. However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned. This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238). It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance. However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution. A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX. Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired. Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality. A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ]. Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition. Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234). This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution. Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1. Heuristic provided using the centralized MDP heuristic is admissible. Proof. For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy. Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this. Base case: t = T − 1. Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents. Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l . Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1. We now have to prove that the proposition holds for t = η − 1. We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ . The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved. PROPOSITION 2. SPIDER provides an optimal solution. Proof. SPIDER examines all possible joint policies given the interaction structure of the agents. The only exception being when a joint policy is pruned based on the heuristic value. Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy. As proved in Proposition 1, the expected value for a joint policy is always an upper bound. Hence when a joint policy is pruned, it cannot be an optimal solution. PROPOSITION 3. Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof. We prove this proposition using mathematical induction on the depth of the DFS tree. Base case: depth = 1 (i.e. one node). Best response is computed by iterating through all policies, Πk. A policy,πk is pruned if ˆv[πk, πk− ] < threshold + . Thus the best response policy computed by VAX would be at most away from the optimal best response. Hence the proposition holds for the base case. Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1. Without loss of generality, lets assume that the root node of this tree has k children. Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root. Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree. In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ]. However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk . Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ . Hence proved. PROPOSITION 4. For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality. Proof. We prove this proposition using mathematical induction on the depth of the DFS tree. Base case: depth = 1 (i.e. one node). Best response is computed by iterating through all policies, Πk. A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold. Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response. Hence the proposition holds for the base case. Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1. Without loss of generality, lets assume that the root node of this tree has k children. Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX. With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children. Thus, overall solution quality is at least δ 100 of the optimal solution. Hence proved. 5. EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2. The five network configurations employed are shown in Figure 4. Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX. We compare against GOA because it is the only global optimal algorithm that considers more than two agents. We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality. Experiments were terminated after 10000 seconds1 . Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80). X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale). The time horizon of policy computation was 3. For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX. GOA did not terminate within the time limit for 4-star and 5-star configurations. SPIDER-Abs dominated the SPIDER and GOA for all the configurations. For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER. The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs. For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs. Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a). X-axis denotes the sensor network configuration while Y-axis indicates the solution quality. Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms. For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality. With both the approximations, we obtained a solution quality that was close to the optimal solution quality. In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ. For other configurations as well, the loss in quality was less than 20% of the optimal solution quality. Figure 5(c) provides the time to solution with PAX (for varying epsilons). X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale). The time horizon for all the configurations was 4. As δ was decreased from 70 to 30, the time to solution decreased drastically. For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30. Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%. Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons). X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale). The time horizon for all the configurations was 4. As was increased, the time to solution decreased drastically. For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140. Again, the actual solution quality did not change with varying epsilon. Figure 4: Sensor network configurations 828 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6. SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX. These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty. Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms. Researchers have typically employed two types of techniques for solving distributed POMDPs. The first set of techniques compute global optimal solutions. Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs. Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs. This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory. The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents. However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents). The second set of techniques seek approximate policies. EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic). Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs. Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies. Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution. This aspect of quality bounds differentiates SPIDER from all the above techniques. Acknowledgements. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No. NBCHD030010. The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7. REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman. Solving transition independent decentralized Markov decision processes. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen, and S. Zilberstein. Bounded policy iteration for decentralized POMDPs. In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman. The complexity of decentralized control of MDPs. In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun. Approximate solutions for partially observable stochastic games with common payoffs. In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein. Dynamic programming for partially observable stochastic games. In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe. Distributed sensor nets: A multiagent perspective. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham. Taking dcop to the real world : Efficient complete solutions for distributed event scheduling. In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo. An asynchronous complete method for distributed constraint optimization. In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella. Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings. In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo. Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs. In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling. Learning to cooperate via policy search. In UAI, 2000. [12] A. Petcu and B. Faltings. A scalable method for multiagent constraint optimization. In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein. MAA*: A heuristic search algorithm for solving decentralized POMDPs. In IJCAI, 2005. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829",
    "original_translation": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829",
    "original_sentences": [
        "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
        "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
        "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
        "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
        "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
        "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
        "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
        "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
        "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
        "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
        "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
        "Researchers have attempted two different types of approaches towards solving these models.",
        "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
        "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
        "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
        "Though these approaches obtain optimal solutions, they typically consider only two agents.",
        "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
        "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
        "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
        "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
        "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
        "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
        "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
        "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
        "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
        "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
        "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
        "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
        "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
        "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
        "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
        "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
        "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
        "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
        "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
        "Thus, sensors have to act in a coordinated fashion.",
        "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
        "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
        "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
        "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
        "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
        "Figure 1: A 3-chain sensor configuration 3.",
        "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
        "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
        "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
        "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
        "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
        "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
        "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
        "Based on the reward function, an interaction hypergraph is constructed.",
        "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
        "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
        "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
        "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
        "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
        "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
        "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
        "GOAs message passing follows that of DPOP.",
        "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
        "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
        "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
        "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
        "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
        "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
        "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
        "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
        "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
        "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
        "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
        "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
        "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
        "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
        "This tree governs how the search for the optimal joint polThe Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
        "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
        "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
        "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
        "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
        "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
        "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
        "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
        "SPIDER exploits the structure of this DFS tree while engaging in its search.",
        "Note that in our example figure, each agent is assigned a policy with T=2.",
        "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
        "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
        "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
        "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
        "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
        "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
        "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
        "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
        "An MDP based heuristic is used to compute these upper bounds on the expected values.",
        "Detailed description about this MDP heuristic is provided in Section 4.2.",
        "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
        "Exploration of these policies (in step 2 below) are performed in this descending order.",
        "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
        "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
        "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
        "This is performed by iterating through all policies of agent i i.e.",
        "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
        "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
        "The policy with the highest expected value is the best response policy.",
        "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
        "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
        "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
        "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
        "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
        "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
        "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
        "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
        "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
        "Algorithm 1 provides the pseudo code for SPIDER.",
        "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
        "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
        "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
        "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
        "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
        "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
        "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
        "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
        "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
        "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
        "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
        "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
        "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
        "Thus, the equation for the The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
        "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
        "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
        "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
        "In this paper, we propose two types of abstraction: 1.",
        "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
        "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
        "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
        "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
        "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
        "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
        "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
        "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
        "Unlike in HBA, this implies multiple levels of abstraction.",
        "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
        "These incomplete T=2 policies are abstractions for T=2 complete policies.",
        "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
        "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
        "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
        "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
        "Algorithm 5 provides the algorithm for this abstraction technique.",
        "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
        "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
        "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
        "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
        "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
        "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
        "Otherwise, πi.absNodes is increased by 1.",
        "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
        "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
        "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
        "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
        "This approximation parameter is used at each agent for pruning out joint policies.",
        "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
        "However, the 826 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
        "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
        "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
        "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
        "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
        "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
        "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
        "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
        "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
        "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
        "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
        "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
        "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
        "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
        "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
        "Heuristic provided using the centralized MDP heuristic is admissible.",
        "Proof.",
        "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
        "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
        "Base case: t = T − 1.",
        "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
        "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
        "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
        "We now have to prove that the proposition holds for t = η − 1.",
        "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
        "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
        "PROPOSITION 2.",
        "SPIDER provides an optimal solution.",
        "Proof.",
        "SPIDER examines all possible joint policies given the interaction structure of the agents.",
        "The only exception being when a joint policy is pruned based on the heuristic value.",
        "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
        "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
        "Hence when a joint policy is pruned, it cannot be an optimal solution.",
        "PROPOSITION 3.",
        "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
        "We prove this proposition using mathematical induction on the depth of the DFS tree.",
        "Base case: depth = 1 (i.e. one node).",
        "Best response is computed by iterating through all policies, Πk.",
        "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
        "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
        "Hence the proposition holds for the base case.",
        "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
        "Without loss of generality, lets assume that the root node of this tree has k children.",
        "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
        "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
        "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
        "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
        "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
        "Hence proved.",
        "PROPOSITION 4.",
        "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
        "Proof.",
        "We prove this proposition using mathematical induction on the depth of the DFS tree.",
        "Base case: depth = 1 (i.e. one node).",
        "Best response is computed by iterating through all policies, Πk.",
        "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
        "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
        "Hence the proposition holds for the base case.",
        "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
        "Without loss of generality, lets assume that the root node of this tree has k children.",
        "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
        "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
        "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
        "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
        "Thus, overall solution quality is at least δ 100 of the optimal solution.",
        "Hence proved. 5.",
        "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
        "The five network configurations employed are shown in Figure 4.",
        "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
        "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
        "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
        "Experiments were terminated after 10000 seconds1 .",
        "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
        "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
        "The time horizon of policy computation was 3.",
        "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
        "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
        "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
        "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
        "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
        "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
        "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
        "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
        "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
        "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
        "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
        "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
        "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
        "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
        "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
        "The time horizon for all the configurations was 4.",
        "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
        "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
        "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
        "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
        "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
        "The time horizon for all the configurations was 4.",
        "As was increased, the time to solution decreased drastically.",
        "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
        "Again, the actual solution quality did not change with varying epsilon.",
        "Figure 4: Sensor network configurations 828 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
        "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
        "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
        "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
        "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
        "The first set of techniques compute global optimal solutions.",
        "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
        "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
        "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
        "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
        "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
        "The second set of techniques seek approximate policies.",
        "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
        "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
        "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
        "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
        "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
        "Acknowledgements.",
        "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
        "NBCHD030010.",
        "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
        "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
        "Solving transition independent decentralized Markov decision processes.",
        "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
        "Hansen, and S. Zilberstein.",
        "Bounded policy iteration for decentralized POMDPs.",
        "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
        "The complexity of decentralized control of MDPs.",
        "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
        "Approximate solutions for partially observable stochastic games with common payoffs.",
        "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
        "Dynamic programming for partially observable stochastic games.",
        "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
        "Distributed sensor nets: A multiagent perspective.",
        "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
        "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
        "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
        "An asynchronous complete method for distributed constraint optimization.",
        "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
        "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
        "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
        "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
        "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
        "Learning to cooperate via policy search.",
        "In UAI, 2000. [12] A. Petcu and B. Faltings.",
        "A scalable method for multiagent constraint optimization.",
        "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
        "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
        "In IJCAI, 2005.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
    ],
    "translated_text_sentences": [
        "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
        "Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos.",
        "Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas.",
        "Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución.",
        "Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional.",
        "Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo.",
        "Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores.",
        "Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1.",
        "Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13].",
        "La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable.",
        "Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo.",
        "Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos.",
        "La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11].",
        "El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución.",
        "Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10].",
        "Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes.",
        "Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes.",
        "Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes.",
        "Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos).",
        "Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS.",
        "Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución.",
        "La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución.",
        "En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas.",
        "La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima.",
        "La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo.",
        "Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos.",
        "En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes.",
        "Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas).",
        "Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
        "DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo.",
        "Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10].",
        "La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores.",
        "Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1).",
        "Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente.",
        "En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente.",
        "Por lo tanto, los sensores deben actuar de manera coordinada.",
        "Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor.",
        "Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos.",
        "Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2.",
        "Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga.",
        "Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP.",
        "Figura 1: Una configuración de sensor de 3 cadenas 3.",
        "ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2.",
        "Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo.",
        "Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables.",
        "El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar.",
        "A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante.",
        "Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante.",
        "La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|.",
        "Basándose en la función de recompensa, se construye un hipercubo de interacción.",
        "Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas.",
        "El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente.",
        "El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b.",
        "Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi.",
        "El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10].",
        "Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes.",
        "GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12].",
        "El paso de mensajes de GOA sigue el de DPOP.",
        "La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz.",
        "El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal.",
        "Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas.",
        "Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas.",
        "En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas.",
        "GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol).",
        "Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4.",
        "Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general.",
        "La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas.",
        "La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes.",
        "A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada.",
        "Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos.",
        "Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]).",
        "La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol.",
        "Este árbol controla cómo se busca el polinomio conjunto óptimo.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER.",
        "Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios.",
        "SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos.",
        "En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i).",
        "Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo.",
        "Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas.",
        "La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes.",
        "Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol.",
        "SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda.",
        "Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2.",
        "Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política.",
        "El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado.",
        "Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta.",
        "SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda).",
        "El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz.",
        "El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes).",
        "El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1.",
        "Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas.",
        "Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados.",
        "Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2.",
        "Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente.",
        "La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente.",
        "Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta.",
        "La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos.",
        "Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−.",
        "Esto se realiza iterando a través de todas las políticas del agente i, es decir,",
        "Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros.",
        "Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−].",
        "La política con el valor esperado más alto es la mejor política de respuesta.",
        "La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ].",
        "De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral.",
        "Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral.",
        "Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral.",
        "Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−.",
        "Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto.",
        "En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3.",
        "La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo.",
        "Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa.",
        "El Algoritmo 1 proporciona el pseudocódigo para SPIDER.",
        "Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i).",
        "Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i).",
        "El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23.",
        "El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas.",
        "El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos.",
        "La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos.",
        "La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i).",
        "El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado.",
        "Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−].",
        "Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i).",
        "Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos.",
        "Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad.",
        "Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)).",
        "Por lo tanto, la ecuación para el Sexto Congreso Internacional.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente.",
        "Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas.",
        "Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución).",
        "Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas.",
        "En este documento, proponemos dos tipos de abstracción: 1.",
        "Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto.",
        "Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta.",
        "En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo.",
        "Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta.",
        "Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal.",
        "La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta.",
        "La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2.",
        "Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas.",
        "A diferencia de en HBA, esto implica múltiples niveles de abstracción.",
        "Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP.",
        "Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2.",
        "Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda.",
        "Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política.",
        "En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción).",
        "Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS.",
        "El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción.",
        "Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17).",
        "El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24).",
        "La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30).",
        "Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32).",
        "La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada.",
        "Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1.",
        "De lo contrario, πi.absNodes se incrementa en 1.",
        "Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes.",
        "Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33).",
        "Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX.",
        "La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima.",
        "Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas.",
        "El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico.",
        "Sin embargo, el 826 La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ].",
        "Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS.",
        "En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs.",
        "Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada.",
        "Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238).",
        "Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución.",
        "Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata.",
        "Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX.",
        "El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada.",
        "El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima.",
        "Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ].",
        "Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda.",
        "Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234).",
        "Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución.",
        "La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1.",
        "La heurística proporcionada utilizando la heurística MDP centralizada es admisible.",
        "Prueba.",
        "Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta.",
        "Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto.",
        "Caso base: t = T − 1.",
        "Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes.",
        "Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l.",
        "Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1.",
        "Ahora tenemos que demostrar que la proposición se cumple para t = η - 1.",
        "Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ .",
        "La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra.",
        "PROPOSICIÓN 2.",
        "SPIDER proporciona una solución óptima.",
        "Prueba.",
        "SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes.",
        "La única excepción es cuando una política conjunta es podada basándose en el valor heurístico.",
        "Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima.",
        "Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior.",
        "Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima.",
        "PROPOSICIÓN 3.",
        "El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba.",
        "Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS.",
        "Caso base: profundidad = 1 (es decir, un nodo).",
        "La mejor respuesta se calcula iterando a través de todas las políticas, Πk.",
        "Una política, πk, se poda si ˆv[πk, πk− ] < umbral + .",
        "Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima.",
        "Por lo tanto, la proposición se cumple para el caso base.",
        "Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1.",
        "Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos.",
        "Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz.",
        "Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol.",
        "En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ].",
        "Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk.",
        "Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ.",
        "Por lo tanto, queda demostrado.",
        "PROPOSICIÓN 4.",
        "Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución.",
        "Prueba.",
        "Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS.",
        "Caso base: profundidad = 1 (es decir, un nodo).",
        "La mejor respuesta se calcula iterando a través de todas las políticas, Πk.",
        "Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral.",
        "Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima.",
        "Por lo tanto, la proposición se cumple para el caso base.",
        "Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1.",
        "Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos.",
        "Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX.",
        "Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
        "Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
        "Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos.",
        "Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima.",
        "Por lo tanto, demostrado. 5.",
        "RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2.",
        "Las cinco configuraciones de red empleadas se muestran en la Figura 4.",
        "Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX.",
        "Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes.",
        "Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución.",
        "Los experimentos fueron terminados después de 10000 segundos.",
        "La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80).",
        "El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica).",
        "El horizonte temporal de cálculo de políticas fue de 3.",
        "Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX.",
        "GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas.",
        "SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones.",
        "Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER.",
        "Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs.",
        "Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs.",
        "La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a).",
        "El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución.",
        "Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos.",
        "Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima.",
        "Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima.",
        "En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ.",
        "Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima.",
        "La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon).",
        "El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica).",
        "El horizonte temporal para todas las configuraciones fue de 4.",
        "A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente.",
        "Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30.",
        "Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%.",
        "La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon).",
        "El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica).",
        "El horizonte temporal para todas las configuraciones fue de 4.",
        "A medida que se aumentó, el tiempo de solución disminuyó drásticamente.",
        "Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140.",
        "Nuevamente, la calidad de la solución real no cambió al variar epsilon.",
        "Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6.",
        "RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX.",
        "Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre.",
        "Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores.",
        "Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos.",
        "El primer conjunto de técnicas calcula soluciones óptimas globales.",
        "Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos.",
        "Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados.",
        "Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado.",
        "Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes.",
        "Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes).",
        "El segundo conjunto de técnicas busca políticas aproximadas.",
        "Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada).",
        "El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito.",
        "Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente.",
        "Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución.",
        "Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores.",
        "Agradecimientos.",
        "Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No.",
        "NBCHD030010.",
        "Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos.",
        "REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman.",
        "Resolviendo procesos de decisión de Markov descentralizados independientes de la transición.",
        "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
        "Hansen y S. Zilberstein.",
        "Iteración de política acotada para POMDP descentralizados.",
        "En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman.",
        "La complejidad del control descentralizado de MDPs.",
        "En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun.",
        "Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes.",
        "En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein.",
        "Programación dinámica para juegos estocásticos parcialmente observables.",
        "En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe.",
        "Redes de sensores distribuidos: Una perspectiva multiagente.",
        "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham.",
        "Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos.",
        "En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo.",
        "Un método completo asíncrono para la optimización de restricciones distribuidas.",
        "En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella.",
        "Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes.",
        "En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo.",
        "POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs.",
        "En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling.",
        "Aprendiendo a cooperar a través de la búsqueda de políticas.",
        "En UAI, 2000. [12] A. Petcu y B. Faltings.",
        "Un método escalable para la optimización de restricciones multiagente.",
        "En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein.",
        "MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados.",
        "En IJCAI, 2005.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829"
    ],
    "error_count": 9,
    "keys": {
        "network": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a <br>network</br> of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a <br>network</br> of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a <br>network</br> of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits <br>network</br> structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor <br>network</br> domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: <br>network</br> Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor <br>network</br> domain from Section 2.",
                "The five <br>network</br> configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor <br>network</br> configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor <br>network</br> configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor <br>network</br> configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a <br>network</br> of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "Letting loose a SPIDER on a <br>network</br> of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a <br>network</br> of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a <br>network</br> of more than two agents.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits <br>network</br> structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We experimented with the sensor <br>network</br> domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments."
            ],
            "translated_annotated_samples": [
                "Liberando una ARAÑA en una <br>red</br> de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una <br>red</br> de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo.",
                "Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una <br>red</br> de más de dos agentes.",
                "Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la <br>estructura de red</br> de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS.",
                "Experimentamos con el dominio de <br>redes</br> de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con <br>redes</br> de agentes que trabajan en entornos inciertos."
            ],
            "translated_text": "Liberando una ARAÑA en una <br>red</br> de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una <br>red</br> de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una <br>red</br> de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la <br>estructura de red</br> de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de <br>redes</br> de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con <br>redes</br> de agentes que trabajan en entornos inciertos. ",
            "candidates": [],
            "error": [
                [
                    "red",
                    "red",
                    "red",
                    "estructura de red",
                    "redes",
                    "redes"
                ]
            ]
        },
        "quality guaranteed policies": {
            "translated_key": "políticas de calidad garantizada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating <br>quality guaranteed policies</br> Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "Letting loose a SPIDER on a network of POMDPs: Generating <br>quality guaranteed policies</br> Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept."
            ],
            "translated_annotated_samples": [
                "Liberando una ARAÑA en una red de POMDPs: Generando <br>políticas de calidad garantizada</br>. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando <br>políticas de calidad garantizada</br>. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "distributed partially observable markov decision problem": {
            "translated_key": "problema de decisión de Markov parcialmente observable distribuido",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "multi-agent system": {
            "translated_key": "sistemas multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling <br>multi-agent system</br>s acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling <br>multi-agent system</br>s acting in uncertain domains."
            ],
            "translated_annotated_samples": [
                "Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar <br>sistemas multiagente</br> que actúan en dominios inciertos."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar <br>sistemas multiagente</br> que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "uncertain domain": {
            "translated_key": "dominios inciertos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in <br>uncertain domain</br>s.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in <br>uncertain domain</br>s."
            ],
            "translated_annotated_samples": [
                "Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en <br>dominios inciertos</br>."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en <br>dominios inciertos</br>. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "approximate solution": {
            "translated_key": "soluciones aproximadas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on <br>approximate solution</br>s.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on <br>approximate solution</br>s."
            ],
            "translated_annotated_samples": [
                "Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en <br>soluciones aproximadas</br>."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en <br>soluciones aproximadas</br>. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "global optimality": {
            "translated_key": "optimalidad global",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on <br>global optimality</br>, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "A second less popular approach focuses on <br>global optimality</br>, but typical results are available only for two agents, and also at considerable computational cost."
            ],
            "translated_annotated_samples": [
                "Un segundo enfoque menos popular se centra en la <br>optimalidad global</br>, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la <br>optimalidad global</br>, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "agent interaction structure": {
            "translated_key": "estructura de interacción de agentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits <br>agent interaction structure</br> given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting <br>agent interaction structure</br> given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits <br>agent interaction structure</br> given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting <br>agent interaction structure</br> given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX."
            ],
            "translated_annotated_samples": [
                "Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la <br>estructura de interacción de agentes</br> dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo.",
                "RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la <br>estructura de interacción de agentes</br> dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la <br>estructura de interacción de agentes</br> dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la <br>estructura de interacción de agentes</br> dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "network of agent": {
            "translated_key": "red de agentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a <br>network of agent</br>s (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a <br>network of agent</br>s (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a <br>network of agent</br>s (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a <br>network of agent</br>s (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX."
            ],
            "translated_annotated_samples": [
                "Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una <br>red de agentes</br> (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo.",
                "RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una <br>red de agentes</br> (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una <br>red de agentes</br> (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una <br>red de agentes</br> (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "agent network": {
            "translated_key": "red de agentes",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "policy search": {
            "translated_key": "búsqueda de políticas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup <br>policy search</br>; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for <br>policy search</br> in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of <br>policy search</br> techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via <br>policy search</br>.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup <br>policy search</br>; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for <br>policy search</br> in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of <br>policy search</br> techniques that search for locally optimal policies.",
                "Learning to cooperate via <br>policy search</br>."
            ],
            "translated_annotated_samples": [
                "Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la <br>búsqueda de políticas</br>; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo.",
                "RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la <br>búsqueda de políticas</br> en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX.",
                "Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de <br>búsqueda de políticas</br> que buscan políticas óptimas localmente.",
                "Aprendiendo a cooperar a través de la <br>búsqueda de políticas</br>."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la <br>búsqueda de políticas</br>; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la <br>búsqueda de políticas</br> en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de <br>búsqueda de políticas</br> que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la <br>búsqueda de políticas</br>. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "heuristic": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound <br>heuristic</br> search technique that uses a MDP-based <br>heuristic</br> function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) <br>heuristic</br> used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP <br>heuristic</br> from [7]) can also be employed.",
                "MCN <br>heuristic</br> tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "<br>heuristic</br> or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based <br>heuristic</br> is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP <br>heuristic</br> is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as <br>heuristic</br> values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the <br>heuristic</br> values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on <br>heuristic</br> values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the <br>heuristic</br> value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based <br>heuristic</br> function The <br>heuristic</br> function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/<br>heuristic</br> values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-<br>heuristic</br> (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-<br>heuristic</br> (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on <br>heuristic</br> computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how <br>heuristic</br> values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to <br>heuristic</br> computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the <br>heuristic</br> computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-<br>heuristic</br>()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the <br>heuristic</br> value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter <br>heuristic</br> computation and exploration, pruning phases.",
                "For NBA, the <br>heuristic</br> computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy <br>heuristic</br> computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of <br>heuristic</br> values and ones that have <br>heuristic</br> values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on <br>heuristic</br> values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the <br>heuristic</br> value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the <br>heuristic</br> value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the <br>heuristic</br> value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the <br>heuristic</br> value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "<br>heuristic</br> provided using the centralized MDP <br>heuristic</br> is admissible.",
                "Proof.",
                "For the value provided by the <br>heuristic</br> to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The <br>heuristic</br> value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the <br>heuristic</br> value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based <br>heuristic</br> function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal <br>heuristic</br> search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical <br>heuristic</br> search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected <br>heuristic</br>).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A <br>heuristic</br> search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "There are two key novel features in SPIDER: (i) it is a branch and bound <br>heuristic</br> search technique that uses a MDP-based <br>heuristic</br> function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We employ the Maximum Constrained Node (MCN) <br>heuristic</br> used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP <br>heuristic</br> from [7]) can also be employed.",
                "MCN <br>heuristic</br> tries to place agents with more number of constraints at the top of the tree.",
                "<br>heuristic</br> or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "An MDP based <br>heuristic</br> is used to compute these upper bounds on the expected values."
            ],
            "translated_annotated_samples": [
                "Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función <br>heurística basada en MDP</br> para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS.",
                "Empleamos la <br>heurística</br> de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras <br>heurística</br>s (como la heurística MLSP de [7]).",
                "La <br>heurística</br> MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol.",
                "El valor esperado <br>heurístico</br> o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado.",
                "Se utiliza una <br>heurística</br> basada en MDP para calcular estos límites superiores en los valores esperados."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función <br>heurística basada en MDP</br> para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la <br>heurística</br> de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras <br>heurística</br>s (como la heurística MLSP de [7]). La <br>heurística</br> MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado <br>heurístico</br> o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una <br>heurística</br> basada en MDP para calcular estos límites superiores en los valores esperados. ",
            "candidates": [],
            "error": [
                [
                    "heurística basada en MDP",
                    "heurística",
                    "heurística",
                    "heurística",
                    "heurístico",
                    "heurística"
                ]
            ]
        },
        "quality guaranteed approximation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows <br>quality guaranteed approximation</br>s, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for <br>quality guaranteed approximation</br>s, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows <br>quality guaranteed approximation</br>s, allowing a systematic tradeoff of solution quality for time.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for <br>quality guaranteed approximation</br>s, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents."
            ],
            "translated_annotated_samples": [
                "Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite <br>aproximaciones garantizadas de calidad</br>, permitiendo un intercambio sistemático de calidad de solución por tiempo.",
                "Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten <br>aproximaciones de calidad garantizada</br>, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite <br>aproximaciones garantizadas de calidad</br>, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten <br>aproximaciones de calidad garantizada</br>, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    "aproximaciones garantizadas de calidad",
                    "aproximaciones de calidad garantizada"
                ]
            ]
        },
        "branch and bound heuristic search technique": {
            "translated_key": "búsqueda heurística de ramificación y acotación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a <br>branch and bound heuristic search technique</br> that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "There are two key novel features in SPIDER: (i) it is a <br>branch and bound heuristic search technique</br> that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree."
            ],
            "translated_annotated_samples": [
                "Hay dos características novedosas clave en SPIDER: (i) es una técnica de <br>búsqueda heurística de ramificación y acotación</br> que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de <br>búsqueda heurística de ramificación y acotación</br> que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "heuristic function": {
            "translated_key": "función heurística",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based <br>heuristic function</br> to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based <br>heuristic function</br> The <br>heuristic function</br> quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based <br>heuristic function</br>; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based <br>heuristic function</br> to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based <br>heuristic function</br> The <br>heuristic function</br> quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based <br>heuristic function</br>; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX."
            ],
            "translated_annotated_samples": [
                "Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una <br>función heurística</br> basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS.",
                "La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 <br>Función heurística</br> basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i).",
                "RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una <br>función heurística</br> basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una <br>función heurística</br> basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 <br>Función heurística</br> basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una <br>función heurística</br> basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "optimal joint policy": {
            "translated_key": "política conjunta óptima",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the <br>optimal joint policy</br> for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an <br>optimal joint policy</br>; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing <br>optimal joint policy</br> with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the <br>optimal joint policy</br> for general distributed POMDPs is NEXP-Complete.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an <br>optimal joint policy</br>; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "For computing <br>optimal joint policy</br> with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17)."
            ],
            "translated_annotated_samples": [
                "Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la <br>política conjunta óptima</br> para POMDP distribuidos generales es NEXP-Completo.",
                "Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una <br>política conjunta óptima</br>; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS.",
                "Para calcular la <br>política conjunta óptima</br> con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17)."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la <br>política conjunta óptima</br> para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una <br>política conjunta óptima</br>; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la <br>política conjunta óptima</br> con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "network structure": {
            "translated_key": "estructura de red",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits <br>network structure</br> of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits <br>network structure</br> of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree."
            ],
            "translated_annotated_samples": [
                "Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la <br>estructura de red</br> de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la <br>estructura de red</br> de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "depth first search": {
            "translated_key": "Búsqueda en Profundidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a <br>depth first search</br> (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a <br>depth first search</br> (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree."
            ],
            "translated_annotated_samples": [
                "Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de <br>Búsqueda en Profundidad</br> (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de <br>Búsqueda en Profundidad</br> (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "distributed sensor network": {
            "translated_key": "red de sensores distribuida",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "overall joint reward": {
            "translated_key": "recompensa conjunta general",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the <br>overall joint reward</br>.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the <br>overall joint reward</br>."
            ],
            "translated_annotated_samples": [
                "Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la <br>recompensa conjunta general</br>."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la <br>recompensa conjunta general</br>. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "maximum constrained node": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the <br>maximum constrained node</br> (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "We employ the <br>maximum constrained node</br> (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed."
            ],
            "translated_annotated_samples": [
                "Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7])."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "partially observable markov decision process": {
            "translated_key": "proceso de decisión de Markov parcialmente observable",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "pomdp": {
            "translated_key": "POMDP",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed <br>pomdp</br> models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed <br>pomdp</br> The ND-<br>pomdp</br> model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-<br>pomdp</br> assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-<br>pomdp</br> is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-<br>pomdp</br> is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-<br>pomdp</br> can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed <br>pomdp</br> in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed <br>pomdp</br> and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed <br>pomdp</br> models.",
                "BACKGROUND 3.1 Model: Network Distributed <br>pomdp</br> The ND-<br>pomdp</br> model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-<br>pomdp</br> assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "The goal in ND-<br>pomdp</br> is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-<br>pomdp</br> is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi."
            ],
            "translated_annotated_samples": [
                "Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de <br>POMDP</br>.",
                "ANTECEDENTES 3.1 Modelo: <br>POMDP</br> Distribuido en Red El modelo ND-<br>POMDP</br> fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2.",
                "A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante.",
                "El objetivo en ND-<br>POMDP</br> es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b.",
                "Un ND-<br>POMDP</br> es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de <br>POMDP</br>. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: <br>POMDP</br> Distribuido en Red El modelo ND-<br>POMDP</br> fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-<br>POMDP</br> es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-<br>POMDP</br> es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "distribute pomdp": {
            "translated_key": "distribuir POMDP",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "globally optimal solution": {
            "translated_key": "solución globalmente óptima",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗ , Milind Tambe, Makoto Yokoo∗ University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept.",
                "of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp ABSTRACT Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains.",
                "Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions.",
                "Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality.",
                "A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost.",
                "This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time.",
                "Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13].",
                "The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable.",
                "Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.",
                "Researchers have attempted two different types of approaches towards solving these models.",
                "The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11].",
                "The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution.",
                "In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10].",
                "Though these approaches obtain optimal solutions, they typically consider only two agents.",
                "Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.",
                "To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents.",
                "We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm.",
                "There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree.",
                "We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution.",
                "The first enhancement uses abstractions for speedup, but does not sacrifice solution quality.",
                "In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies.",
                "The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution.",
                "The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.",
                "We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments.",
                "In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents.",
                "Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions).",
                "We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "DOMAIN: DISTRIBUTED SENSOR NETS Distributed sensor networks are a large, important class of domains that motivate our work.",
                "This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10].",
                "Figure 1 shows a specific problem instance within this type consisting of three sensors.",
                "Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1).",
                "To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously.",
                "In Figure 1, to track a target in Loc11, sensor1 needs to scan East and sensor2 needs to scan West simultaneously.",
                "Thus, sensors have to act in a coordinated fashion.",
                "We assume that there are two independent targets and that each targets movement is uncertain and unaffected by the sensor agents.",
                "Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives.",
                "The sensors observations and transitions are independent of each others actions e.g.the observations that sensor1 receives are independent of sensor2s actions.",
                "Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off.",
                "Given the sensors observational uncertainty, the targets uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.",
                "Figure 1: A 3-chain sensor configuration 3.",
                "BACKGROUND 3.1 Model: Network Distributed POMDP The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2.",
                "It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states.",
                "Si refers to the set of local states of agent i and Su is the set of unaffectable states.",
                "Unaffectable state refers to that part of the world state that cannot be affected by the agents actions, e.g. environmental factors like target locations that no agent can control.",
                "A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i. ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.",
                "Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agents observation depends only on the unaffectable state, its local action and on its resulting local state.",
                "The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|.",
                "Based on the reward function, an interaction hypergraph is constructed.",
                "A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.",
                "The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent is initial belief state, respectively.",
                "The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes teams expected reward over a finite horizon T starting from the belief state b.",
                "An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.",
                "The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph. 3.2 Algorithm: Global Optimal Algorithm (GOA) In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10].",
                "We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents.",
                "GOA borrows from a global optimal DCOP algorithm called DPOP[12].",
                "GOAs message passing follows that of DPOP.",
                "The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root.",
                "Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.",
                "Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies.",
                "This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies.",
                "In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.",
                "GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree).",
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a <br>globally optimal solution</br>. 4.",
                "SPIDER As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.",
                "The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.",
                "The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.",
                "Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.",
                "Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.",
                "We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.",
                "MCN heuristic tries to place agents with more number of constraints at the top of the tree.",
                "This tree governs how the search for the optimal joint polThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.",
                "The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.",
                "SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.",
                "In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).",
                "Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.",
                "Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies.",
                "Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain.",
                "Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.",
                "SPIDER exploits the structure of this DFS tree while engaging in its search.",
                "Note that in our example figure, each agent is assigned a policy with T=2.",
                "Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.",
                "Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.",
                "If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.",
                "SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).",
                "Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agents policies.",
                "Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).",
                "The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps: 1.",
                "Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.",
                "An MDP based heuristic is used to compute these upper bounds on the expected values.",
                "Detailed description about this MDP heuristic is provided in Section 4.2.",
                "All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.",
                "Exploration of these policies (in step 2 below) are performed in this descending order.",
                "As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.",
                "The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values. 2.",
                "Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− .",
                "This is performed by iterating through all policies of agent i i.e.",
                "Πi and summing two quantities for each policy: (i) the best response for all of is children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.",
                "Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].",
                "The policy with the highest expected value is the best response policy.",
                "Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].",
                "Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.",
                "A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.",
                "This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.",
                "On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .",
                "This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.",
                "In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.",
                "The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.",
                "These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.",
                "Algorithm 1 provides the pseudo code for SPIDER.",
                "This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).",
                "Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).",
                "This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.",
                "Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.",
                "Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.",
                "Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children.",
                "The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds. 4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).",
                "The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.",
                "To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .",
                "We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).",
                "Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.",
                "We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption.",
                "This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).",
                "Thus, the equation for the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.",
                "While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ . 4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended.",
                "We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).",
                "The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.",
                "In this paper, we propose two types of abstraction: 1.",
                "Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.",
                "It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.",
                "In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.",
                "For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy.",
                "This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon.",
                "This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.",
                "Sum of (a) and (b) is the heuristic value for a HBA abstract policy. 2.",
                "Node Based Abstraction (NBA): Here an abstract policy is obtained by not associating actions to certain nodes of the policy tree.",
                "Unlike in HBA, this implies multiple levels of abstraction.",
                "This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation TP.",
                "These incomplete T=2 policies are abstractions for T=2 complete policies.",
                "Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.",
                "For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.",
                "In such cases, the immediate reward is taken as Rmax (maximum reward for any action).",
                "We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.",
                "Algorithm 5 provides the algorithm for this abstraction technique.",
                "For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17).",
                "The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).",
                "Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).",
                "However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).",
                "EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.",
                "If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.",
                "Otherwise, πi.absNodes is increased by 1.",
                "Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.",
                "Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).",
                "Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14). 4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to SPIDER called VAX.",
                "The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality.",
                "This approximation parameter is used at each agent for pruning out joint policies.",
                "The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.",
                "However, the 826 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].",
                "Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.",
                "In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.",
                "However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.",
                "This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238).",
                "It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance.",
                "However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution.",
                "A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3. 4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation enhancement over SPIDER called PAX.",
                "Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.",
                "Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.",
                "A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].",
                "Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.",
                "Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).",
                "This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.",
                "Proposition 4 provides the bound on quality loss. 4.6 Theoretical Results PROPOSITION 1.",
                "Heuristic provided using the centralized MDP heuristic is admissible.",
                "Proof.",
                "For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.",
                "Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.",
                "Base case: t = T − 1.",
                "Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.",
                "Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .",
                "Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.",
                "We now have to prove that the proposition holds for t = η − 1.",
                "We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .",
                "The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.",
                "PROPOSITION 2.",
                "SPIDER provides an optimal solution.",
                "Proof.",
                "SPIDER examines all possible joint policies given the interaction structure of the agents.",
                "The only exception being when a joint policy is pruned based on the heuristic value.",
                "Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.",
                "As proved in Proposition 1, the expected value for a joint policy is always an upper bound.",
                "Hence when a joint policy is pruned, it cannot be an optimal solution.",
                "PROPOSITION 3.",
                "Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .",
                "Thus the best response policy computed by VAX would be at most away from the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.",
                "Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.",
                "In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].",
                "However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .",
                "Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .",
                "Hence proved.",
                "PROPOSITION 4.",
                "For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.",
                "Proof.",
                "We prove this proposition using mathematical induction on the depth of the DFS tree.",
                "Base case: depth = 1 (i.e. one node).",
                "Best response is computed by iterating through all policies, Πk.",
                "A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.",
                "Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.",
                "Hence the proposition holds for the base case.",
                "Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.",
                "Without loss of generality, lets assume that the root node of this tree has k children.",
                "Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.",
                "With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].",
                "Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children.",
                "Thus, overall solution quality is at least δ 100 of the optimal solution.",
                "Hence proved. 5.",
                "EXPERIMENTAL RESULTS All our experiments were conducted on the sensor network domain from Section 2.",
                "The five network configurations employed are shown in Figure 4.",
                "Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX.",
                "We compare against GOA because it is the only global optimal algorithm that considers more than two agents.",
                "We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.",
                "Experiments were terminated after 10000 seconds1 .",
                "Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80).",
                "X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale).",
                "The time horizon of policy computation was 3.",
                "For each configuration (3-chain, 4-chain, 4-star and 5-star), there are five bars indicating the time taken by GOA, SPIDER, SPIDERAbs, PAX and VAX.",
                "GOA did not terminate within the time limit for 4-star and 5-star configurations.",
                "SPIDER-Abs dominated the SPIDER and GOA for all the configurations.",
                "For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER.",
                "The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.",
                "For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.",
                "Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a).",
                "X-axis denotes the sensor network configuration while Y-axis indicates the solution quality.",
                "Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms.",
                "For 5-P configuration, the global optimal algorithms did not terminate within the limit of 10000 seconds, so the bar for optimal quality indicates an upper bound on the optimal solution quality.",
                "With both the approximations, we obtained a solution quality that was close to the optimal solution quality.",
                "In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ.",
                "For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.",
                "Figure 5(c) provides the time to solution with PAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As δ was decreased from 70 to 30, the time to solution decreased drastically.",
                "For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30.",
                "Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.",
                "Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons).",
                "X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale).",
                "The time horizon for all the configurations was 4.",
                "As was increased, the time to solution decreased drastically.",
                "For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140.",
                "Again, the actual solution quality did not change with varying epsilon.",
                "Figure 4: Sensor network configurations 828 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4 6.",
                "SUMMARY AND RELATED WORK This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.",
                "These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty.",
                "Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.",
                "Researchers have typically employed two types of techniques for solving distributed POMDPs.",
                "The first set of techniques compute global optimal solutions.",
                "Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.",
                "Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs.",
                "This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory.",
                "The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*s inability to exploit interaction structure, it was illustrated only with two agents.",
                "However, SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).",
                "The second set of techniques seek approximate policies.",
                "EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).",
                "Nair et al. [9]s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.",
                "Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies.",
                "Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution.",
                "This aspect of quality bounds differentiates SPIDER from all the above techniques.",
                "Acknowledgements.",
                "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.",
                "NBCHD030010.",
                "The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 7.",
                "REFERENCES [1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.",
                "Solving transition independent decentralized Markov decision processes.",
                "JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A.",
                "Hansen, and S. Zilberstein.",
                "Bounded policy iteration for decentralized POMDPs.",
                "In IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of MDPs.",
                "In UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun.",
                "Approximate solutions for partially observable stochastic games with common payoffs.",
                "In AAMAS, 2004. [5] E. Hansen, D. Bernstein, and S. Zilberstein.",
                "Dynamic programming for partially observable stochastic games.",
                "In AAAI, 2004. [6] V. Lesser, C. Ortiz, and M. Tambe.",
                "Distributed sensor nets: A multiagent perspective.",
                "Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and P. Varakantham.",
                "Taking dcop to the real world : Efficient complete solutions for distributed event scheduling.",
                "In AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo.",
                "An asynchronous complete method for distributed constraint optimization.",
                "In AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs.",
                "In AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.",
                "Learning to cooperate via policy search.",
                "In UAI, 2000. [12] A. Petcu and B. Faltings.",
                "A scalable method for multiagent constraint optimization.",
                "In IJCAI, 2005. [13] D. Szer, F. Charpillet, and S. Zilberstein.",
                "MAA*: A heuristic search algorithm for solving decentralized POMDPs.",
                "In IJCAI, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829"
            ],
            "original_annotated_samples": [
                "Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a <br>globally optimal solution</br>. 4."
            ],
            "translated_annotated_samples": [
                "Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una <br>solución globalmente óptima</br>. 4."
            ],
            "translated_text": "Liberando una ARAÑA en una red de POMDPs: Generando políticas de calidad garantizada. Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗, Milind Tambe, Makoto Yokoo∗ Universidad del Sur de California, Los Ángeles, CA 90089, {varakant, marecki, tambe}@usc.edu ∗ Dept. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) son un enfoque popular para modelar sistemas multiagente que actúan en dominios inciertos. Dada la complejidad significativa de resolver POMDP distribuidos, especialmente a medida que aumentamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no ofrecen garantías sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimalidad global, pero los resultados típicos solo están disponibles para dos agentes y también con un considerable costo computacional. Este documento supera las limitaciones de ambos enfoques al proporcionar SPIDER, una novedosa combinación de tres características clave para la generación de políticas en POMDP distribuidos: (i) explota la estructura de interacción de agentes dada una red de agentes (es decir, permitiendo una escalabilidad más fácil a un mayor número de agentes); (ii) utiliza una combinación de heurísticas para acelerar la búsqueda de políticas; y (iii) permite aproximaciones garantizadas de calidad, permitiendo un intercambio sistemático de calidad de solución por tiempo. Los resultados experimentales muestran mejoras de órdenes de magnitud en el rendimiento en comparación con algoritmos óptimos globales anteriores. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. Los Problemas de Decisión de Markov Parcialmente Observables Distribuidos (Distributed POMDPs) están emergiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge debido al no determinismo en los resultados de las acciones y porque el estado del mundo puede ser solo parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestra Bernstein et al. [3], el problema de encontrar la política conjunta óptima para POMDP distribuidos generales es NEXP-Completo. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas a nivel global [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. Por el contrario, la segunda categoría menos popular de enfoques se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, típicamente solo consideran dos agentes. Además, no logran aprovechar la estructura en las interacciones de los agentes y, por lo tanto, se ven severamente limitados en cuanto a escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que brinden garantías sobre la calidad de la solución al centrarse en una red de más de dos agentes. Primero proponemos el algoritmo básico SPIDER (Búsqueda de Políticas en Entornos Distribuidos). Hay dos características novedosas clave en SPIDER: (i) es una técnica de búsqueda heurística de ramificación y acotación que utiliza una función heurística basada en MDP para buscar una política conjunta óptima; (ii) explota la estructura de red de agentes organizando agentes en un pseudoárbol de Búsqueda en Profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo SPIDER básico al tiempo que garantizamos la calidad de la solución. La primera mejora utiliza abstracciones para acelerar el proceso, pero no sacrifica la calidad de la solución. En particular, primero realiza una búsqueda de ramificación y acotación en políticas abstractas y luego se extiende a políticas completas. La segunda mejora obtiene aceleraciones sacrificando la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda para lograr eficiencia, sin embargo con un parámetro de tolerancia que se proporciona como un porcentaje del óptimo. Experimentamos con el dominio de redes de sensores presentado en Nair et al. [10], un dominio representativo de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que SPIDER domina un enfoque óptimo global existente llamado GOA [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora significativamente el rendimiento de SPIDER (proporcionando soluciones óptimas). Finalmente demostramos una característica clave de SPIDER: al utilizar las mejoras de aproximación, permite realizar compensaciones fundamentadas entre el tiempo de ejecución y la calidad de la solución. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. DOMINIO: REDES DE SENSORES DISTRIBUIDOS Las redes de sensores distribuidos son una clase grande e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidos por primera vez en [10]. La Figura 1 muestra una instancia específica de un problema dentro de este tipo que consta de tres sensores. Aquí, cada nodo sensor puede escanear en una de las cuatro direcciones: Norte, Sur, Este u Oeste (ver Figura 1). Para rastrear un objetivo y obtener la recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en Loc11, el sensor1 necesita escanear hacia el Este y el sensor2 necesita escanear hacia el Oeste simultáneamente. Por lo tanto, los sensores deben actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que el movimiento de cada objetivo es incierto y no se ve afectado por los agentes del sensor. Basándose en el área que está escaneando, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y transiciones de los sensores son independientes entre sí, por ejemplo, las observaciones que recibe el sensor 1 son independientes de las acciones del sensor 2. Cada agente incurre en un costo por escanear si el objetivo está presente o no, pero no incurre en ningún costo si se apaga. Dada la incertidumbre observacional de los sensores, las transiciones inciertas de los objetivos y la naturaleza distribuida de los nodos sensores, estas redes de sensores proporcionan un dominio útil para aplicar modelos distribuidos de POMDP. Figura 1: Una configuración de sensor de 3 cadenas 3. ANTECEDENTES 3.1 Modelo: POMDP Distribuido en Red El modelo ND-POMDP fue introducido en [10], motivado por dominios como las redes de sensores presentadas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, b, donde S = ×1≤i≤nSi × Su es el conjunto de estados del mundo. Si se refiere al conjunto de estados locales del agente i y Su es el conjunto de estados inafectables. El estado inafectable se refiere a esa parte del estado del mundo que no puede ser afectada por las acciones de los agentes, por ejemplo, factores ambientales como las ubicaciones de destino que ningún agente puede controlar. A = ×1≤i≤nAi es el conjunto de acciones conjuntas, donde Ai es el conjunto de acciones para el agente i. ND-POMDP asume independencia de transición, donde la función de transición se define como P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), donde a = a1, . . . , an es la acción conjunta realizada en el estado s = s1, . . . , sn, su y s = s1, . . . , sn, su es el estado resultante. Ω = ×1≤i≤nΩi es el conjunto de observaciones conjuntas donde Ωi es el conjunto de observaciones para los agentes i. En los ND-POMDPs se asume independencia observacional, es decir, la función de observación conjunta se define como O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), donde s = s1, . . . , sn, su es el estado del mundo que resulta de que los agentes realicen a = a1, . . . , an en el estado anterior, y ω = ω1, . . . , ωn ∈ Ω es la observación recibida en el estado s. Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y su estado local resultante. La función de recompensa, R, se define como R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), donde cada l podría referirse a cualquier subgrupo de agentes y r = |l|. Basándose en la función de recompensa, se construye un hipercubo de interacción. Existe un hiperenlace, l, entre un subconjunto de agentes para todos los Rl que conforman R. El hipergrafo de interacción se define como G = (Ag, E), donde los agentes, Ag, son los vértices y E = {l|l ⊆ Ag ∧ Rl es un componente de R} son las aristas. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b(s) = bu(su) · 1≤i≤n bi(si), donde bu y bi se refieren a la distribución sobre el estado inicial inafectable y el agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1, . . . , πn que maximiza la recompensa esperada del equipo durante un horizonte finito T a partir del estado de creencia b. Un ND-POMDP es similar a un Problema de Optimización de Restricciones Distribuidas (DCOP) n-ario donde la variable en cada nodo representa la política seleccionada por un agente individual, πi, con el dominio de la variable siendo el conjunto de todas las políticas locales, Πi. El componente de recompensa Rl donde |l| = 1 se puede considerar como una restricción local, mientras que el componente de recompensa Rl donde l > 1 corresponde a una restricción no local en el grafo de restricciones. 3.2 Algoritmo: Algoritmo Óptimo Global (GOA) En trabajos anteriores, GOA ha sido definido como un algoritmo óptimo global para ND-POMDPs [10]. Utilizaremos GOA en nuestras comparaciones experimentales, ya que GOA es un algoritmo global óptimo de vanguardia, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. GOA se basa en un algoritmo DCOP óptimo global llamado DPOP[12]. El paso de mensajes de GOA sigue el de DPOP. La primera fase es la propagación de UTIL, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten desde las hojas hasta la raíz. El valor de una política en un agente se define como la suma de los valores de mejor respuesta de sus hijos y la recompensa de política conjunta asociada con la política principal. Por lo tanto, dado un política para un nodo padre, GOA requiere que un agente itere a través de todas sus políticas, encontrando la mejor política de respuesta y devolviendo el valor al padre - mientras que en el nodo padre, para encontrar la mejor política, un agente requiere que sus hijos devuelvan sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de UTIL se repite en cada nivel del árbol, hasta que la raíz agote todas sus políticas. En la segunda fase de propagación de VALOR, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el grafo de interacción, eliminando las evaluaciones conjuntas de políticas innecesarias (asociadas con nodos que no están conectados directamente en el árbol). Dado que el grafo de interacción captura todas las interacciones de recompensa entre agentes y, a medida que este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una <br>solución globalmente óptima</br>. 4. Como se menciona en la Sección 3.1, un ND-POMDP puede ser tratado como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa conjunta general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados de todas las posibles políticas conjuntas. La idea clave en SPIDER es evitar el cálculo de los valores esperados para todo el espacio de políticas conjuntas, utilizando límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. A semejanza de algunos de los algoritmos para DCOP [8, 12], SPIDER tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Ten en cuenta que estos árboles DFS son árboles pseudo [12] que permiten enlaces entre ancestros e hijos. Empleamos la heurística de Nodo Máximo Restringido (MCN) utilizada en el algoritmo DCOP, ADOPT [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística MCN intenta colocar agentes con un mayor número de restricciones en la parte superior del árbol. Este árbol controla cómo se busca el polinomio conjunto óptimo. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 823 procedimientos helados en SPIDER. Los algoritmos presentados en este documento son fácilmente adaptables a hiper-árboles, sin embargo, para propósitos expositivos, asumimos árboles binarios. SPIDER es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este documento, empleamos la siguiente notación para denotar políticas y valores esperados: Ancestros(i) ⇒ agentes desde i hasta la raíz (sin incluir i). Árbol(i) ⇒ agentes en el sub-árbol (sin incluir i) para los cuales i es la raíz. πroot+ ⇒ política conjunta de todos los agentes. πi+ ⇒ política conjunta de todos los agentes en Árbol(i) ∪ i. πi− ⇒ política conjunta de agentes que están en Antecesores(i). πi ⇒ política del agente i. ˆv[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ dado πi y políticas de agentes ancestros, es decir, πi− . ˆvj[πi, πi− ] ⇒ límite superior en el valor esperado para πi+ del j-ésimo hijo. v[πi, πi− ] ⇒ valor esperado para πi dado las políticas de agentes ancestros, πi− . v[πi+ , πi− ] ⇒ valor esperado para πi+ dado las políticas de agentes ancestros, πi− . vj[πi+ , πi− ] ⇒ valor esperado para πi+ del j-ésimo hijo. Figura 2: Ejecución de SPIDER, un ejemplo 4.1 Esquema de SPIDER. SPIDER se basa en la idea de búsqueda de ramificación y acotación, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo SPIDER, utilizando un ejemplo de una cadena de tres agentes. Antes de que SPIDER comience su búsqueda, creamos un árbol DFS (es decir, árbol pseudo) a partir de la cadena de tres agentes, con el agente del medio como la raíz de este árbol. SPIDER explota la estructura de este árbol DFS mientras realiza su búsqueda. Ten en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con T=2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política conjunta parcial/completa, un rectángulo indica un agente y las elipses internas a un agente muestran su política. El valor esperado heurístico o real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. SPIDER comienza sin ninguna política asignada a ninguno de los agentes (mostrado en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas están ordenadas según los límites superiores calculados para las políticas de los agentes raíz. El nivel 3 muestra un nodo de búsqueda SPIDER con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se utiliza para podar los nodos en el nivel 2 (los que tienen límites superiores < 234) al crear políticas para cada agente no hoja i, SPIDER potencialmente realiza dos pasos: 1. Obtención de límites superiores y ordenación: En este paso, el agente i calcula los límites superiores de los valores esperados, ˆv[πi, πi− ] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas ancestrales fijas. Se utiliza una heurística basada en MDP para calcular estos límites superiores en los valores esperados. Una descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente i, Πi, se ordenan luego según estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realiza en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas están ordenadas según los valores heurísticos, indicados en la esquina superior derecha de cada política conjunta. La intuición detrás de ordenar y luego explorar las políticas en orden descendente de límites superiores, es que las políticas con límites superiores más altos podrían producir políticas conjuntas con valores esperados más altos. Exploración y Poda: La exploración implica calcular la mejor política conjunta de respuesta πi+,∗ correspondiente a las políticas ancestrales fijas del agente i, πi−. Esto se realiza iterando a través de todas las políticas del agente i, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos sus hijos (obtenida realizando los pasos 1 y 2 en cada uno de los nodos hijos); (ii) el valor esperado obtenido por i para las políticas fijas de los ancestros. Por lo tanto, la exploración de una política πi produce el valor esperado real de una política conjunta, πi+, representada como v[πi+ , πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular valores esperados) en el agente i mediante el uso del mejor valor esperado actual, vmax [πi+ , πi− ]. De ahora en adelante, este vmax [πi+ , πi− ] se referirá como umbral. Una política, πi, no necesita ser explorada si el límite superior para esa política, ˆv[πi, πi−], es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, SPIDER calcula la política de mejor respuesta (y consecuentemente su valor esperado) correspondiente a las políticas fijas de sus ancestros, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a las políticas fijas de los ancestros) y seleccionando la política con el valor esperado más alto. En la Figura 2, SPIDER asigna políticas de mejor respuesta a los agentes hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción Este en cada paso de tiempo en la política, mientras que la política para el agente de la hoja derecha es realizar la acción Apagado en cada paso de tiempo. Estas políticas de mejor respuesta de los agentes de hojas generan un valor esperado real de 234 para la política conjunta completa. El Algoritmo 1 proporciona el pseudocódigo para SPIDER. Este algoritmo produce la mejor política conjunta, πi+,∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol(i). Las líneas 3-8 calculan la política de mejor respuesta de un agente hoja i, mientras que las líneas 9-23 calculan la mejor política de respuesta conjunta para los agentes en el árbol(i). El cálculo de la mejor respuesta para un agente no hoja i incluye: (a) Ordenar las políticas (en orden descendente) basadas en valores heurísticos en la línea 11; (b) Calcular las políticas de mejor respuesta en cada uno de los hijos para las políticas fijas del agente i en las líneas 16-20; y (c) Mantener 824 La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1 ARAÑA(i, πi−, umbral) 1: πi+,∗ ← nulo 2: Πi ← OBTENER-TODAS-LAS-POLÍTICAS (horizonte, Ai, Ωi) 3: si ES-HOJA(i) entonces 4: para todo πi ∈ Πi hacer 5: v[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 6: si v[πi, πi−] > umbral entonces 7: πi+,∗ ← πi 8: umbral ← v[πi, πi−] 9: sino 10: hijos ← HIJOS(i) 11: ˆΠi ← ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 12: para todo πi ∈ ˆΠi hacer 13: ˜πi+ ← πi 14: si ˆv[πi, πi−] < umbral entonces 15: Ir a la línea 12 16: para todo j ∈ hijos hacer 17: jUmbral ← umbral − v[πi, πi−]− Σk∈hijos,k=j ˆvk[πi, πi−] 18: πj+,∗ ← ARAÑA(j, πi πi−, jUmbral) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: si v[˜πi+, πi−] > umbral entonces 22: umbral ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: regresar πi+,∗ Algoritmo 2 ORDENAR-LÍMITE-SUPERIOR(i, Πi, πi−) 1: hijos ← HIJOS(i) 2: ˆΠi ← nulo /* Almacena la lista ordenada */ 3: para todo πi ∈ Πi hacer 4: ˆv[πi, πi−] ← RECOMPENSA-CONJUNTA (πi, πi−) 5: para todo j ∈ hijos hacer 6: ˆvj[πi, πi−] ← LÍMITE-SUPERIOR(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERTAR-EN-ORDEN (πi, ˆΠi) 9: regresar ˆΠi mejor valor esperado, política conjunta en líneas 21-23. El Algoritmo 2 proporciona el pseudocódigo para ordenar políticas basadas en los límites superiores de los valores esperados de políticas conjuntas. El valor esperado para un agente i consta de dos partes: el valor obtenido de sus ancestros y el valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los ancestros del agente (usando la función RECOMPENSA-CONJUNTA), mientras que las líneas 5-7 calculan el valor heurístico de los hijos. La suma de estas dos partes proporciona un límite superior en el valor esperado para el agente i, y la línea 8 del algoritmo ordena las políticas basadas en estos límites superiores. 4.2 Función heurística basada en MDP La función heurística proporciona rápidamente un límite superior en el valor esperado obtenible de los agentes en Tree(i). El subárbol de agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido del subárbol y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos plena observabilidad para los agentes en Árbol(i) y para las políticas fijas de los agentes en {Antecesores(i) ∪ i}, calculamos el valor conjunto ˆv[πi+ , πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular límites superiores/valores heurísticos (para los agentes i y k): Sea Ei− el conjunto de enlaces entre agentes en {Ancestros(i)∪ i} y Árbol(i), y Ei+ el conjunto de enlaces entre agentes en Árbol(i). Además, si l ∈ Ei−, entonces l1 es el agente en {Ancestros(i) ∪ i} y l2 es el agente en Árbol(i) que l conecta juntos. Primero compactamos la notación estándar: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Dependiendo de la ubicación del agente k en el árbol de agentes, tenemos los siguientes casos: SI k ∈ {Ancestros(i) ∪ i}, ˆpt k = pt k, (2) SI k ∈ Árbol(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) SI l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) SI l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η − 1 está dada por la ecuación: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) donde vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algoritmo 3 LÍMITE SUPERIOR (i, j, πj− ) 1: val ← 0 2: para todo l ∈ Ej− ∪ Ej+ hacer 3: si l ∈ Ej− entonces πl1 ← φ 4: para todo s0 l hacer 5: val + ← startBel[s0 l ]· LÍMITE SUPERIOR-TIEMPO (i, s0 l , j, πl1 , ) 6: retornar val Algoritmo 4 LÍMITE SUPERIOR-TIEMPO (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: para todo al1 , al2 hacer 3: si l ∈ Ei− y l ∈ Ej− entonces al1 ← πl1 (ωt l1 ) 4: val ← OBTENER-RECOMPENSA(st l , al1 , al2 ) 5: si t < πi.horizonte − 1 entonces 6: para todo st+1 l , ωt+1 l1 hacer 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← LÍMITE-SUPERIOR-TIEMPO(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: si val > maxV al entonces maxV al ← val 11: retornar maxV al El límite superior en el valor esperado para un enlace se calcula modificando la ecuación 3 para reflejar la suposición de plena observabilidad. Esto implica eliminar el término de probabilidad observacional para los agentes en Árbol(i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en Árbol(i)). Por lo tanto, la ecuación para el Sexto Congreso Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 825 cálculo del límite superior en un enlace l, es el siguiente: SI l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l SI l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Los Algoritmos 3 y 4 proporcionan el algoritmo para calcular el límite superior para el hijo j del agente i, utilizando las ecuaciones descritas anteriormente. Mientras que el Algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el Algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+. En SPIDER, la fase de exploración/poda solo puede comenzar después de que haya terminado el cálculo de la heurística (o límite superior) y la clasificación de las políticas. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en computación heurística para una política abstracta, lo que conduce a una mejora en el rendimiento en tiempo de ejecución (sin pérdida en la calidad de la solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este documento, proponemos dos tipos de abstracción: 1. Abstracción basada en el horizonte (HBA): Aquí, la política abstracta se define como una política de horizonte más corto. Representa un grupo de políticas de horizonte más largo que tienen las mismas acciones que la política abstracta para momentos menores o iguales al horizonte de la política abstracta. En la Figura 3(a), una política abstracta T=1 que realiza la acción Este, representa un grupo de políticas T=2 que realizan la acción Este en el primer paso de tiempo. Para HBA, hay dos partes en la computación heurística: (a) Calcular el límite superior para el horizonte de la política abstracta. Esto es igual que la computación heurística definida por el algoritmo GETHEURISTIC() para SPIDER, sin embargo con un horizonte temporal más corto (horizonte de la política abstracta). (b) Calculando la recompensa máxima posible que se puede acumular en un paso de tiempo (usando GET-ABS-HEURISTIC()) y multiplicándola por el número de pasos de tiempo hasta el horizonte temporal. La recompensa máxima posible (para un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol(i) y calculando la recompensa conjunta máxima para cualquier acción conjunta. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA. 2. Abstracción basada en nodos (NBA): Aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de en HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3(b), donde hay T=2 políticas que no tienen una acción para la observación TP. Estas políticas incompletas de T=2 son abstracciones de políticas completas de T=2. Niveles más altos de abstracción conducen a una computación más rápida de una política conjunta completa, πroot+, y también a una computación heurística más corta y a fases de exploración y poda. Para la NBA, la computación heurística es similar a la de una política normal, excepto en los casos en los que no hay una acción asociada con los nodos de la política. En tales casos, la recompensa inmediata se toma como Rmax (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, SPIDER-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular la política conjunta óptima con SPIDER-ABS, un agente no hoja i examina inicialmente todas las políticas abstractas T=1 (línea 2) y las ordena según cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción se incrementa gradualmente y luego se exploran estas políticas abstractas en orden descendente de valores heurísticos, eliminando aquellas que tienen valores heurísticos inferiores al umbral (líneas 23-24). La exploración en SPIDER-ABS tiene la misma definición que en SPIDER si la política que se está explorando tiene un horizonte de cálculo de política que es igual al horizonte temporal real y si todos los nodos de la política tienen una acción asociada a ellos (líneas 25-30). Sin embargo, si esas condiciones no se cumplen, entonces se sustituye por un grupo de políticas que representa (usando la función EXTEND-POLICY ()) (líneas 31-32). La función EXTEND-POLICY() también es responsable de inicializar el horizonte y absNodes de una política. absNodes representa el número de nodos en el último nivel del árbol de políticas que no tienen una acción asignada. Si πi.absNodes = |Ωi|πi.horizon−1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absNodes se establece en cero y πi.horizon se incrementa en 1. De lo contrario, πi.absNodes se incrementa en 1. Por lo tanto, esta función combina tanto HBA como NBA utilizando las variables de política, horizonte y absNodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se ordenan según los valores heurísticos (línea 33). Se adopta un tipo similar de abstracción basada en la computación de la mejor respuesta en los agentes hoja (líneas 3-14). 4.4 Aproximación del Valor (VAX) En esta sección, presentamos una mejora aproximada a SPIDER llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia de calidad de la solución óptima. Este parámetro de aproximación se utiliza en cada agente para podar políticas conjuntas. El mecanismo de poda en SPIDER y SPIDER-Abs dicta que una política conjunta solo se podará si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Ejemplo de abstracción para (a) HBA (Abstracción Basada en Horizonte) y (b) NBA (Abstracción Basada en Nodos) la idea en esta técnica es podar una política conjunta si se cumple la siguiente condición: umbral + > ˆv[πi , πi− ]. Aparte de la condición de poda, VAX es igual que SPIDER/SPIDER-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o segundo nodo del árbol de búsqueda) en el nivel 2 fuera 238 en lugar de 232, entonces esa política no podría ser podada utilizando SPIDER o SPIDER-Abs. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también sería podada. Esto se debe a que el umbral (234) en ese momento más el parámetro de aproximación (5), es decir, 239, habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar a partir del ejemplo (recién discutido) que este tipo de poda puede llevar a menos exploraciones y, por lo tanto, a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Una cota del error introducido por este algoritmo aproximado en función de , es proporcionada por la Proposición 3. 4.5 Aproximación porcentual (PAX) En esta sección, presentamos la segunda mejora de aproximación sobre SPIDER llamada PAX. El parámetro de entrada de esta técnica es δ, que representa el porcentaje mínimo de la calidad de la solución óptima deseada. El resultado de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de la solución óptima. Una política se poda si se cumple la siguiente condición: umbral > δ 100 ˆv[πi , πi− ]. Al igual que en VAX, la única diferencia entre PAX y SPIDER/SPIDER-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces PAX con un parámetro de entrada del 98% podría podar ese nodo del árbol de búsqueda (ya que 98 100 ∗238 < 234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, a una mejora en el rendimiento en tiempo de ejecución, aunque potencialmente puede resultar en una pérdida de calidad de la solución. La Proposición 4 establece el límite de pérdida de calidad. 4.6 Resultados Teóricos PROPOSICIÓN 1. La heurística proporcionada utilizando la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una sobreestimación del valor esperado para una política conjunta. Por lo tanto, necesitamos demostrar que: Para l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (consulte la notación en la Sección 4.2). Utilizamos inducción matemática en t para probar esto. Caso base: t = T − 1. Independientemente de si l ∈ Ei− o l ∈ Ei+ , ˆrt l se calcula maximizando sobre todas las acciones de los agentes en el Árbol(i), mientras que rt l se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Suposición: La proposición se cumple para t = η, donde 1 ≤ η < T − 1. Ahora tenemos que demostrar que la proposición se cumple para t = η - 1. Mostramos la prueba para l ∈ Ei− y un razonamiento similar puede ser adoptado para probar para l ∈ Ei+ . La función de valor heurístico para l ∈ Ei− se proporciona mediante la siguiente ecuación: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Reescribiendo el RHS y usando la Ecuación 2 (en la Sección 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Dado que maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l y pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Dado que ˆvη l ≥ vη l (por la suposición) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Dado que ˆrη−1 l ≥ rη−1 l (por definición) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Así se demuestra. PROPOSICIÓN 2. SPIDER proporciona una solución óptima. Prueba. SPIDER examina todas las posibles políticas conjuntas dadas la estructura de interacción de los agentes. La única excepción es cuando una política conjunta es podada basándose en el valor heurístico. Por lo tanto, mientras una política óptima de candidato no sea podada, SPIDER devolverá una política óptima. Como se demostró en la Proposición 1, el valor esperado para una política conjunta siempre es un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. PROPOSICIÓN 3. El límite de error en la calidad de la solución para VAX (implementado sobre SPIDER-ABS) con un parámetro de aproximación es ρ, donde ρ es el número de nodos hoja en el árbol DFS. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 827 Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si ˆv[πk, πk− ] < umbral + . Por lo tanto, la política de respuesta óptima calculada por VAX estaría como máximo alejada de la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, el error introducido en el k-ésimo niño es ρk, donde ρk es el número de nodos hoja en el k-ésimo hijo de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos hoja en el árbol. En SPIDER-ABS, umbral en el agente raíz, thresspider = k v[πk+ , πk− ]. Sin embargo, con VAX el umbral en el agente raíz será (en el peor de los casos), threshvax = k v[πk+ , πk− ]− k ρk. Por lo tanto, con VAX, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ. Por lo tanto, queda demostrado. PROPOSICIÓN 4. Para PAX (implementado sobre SPIDER-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos δ 100 v[πroot+,∗], donde v[πroot+,∗] denota la calidad óptima de la solución. Prueba. Demostramos esta proposición utilizando inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, Πk. Una política, πk, se poda si δ 100 ˆv[πk, πk− ] < umbral. Por lo tanto, la política de respuesta óptima calculada por PAX sería al menos δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición se cumple para el caso base. Suposición: La proposición se cumple para d, donde 1 ≤ profundidad ≤ d. Ahora debemos demostrar que la proposición se cumple para d + 1. Sin pérdida de generalidad, asumamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños tiene una profundidad ≤ d, y por lo tanto, a partir de la suposición, la calidad de la solución en el k-ésimo niño es al menos δ 100 v[πk+,∗ , πk− ] para PAX. Con SPIDER-ABS, una política conjunta se poda en el agente raíz si ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Sin embargo, con PAX, una política conjunta se poda si δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Dado que la condición de poda en el agente raíz en PAX es la misma que en SPIDER-ABS, no se introduce ningún error en el agente raíz y todo el error se introduce en los hijos. Por lo tanto, la calidad de la solución global es al menos δ 100 de la solución óptima. Por lo tanto, demostrado. 5. RESULTADOS EXPERIMENTALES Todos nuestros experimentos se llevaron a cabo en el dominio de redes de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son GOA, SPIDER, SPIDER-ABS, PAX y VAX. Comparamos con GOA porque es el único algoritmo óptimo global que considera más de dos agentes. Realizamos dos conjuntos de experimentos: (i) en primer lugar, comparamos el rendimiento en tiempo de ejecución de los algoritmos mencionados anteriormente y (ii) en segundo lugar, experimentamos con PAX y VAX para estudiar el equilibrio entre el tiempo de ejecución y la calidad de la solución. Los experimentos fueron terminados después de 10000 segundos. La Figura 5(a) proporciona comparaciones de tiempo de ejecución entre los algoritmos óptimos GOA, SPIDER, SPIDER-Abs y los algoritmos aproximados, PAX (con un δ de 30) y VAX (con un δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon de 3.6 GHz, 2GB de RAM y configuración de red de sensores utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala logarítmica). El horizonte temporal de cálculo de políticas fue de 3. Para cada configuración (cadena de 3, cadena de 4, estrella de 4 y estrella de 5), hay cinco barras que indican el tiempo tomado por GOA, SPIDER, SPIDERAbs, PAX y VAX. GOA no se terminó dentro del límite de tiempo para las configuraciones de 4 estrellas y 5 estrellas. SPIDER-Abs dominó al SPIDER y al GOA en todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, SPIDER-ABS proporciona una aceleración de 230 veces sobre GOA y una aceleración de 2 veces sobre SPIDER, y para la configuración de 4 cadenas proporciona una aceleración de 58 veces sobre GOA y una aceleración de 2 veces sobre SPIDER. Los dos enfoques de aproximación, VAX y PAX, proporcionaron una mejora adicional en el rendimiento sobre SPIDER-Abs. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre SPIDER-Abs. La figura 5(b) proporciona una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la figura 5(a). El eje X denota la configuración de la red de sensores, mientras que el eje Y indica la calidad de la solución. Dado que GOA, SPIDER y SPIDER-Abs son todos algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra de calidad óptima indica un límite superior en la calidad de la solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estuvo cerca de la calidad de la solución óptima. En las configuraciones de 3-cadena y 4-estrella, es notable que tanto PAX como VAX obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue menor al 20% de la calidad de la solución óptima. La Figura 5(c) muestra el tiempo de solución con PAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación, δ (porcentaje respecto al óptimo) utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que δ disminuyó de 70 a 30, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de la cadena de 3, hubo un aumento total de velocidad de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un δ bajo del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5(d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes valores de épsilon). El eje X denota el parámetro de aproximación utilizado, mientras que el eje Y denota el tiempo tomado para calcular la solución (en una escala logarítmica). El horizonte temporal para todas las configuraciones fue de 4. A medida que se aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de las 4 estrellas hubo un aumento total de velocidad de 73 veces cuando se cambió de 60 a 140. Nuevamente, la calidad de la solución real no cambió al variar epsilon. Figura 4: Configuraciones de redes de sensores 828 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 5: Comparación de GOA, SPIDER, SPIDER-Abs y VAX para T = 3 en (a) Tiempo de ejecución y (b) Calidad de la solución; (c) Tiempo de solución para PAX con porcentaje variable a lo óptimo para T=4 (d) Tiempo de solución para VAX con epsilon variable para T=4 6. RESUMEN Y TRABAJO RELACIONADO Este artículo presenta cuatro algoritmos SPIDER, SPIDER-ABS, PAX y VAX que ofrecen una combinación novedosa de características para la búsqueda de políticas en POMDP distribuidos: (i) explotando la estructura de interacción de agentes dada una red de agentes (es decir, una escalabilidad más fácil a un mayor número de agentes); (ii) utilizando búsqueda de ramificación y acotación con una función heurística basada en MDP; (iii) utilizando abstracción para mejorar el rendimiento en tiempo de ejecución sin sacrificar la calidad de la solución; (iv) proporcionando límites de porcentaje a priori sobre la calidad de las soluciones utilizando PAX; y (v) proporcionando límites de valor esperado sobre la calidad de las soluciones utilizando VAX. Estas características permiten el intercambio sistemático de la calidad de la solución por el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran una mejora de órdenes de magnitud en el rendimiento respecto a los algoritmos óptimos globales anteriores. Los investigadores suelen emplear dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al. [5] presentan un algoritmo basado en programación dinámica y eliminación iterada de políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al. [13] proporcionan un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo clásico de búsqueda heurística, A∗ y la teoría de control descentralizado. Las principales diferencias entre SPIDER y MAA* son: (a) Las mejoras en SPIDER (VAX y PAX) permiten aproximaciones de calidad garantizada, mientras que MAA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa; (b) Debido a la incapacidad de MAA* para explotar la estructura de interacción, solo se ilustró con dos agentes. Sin embargo, SPIDER ha sido diseñado para redes de agentes; y (c) SPIDER explora la política conjunta de un agente a la vez, mientras que MAA* la expande un paso a la vez (de manera simultánea para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emery Montemerlo et al. [4] aproximan los POSGs como una serie de juegos bayesianos de un paso utilizando heurísticas para aproximar el valor futuro, intercambiando una mirada limitada por eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). El algoritmo JESP de Nair et al. [9] utiliza programación dinámica para alcanzar una solución óptima local para POMDP descentralizados de horizonte finito. Peshkin et al. [11] y Bernstein et al. [2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas óptimas localmente. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error sobre la calidad de la solución. Este aspecto de los límites de calidad diferencia a SPIDER de todas las técnicas anteriores. Agradecimientos. Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el Contrato No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o del Gobierno de los Estados Unidos. REFERENCIAS [1] R. Becker, S. Zilberstein, V. Lesser y C.V. Goldman. Resolviendo procesos de decisión de Markov descentralizados independientes de la transición. JAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. \n\nJAIR, 22:423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política acotada para POMDP descentralizados. En IJCAI, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de MDPs. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En AAMAS, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes de sensores distribuidos: Una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Llevando dcop al mundo real: Soluciones completas eficientes para la programación distribuida de eventos. En AAMAS, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asíncrono para la optimización de restricciones distribuidas. En AAMAS, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Domando POMDP descentralizados: Hacia el cálculo eficiente de políticas para entornos multiagentes. En IJCAI, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: Una síntesis de optimización de restricciones distribuidas y POMDPs. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E. Kim y L. Kaelbling. Aprendiendo a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En IJCAI, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDP descentralizados. En IJCAI, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 829 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}