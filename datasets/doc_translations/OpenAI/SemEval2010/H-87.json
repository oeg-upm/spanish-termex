{
    "id": "H-87",
    "original_text": "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering. Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings. We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function. Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11. Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1. INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval. The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest. Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate. The above conditions attempt to mimic realistic situations where an AF system would be used. That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback. Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing. These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs. None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once. The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13]. Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated. Addressing the third issue is the main focus in this paper. We argue that robustness is an important measure for evaluating and comparing AF methods. By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora. Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts. Available training examples, on the other hand, are often insufficient for tuning the parameters. In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective. This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set. Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other. Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters? Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported. In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR). Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13]. Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14]. It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1). Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus. Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing. Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora. The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study. Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences. Section 4 outlines the Rocchio and LR approaches to AF, respectively. Section 5 reports the experiments and results. Section 6 concludes the main findings in this study. 2. BENCHMARK CORPORA We used four benchmark corpora in our study. Table 1 shows the statistics about these data sets. TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7]. The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set. TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets. The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8]. TDT3 was the evaluation benchmark in the TDT2001 dry run1 . The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998. Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well. The splitting point for training-test sets is different for each topic in TDT. TDT5 was the evaluation benchmark in TDT2004 [4]. The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories. We only used the English versions of those documents in our experiments for this paper. The TDT topics differ from TREC topics both conceptually and statistically. Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories. The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics. Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one. The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting. For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa. Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3. METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents. The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002). For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively. The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics. For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk). To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms. In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric. To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper. Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function. Our objective is to maximize the former or to minimize the latter on test documents. The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions. For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU. The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU. More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU. That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme. At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w . However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus. Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets. Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking. To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth. Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk. Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1. The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied. Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall. This was a challenging part of the TDT2004 evaluation for AF. Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods. This is the first time this issue is explicitly analyzed, to our knowledge. 4. METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights. The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights. The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid. The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype. The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic. If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype. Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF). To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback. The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic. Multiple approaches have been developed. The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase. More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11]. It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF. Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization. Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase. This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1). Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic. Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14]. Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation. If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics. We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents. The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix. Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5]. How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range. The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5. EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004. Multiple research teams participated and multiple runs from each team were allowed. Ctrk and TDT5SU were used as the metrics. Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively. Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU. All the parameters of our runs were tuned on the TDT3 corpus. Results for other sites are also listed anonymously for comparison. Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.) We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ). CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.) Adaptive filtering without using true relevance feedback was also a part of the evaluations. In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made. Such a setting has been conventional for the Topic Tracking task in TDT until 2004. Figure 4 shows the summarized official submissions from each team. Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question. Both Rocchio and LR have parameters that must be prespecified before the AF process. The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector. The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR. Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation. Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f. Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2). We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004. We also tested our methods on TREC10 and TREC11 for further analysis. Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters. We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied. These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal. If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004. The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem. Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate. Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively. Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings. With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ. Table 2 summarizes the results 3 . We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU. For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11. From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0. This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report. More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR. The robustness, we believe, comes from the probabilistic nature of the system-generated scores. That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem. Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not. Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases. This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR. We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6. Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda. The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3. The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,. In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR. In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed. In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning. Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes. We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications. To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3. Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information. Incremental LR, on the other hand, was weaker but still impressive. Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR. For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost. The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk. Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio. Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6. CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization. Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme. For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting. Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7. REFERENCES [1] J. Allan. Incremental relevance feedback for information filtering. In SIGIR-96, 1996. [2] J. Callan. Learning while filtering documents. In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington. Topic detection and tracking overview. In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley. Overview of the TDT 2004 Evaluation and Results. In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning. Springer, 2001. [6] S. Robertson and D. Hull. The TREC-9 filtering track final report. In TREC-9, 2000. [7] S. Robertson and I. Soboroff. The TREC-10 filtering track final report. In TREC-10, 2001. [8] S. Robertson and I. Soboroff. The TREC 2002 filtering track report. In TREC-11, 2002. [9] S. Robertson and S. Walker. Microsoft Cambridge at TREC-9. In TREC-9, 2000. [10] R. Schapire, Y. Singer and A. Singhal. Boosting and Rocchio applied to text filtering. In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel. Margin-based local regression for adaptive filtering. In CIKM-03, 2003. [12] Y. Zhang and J. Callan. Maximum likelihood estimation for filtering thresholds. In SIGIR-01, 2001. [13] Y. Zhang. Using Bayesian priors to combine classifiers for adaptive filtering. In SIGIR-04, 2004. [14] J. Zhang and Y. Yang. Robustness of regularized linear classification methods in text categorization. In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Text Categorization Based on Regularized Linear Classification Methods. Inf. Retr. 4(1): 5-31 (2001).",
    "original_translation": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001).",
    "original_sentences": [
        "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
        "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
        "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
        "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
        "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
        "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
        "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
        "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
        "The above conditions attempt to mimic realistic situations where an AF system would be used.",
        "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
        "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
        "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
        "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
        "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
        "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
        "Addressing the third issue is the main focus in this paper.",
        "We argue that robustness is an important measure for evaluating and comparing AF methods.",
        "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
        "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
        "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
        "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
        "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
        "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
        "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
        "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
        "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
        "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
        "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
        "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
        "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
        "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
        "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
        "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
        "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
        "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
        "Section 5 reports the experiments and results.",
        "Section 6 concludes the main findings in this study. 2.",
        "BENCHMARK CORPORA We used four benchmark corpora in our study.",
        "Table 1 shows the statistics about these data sets.",
        "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
        "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
        "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
        "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
        "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
        "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
        "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
        "The splitting point for training-test sets is different for each topic in TDT.",
        "TDT5 was the evaluation benchmark in TDT2004 [4].",
        "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
        "We only used the English versions of those documents in our experiments for this paper.",
        "The TDT topics differ from TREC topics both conceptually and statistically.",
        "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
        "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
        "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
        "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
        "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
        "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
        "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
        "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
        "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
        "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
        "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
        "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
        "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
        "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
        "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
        "Our objective is to maximize the former or to minimize the latter on test documents.",
        "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
        "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
        "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
        "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
        "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
        "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
        "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
        "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
        "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
        "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
        "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
        "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
        "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
        "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
        "This was a challenging part of the TDT2004 evaluation for AF.",
        "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
        "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
        "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
        "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
        "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
        "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
        "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
        "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
        "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
        "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
        "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
        "Multiple approaches have been developed.",
        "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
        "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
        "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
        "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
        "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
        "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
        "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
        "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
        "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
        "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
        "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
        "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
        "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
        "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
        "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
        "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
        "Multiple research teams participated and multiple runs from each team were allowed.",
        "Ctrk and TDT5SU were used as the metrics.",
        "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
        "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
        "All the parameters of our runs were tuned on the TDT3 corpus.",
        "Results for other sites are also listed anonymously for comparison.",
        "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
        "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
        "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
        "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
        "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
        "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
        "Figure 4 shows the summarized official submissions from each team.",
        "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
        "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
        "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
        "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
        "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
        "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
        "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
        "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
        "We also tested our methods on TREC10 and TREC11 for further analysis.",
        "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
        "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
        "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
        "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
        "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
        "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
        "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
        "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
        "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
        "Table 2 summarizes the results 3 .",
        "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
        "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
        "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
        "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
        "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
        "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
        "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
        "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
        "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
        "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
        "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
        "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
        "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
        "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
        "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
        "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
        "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
        "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
        "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
        "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
        "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
        "Incremental LR, on the other hand, was weaker but still impressive.",
        "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
        "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
        "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
        "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
        "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
        "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
        "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
        "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
        "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
        "NBCHD030010.",
        "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
        "REFERENCES [1] J. Allan.",
        "Incremental relevance feedback for information filtering.",
        "In SIGIR-96, 1996. [2] J. Callan.",
        "Learning while filtering documents.",
        "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
        "Topic detection and tracking overview.",
        "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
        "Overview of the TDT 2004 Evaluation and Results.",
        "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
        "Elements of Statistical Learning.",
        "Springer, 2001. [6] S. Robertson and D. Hull.",
        "The TREC-9 filtering track final report.",
        "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
        "The TREC-10 filtering track final report.",
        "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
        "The TREC 2002 filtering track report.",
        "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
        "Microsoft Cambridge at TREC-9.",
        "In TREC-9, 2000. [10] R. Schapire, Y.",
        "Singer and A. Singhal.",
        "Boosting and Rocchio applied to text filtering.",
        "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
        "Margin-based local regression for adaptive filtering.",
        "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
        "Maximum likelihood estimation for filtering thresholds.",
        "In SIGIR-01, 2001. [13] Y. Zhang.",
        "Using Bayesian priors to combine classifiers for adaptive filtering.",
        "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
        "Robustness of regularized linear classification methods in text categorization.",
        "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
        "Text Categorization Based on Regularized Linear Classification Methods.",
        "Inf.",
        "Retr. 4(1): 5-31 (2001)."
    ],
    "translated_text_sentences": [
        "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo.",
        "Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad.",
        "Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo.",
        "Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11.",
        "El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1.",
        "La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información.",
        "La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido.",
        "A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios.",
        "Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático.",
        "Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia.",
        "Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen.",
        "Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes.",
        "Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez.",
        "Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local.",
        "Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo.",
        "Abordar el tercer problema es el enfoque principal de este documento.",
        "Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF.",
        "Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus.",
        "La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba.",
        "Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros.",
        "En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz.",
        "Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación.",
        "Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí.",
        "Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros?",
        "La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF.",
        "En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR).",
        "Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13].",
        "La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14].",
        "Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1).",
        "Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11.",
        "Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez.",
        "Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia.",
        "La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio.",
        "La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias.",
        "La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF.",
        "La sección 5 informa sobre los experimentos y resultados.",
        "La sección 6 concluye los hallazgos principales de este estudio. 2.",
        "CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio.",
        "La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos.",
        "TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos).",
        "Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas.",
        "TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba.",
        "Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8].",
        "TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001.",
        "La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998.",
        "Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín).",
        "El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT.",
        "TDT5 fue el punto de referencia de evaluación en TDT2004 [4].",
        "La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas.",
        "Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo.",
        "Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente.",
        "En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente.",
        "El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10.",
        "La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno.",
        "Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia.",
        "Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa.",
        "Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3.",
        "MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba.",
        "Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002).",
        "Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente.",
        "Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas.",
        "Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema).",
        "Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas.",
        "Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad.",
        "Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo.",
        "Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo.",
        "Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba.",
        "Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones.",
        "Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU.",
        "La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU.",
        "Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU.",
        "Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema.",
        "A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w.",
        "Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba.",
        "Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas.",
        "Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales.",
        "A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad.",
        "Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk.",
        "De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1.",
        "Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió.",
        "Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación.",
        "Esta fue una parte desafiante de la evaluación TDT2004 para AF.",
        "Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo.",
        "Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4.",
        "MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos.",
        "El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento.",
        "El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo.",
        "Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo.",
        "El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema.",
        "Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo.",
        "Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF).",
        "Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia.",
        "Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación.",
        "Se han desarrollado múltiples enfoques.",
        "Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas.",
        "Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11].",
        "Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF.",
        "En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus.",
        "Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba.",
        "Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1).",
        "También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular.",
        "Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14].",
        "Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación.",
        "Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas.",
        "Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes.",
        "El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad.",
        "Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5].",
        "Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo.",
        "La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5.",
        "EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004.",
        "Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo.",
        "Se utilizaron Ctrk y TDT5SU como métricas.",
        "La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente.",
        "Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU.",
        "Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3.",
        "Los resultados de otros sitios también se enumeran de forma anónima para su comparación.",
        "Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.)",
        "También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ).",
        "CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.)",
        "El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones.",
        "En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos.",
        "Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004.",
        "La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo.",
        "Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante.",
        "Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF.",
        "Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento.",
        "Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR.",
        "Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación.",
        "Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f.",
        "Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2).",
        "Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004.",
        "También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado.",
        "Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes.",
        "Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión.",
        "Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5.",
        "Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004.",
        "La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado.",
        "La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa.",
        "Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente.",
        "Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas.",
        "Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ.",
        "La Tabla 2 resume los resultados 3.",
        "Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU.",
        "Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11.",
        "De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0.",
        "Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe.",
        "Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR.",
        "La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema.",
        "Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil.",
        "Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen.",
        "Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos.",
        "Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR.",
        "También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6.",
        "Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable.",
        "El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3.",
        "El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3.",
        "En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar.",
        "En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización.",
        "En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado.",
        "Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc).",
        "Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real.",
        "Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3.",
        "La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia.",
        "El LR incremental, por otro lado, era más débil pero aún impresionante.",
        "Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR.",
        "Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable.",
        "El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk.",
        "La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio.",
        "Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.",
        "CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados.",
        "Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema.",
        "Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido.",
        "Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No.",
        "NBCHD030010.",
        "Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores.",
        "REFERENCIAS [1] J. Allan.",
        "Retroalimentación incremental de relevancia para la filtración de información.",
        "En SIGIR-96, 1996. [2] J. Callan.",
        "Aprendiendo mientras se filtran documentos.",
        "En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington.",
        "Resumen de detección y seguimiento de temas.",
        "En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley.",
        "Resumen de la Evaluación y Resultados de TDT 2004.",
        "En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman.",
        "Elementos del Aprendizaje Estadístico.",
        "Springer, 2001. [6] S. Robertson y D. Hull.",
        "El informe final de la pista de filtrado TREC-9.",
        "En TREC-9, 2000. [7] S. Robertson e I. Soboroff.",
        "El informe final de la pista de filtrado TREC-10.",
        "En TREC-10, 2001. [8] S. Robertson e I. Soboroff.",
        "El informe de la pista de filtrado TREC 2002.",
        "En TREC-11, 2002. [9] S. Robertson y S. Walker.",
        "Microsoft Cambridge en TREC-9.",
        "En TREC-9, 2000. [10] R. Schapire, Y.",
        "Cantante y A. Singhal.",
        "Boosting y Rocchio aplicados a la filtración de texto.",
        "En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel.",
        "Regresión local basada en márgenes para filtrado adaptativo.",
        "En CIKM-03, 2003. [12] Y. Zhang y J. Callan.",
        "Estimación de máxima verosimilitud para umbrales de filtrado.",
        "En SIGIR-01, 2001. [13] Y. Zhang.",
        "Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo.",
        "En SIGIR-04, 2004. [14] J. Zhang y Y. Yang.",
        "Robustez de los métodos de clasificación lineal regularizados en la categorización de texto.",
        "En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
        "Categorización de texto basada en métodos de clasificación lineal regularizados.",
        "I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish?",
        "Rev. 4(1): 5-31 (2001)."
    ],
    "error_count": 8,
    "keys": {
        "adaptive filtering": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of <br>adaptive filtering</br> Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for <br>adaptive filtering</br>.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION <br>adaptive filtering</br> (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make <br>adaptive filtering</br> a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the <br>adaptive filtering</br> literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in <br>adaptive filtering</br> and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for <br>adaptive filtering</br> in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for <br>adaptive filtering</br> in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to <br>adaptive filtering</br> in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for <br>adaptive filtering</br> evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of <br>adaptive filtering</br> methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC <br>adaptive filtering</br>), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for <br>adaptive filtering</br> with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for <br>adaptive filtering</br> were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "<br>adaptive filtering</br> without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in <br>adaptive filtering</br>, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the <br>adaptive filtering</br> in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in <br>adaptive filtering</br>, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for <br>adaptive filtering</br>.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for <br>adaptive filtering</br>.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Robustness of <br>adaptive filtering</br> Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for <br>adaptive filtering</br>.",
                "INTRODUCTION <br>adaptive filtering</br> (AF) has been a challenging research topic in information retrieval.",
                "These conditions make <br>adaptive filtering</br> a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "The first two problems have been studied in the <br>adaptive filtering</br> literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "It was recently evaluated in <br>adaptive filtering</br> and was found to have relatively strong performance (Section 5.1)."
            ],
            "translated_annotated_samples": [
                "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el <br>filtrado adaptativo</br>.",
                "La <br>filtración adaptativa</br> (AF) ha sido un tema de investigación desafiante en la recuperación de información.",
                "Estas condiciones hacen que el <br>filtrado adaptativo</br> sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes.",
                "Los dos primeros problemas han sido estudiados en la literatura de <br>filtrado adaptativo</br>, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local.",
                "Recientemente fue evaluado en <br>filtrado adaptativo</br> y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1)."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el <br>filtrado adaptativo</br>. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La <br>filtración adaptativa</br> (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el <br>filtrado adaptativo</br> sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de <br>filtrado adaptativo</br>, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en <br>filtrado adaptativo</br> y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). ",
            "candidates": [],
            "error": [
                [
                    "filtrado adaptativo",
                    "filtración adaptativa",
                    "filtrado adaptativo",
                    "filtrado adaptativo",
                    "filtrado adaptativo"
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in <br>information retrieval</br>.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in <br>information retrieval</br>."
            ],
            "translated_annotated_samples": [
                "La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la <br>recuperación de información</br>."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la <br>recuperación de información</br>. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "topic detection": {
            "translated_key": "Detección y Seguimiento de Temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the <br>topic detection</br> and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the <br>topic detection</br> and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "<br>topic detection</br> and tracking overview.",
                "In <br>topic detection</br> and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Using four corpora from the <br>topic detection</br> and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "Starting from 1997 in the <br>topic detection</br> and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "<br>topic detection</br> and tracking overview.",
                "In <br>topic detection</br> and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley."
            ],
            "translated_annotated_samples": [
                "Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad.",
                "A partir de 1997 en el área de <br>Detección y Seguimiento de Temas</br> (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios.",
                "Resumen de <br>detección y seguimiento de temas</br>.",
                "En <br>Detección y seguimiento de temas</br>: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de <br>Detección y Seguimiento de Temas</br> (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de <br>detección y seguimiento de temas</br>. En <br>Detección y seguimiento de temas</br>: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "topic tracking": {
            "translated_key": "seguimiento de temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or <br>topic tracking</br> in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for <br>topic tracking</br> is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for <br>topic tracking</br>, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the <br>topic tracking</br> task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for <br>topic tracking</br> without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or <br>topic tracking</br> in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for <br>topic tracking</br> is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for <br>topic tracking</br>, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Such a setting has been conventional for the <br>topic tracking</br> task in TDT until 2004.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for <br>topic tracking</br> without relevance feedback information."
            ],
            "translated_annotated_samples": [
                "A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o <br>seguimiento de temas</br> en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios.",
                "Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el <br>seguimiento de temas</br> se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente.",
                "Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el <br>seguimiento de temas</br>, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1).",
                "Un entorno así ha sido convencional para la tarea de <br>seguimiento de temas</br> en TDT hasta 2004.",
                "La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el <br>seguimiento de temas</br> sin información de retroalimentación de relevancia."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o <br>seguimiento de temas</br> en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el <br>seguimiento de temas</br> se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el <br>seguimiento de temas</br>, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de <br>seguimiento de temas</br> en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el <br>seguimiento de temas</br> sin información de retroalimentación de relevancia. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "relevance feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "<br>relevance feedback</br> on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without <br>relevance feedback</br> information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, <br>relevance feedback</br>, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • <br>relevance feedback</br> was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • <br>relevance feedback</br> (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through <br>relevance feedback</br>.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., <br>relevance feedback</br> on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If <br>relevance feedback</br> is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if <br>relevance feedback</br> is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without <br>relevance feedback</br> (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of <br>relevance feedback</br>. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true <br>relevance feedback</br>. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true <br>relevance feedback</br>. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true <br>relevance feedback</br>. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true <br>relevance feedback</br> was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much <br>relevance feedback</br> (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true <br>relevance feedback</br>; • Adaptive Rocchio, updating topic profiles using <br>relevance feedback</br> on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with <br>relevance feedback</br> on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without <br>relevance feedback</br> information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with <br>relevance feedback</br> on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get <br>relevance feedback</br> for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true <br>relevance feedback</br>. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental <br>relevance feedback</br> for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "<br>relevance feedback</br> on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without <br>relevance feedback</br> information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, <br>relevance feedback</br>, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • <br>relevance feedback</br> was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • <br>relevance feedback</br> (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through <br>relevance feedback</br>.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., <br>relevance feedback</br> on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs."
            ],
            "translated_annotated_samples": [
                "El <br>feedback de relevancia</br> en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de <br>feedback de relevancia</br>.",
                "Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1.",
                "A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La <br>retroalimentación de relevancia</br> estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La <br>retroalimentación de relevancia</br> (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios.",
                "Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de <br>retroalimentación de relevancia</br>.",
                "Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, <br>retroalimentación de relevancia</br> solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El <br>feedback de relevancia</br> en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de <br>feedback de relevancia</br>. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La <br>retroalimentación de relevancia</br> estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La <br>retroalimentación de relevancia</br> (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de <br>retroalimentación de relevancia</br>. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, <br>retroalimentación de relevancia</br> solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. ",
            "candidates": [],
            "error": [
                [
                    "feedback de relevancia",
                    "feedback de relevancia",
                    "retroalimentación de relevancia",
                    "retroalimentación de relevancia",
                    "retroalimentación de relevancia",
                    "retroalimentación de relevancia"
                ]
            ]
        },
        "statistical learning": {
            "translated_key": "aprendizaje estadístico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in <br>statistical learning</br> (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of <br>statistical learning</br> for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in <br>statistical learning</br>, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in <br>statistical learning</br>, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of <br>statistical learning</br>.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "These conditions make adaptive filtering a difficult task in <br>statistical learning</br> (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of <br>statistical learning</br> for batch classification where all the training data are given at once.",
                "Logistic regression is a classical method in <br>statistical learning</br>, and one of the best in batch-mode text categorization [15][14].",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in <br>statistical learning</br>, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "Elements of <br>statistical learning</br>."
            ],
            "translated_annotated_samples": [
                "Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el <br>aprendizaje estadístico</br> (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes.",
                "Ninguno de estos problemas se aborda en la literatura de <br>aprendizaje estadístico</br> para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez.",
                "La regresión logística es un método clásico en el <br>aprendizaje estadístico</br>, y uno de los mejores en la categorización de texto en modo por lotes [15][14].",
                "Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el <br>aprendizaje estadístico</br>, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.",
                "Elementos del <br>Aprendizaje Estadístico</br>."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el <br>aprendizaje estadístico</br> (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de <br>aprendizaje estadístico</br> para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el <br>aprendizaje estadístico</br>, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el <br>aprendizaje estadístico</br>, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del <br>Aprendizaje Estadístico</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "robustness": {
            "translated_key": "robustez",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>robustness</br> of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and <br>robustness</br> of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that <br>robustness</br> is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the <br>robustness</br> of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for <br>robustness</br> testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the <br>robustness</br> of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the <br>robustness</br> of LR.",
                "The <br>robustness</br>, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their <br>robustness</br> in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • <br>robustness</br> in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "<br>robustness</br> of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "<br>robustness</br> of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and <br>robustness</br> of current methods because the third problem has not been thoroughly investigated.",
                "We argue that <br>robustness</br> is an important measure for evaluating and comparing AF methods.",
                "Current literature does not offer an answer because no thorough investigation on the <br>robustness</br> of AF methods has been reported.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for <br>robustness</br> testing."
            ],
            "translated_annotated_samples": [
                "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo.",
                "Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo.",
                "Sostenemos que la <br>robustez</br> es una medida importante para evaluar y comparar los métodos de AF.",
                "La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la <br>robustez</br> de los métodos de AF.",
                "Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de <br>robustez</br>."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la <br>robustez</br> es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la <br>robustez</br> de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de <br>robustez</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "systematic method for parameter tuning across multiple corpora": {
            "translated_key": "método sistemático para ajustar parámetros en múltiples corpus",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a <br>systematic method for parameter tuning across multiple corpora</br>.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "By robust we mean consistent and strong performance across benchmark corpora with a <br>systematic method for parameter tuning across multiple corpora</br>."
            ],
            "translated_annotated_samples": [
                "Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un <br>método sistemático para ajustar parámetros en múltiples corpus</br>."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un <br>método sistemático para ajustar parámetros en múltiples corpus</br>. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "external corpus": {
            "translated_key": "corpus externo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an <br>external corpus</br> as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an <br>external corpus</br> for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an <br>external corpus</br> as the validation set.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an <br>external corpus</br> for validation."
            ],
            "translated_annotated_samples": [
                "Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un <br>corpus externo</br> como conjunto de validación.",
                "Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un <br>corpus externo</br> para la validación."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un <br>corpus externo</br> como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un <br>corpus externo</br> para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "validation set": {
            "translated_key": "conjunto de validación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the <br>validation set</br>.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a <br>validation set</br> and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the <br>validation set</br>.",
                "The simplest is to use a universal threshold for all topics, tuned on a <br>validation set</br> and fixed during the testing phase."
            ],
            "translated_annotated_samples": [
                "Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como <br>conjunto de validación</br>.",
                "Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un <br>conjunto de validación</br> y fijo durante la fase de pruebas."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como <br>conjunto de validación</br>. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un <br>conjunto de validación</br> y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "logistic regression": {
            "translated_key": "regresión logística",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized <br>logistic regression</br> (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, <br>logistic regression</br> in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized <br>logistic regression</br> (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "<br>logistic regression</br> is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 <br>logistic regression</br> for AF <br>logistic regression</br> (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our <br>logistic regression</br> had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "<br>logistic regression</br> has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, <br>logistic regression</br> is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized <br>logistic regression</br> (LR) and incremental Rocchio for adaptive filtering.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, <br>logistic regression</br> in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized <br>logistic regression</br> (LR).",
                "<br>logistic regression</br> is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 <br>logistic regression</br> for AF <br>logistic regression</br> (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic."
            ],
            "translated_annotated_samples": [
                "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la <br>regresión logística</br> regularizada (LR) y Rocchio incremental para el filtrado adaptativo.",
                "Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, <br>regresión logística</br> en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local.",
                "En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y <br>regresión logística</br> regularizada (LR).",
                "La <br>regresión logística</br> es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14].",
                "También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La <br>regresión logística</br> (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la <br>regresión logística</br> regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, <br>regresión logística</br> en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y <br>regresión logística</br> regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La <br>regresión logística</br> es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La <br>regresión logística</br> (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "granularity difference": {
            "translated_key": "diferencias de granularidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The <br>granularity difference</br>s among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "The <br>granularity difference</br>s among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting."
            ],
            "translated_annotated_samples": [
                "Las <br>diferencias de granularidad</br> entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las <br>diferencias de granularidad</br> entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "cross-benchmark evaluation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a <br>cross-benchmark evaluation</br> Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a <br>cross-benchmark evaluation</br> of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a <br>cross-benchmark evaluation</br> with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the <br>cross-benchmark evaluation</br> interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our <br>cross-benchmark evaluation</br> gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a <br>cross-benchmark evaluation</br> of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Robustness of Adaptive Filtering Methods In a <br>cross-benchmark evaluation</br> Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a <br>cross-benchmark evaluation</br> of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "In this paper we address the above question by conducting a <br>cross-benchmark evaluation</br> with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "The granularity differences among topics and the corresponding non-stationary distributions make the <br>cross-benchmark evaluation</br> interesting.",
                "More importantly, our <br>cross-benchmark evaluation</br> gives strong evidence for the robustness of LR.",
                "CONCLUDING REMARKS We presented a <br>cross-benchmark evaluation</br> of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization."
            ],
            "translated_annotated_samples": [
                "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una <br>evaluación cruzada</br> de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo.",
                "En este artículo abordamos la pregunta anterior realizando una <br>evaluación de referencia cruzada</br> con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR).",
                "Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la <br>evaluación cruzada</br> de referencia.",
                "Más importante aún, nuestra <br>evaluación de referencia cruzada</br> proporciona evidencia sólida de la robustez de LR.",
                "CONCLUSIONES FINALES Presentamos una <br>evaluación de referencia cruzada</br> de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una <br>evaluación cruzada</br> de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una <br>evaluación de referencia cruzada</br> con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la <br>evaluación cruzada</br> de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra <br>evaluación de referencia cruzada</br> proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una <br>evaluación de referencia cruzada</br> de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. ",
            "candidates": [],
            "error": [
                [
                    "evaluación cruzada",
                    "evaluación de referencia cruzada",
                    "evaluación cruzada",
                    "evaluación de referencia cruzada",
                    "evaluación de referencia cruzada"
                ]
            ]
        },
        "utility function": {
            "translated_key": "función de utilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC <br>utility function</br>) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC <br>utility function</br>) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function."
            ],
            "translated_annotated_samples": [
                "Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una <br>función de utilidad</br> de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una <br>función de utilidad</br> de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "cost function": {
            "translated_key": "función de costo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a <br>cost function</br>.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a <br>cost function</br>."
            ],
            "translated_annotated_samples": [
                "Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una <br>función de costo</br>."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una <br>función de costo</br>. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "penalty ratio": {
            "translated_key": "proporción de penalización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the <br>penalty ratio</br> for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different <br>penalty ratio</br> for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the <br>penalty ratio</br> in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended <br>penalty ratio</br> of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual <br>penalty ratio</br> for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual <br>penalty ratio</br> for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the <br>penalty ratio</br> in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the <br>penalty ratio</br> of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the <br>penalty ratio</br> for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the <br>penalty ratio</br> for misses vs. false alarms.",
                "That is, Ctrk has a very different <br>penalty ratio</br> for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the <br>penalty ratio</br> in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended <br>penalty ratio</br> of 10:1 to 100:1, roughly speaking.",
                "Comparing the above result to formula 2, we can see the actual <br>penalty ratio</br> for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk."
            ],
            "translated_annotated_samples": [
                "Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la <br>proporción de penalización</br> para omisiones vs. falsas alarmas.",
                "Es decir, Ctrk tiene una <br>proporción de penalización</br> muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema.",
                "A primera vista, uno pensaría que la <br>proporción de penalización</br> en Ctrk es de 10:1 ya que 11 =w y 1.02 =w.",
                "Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la <br>proporción de penalización</br> prevista de 10:1 a 100:1, hablando en términos generales.",
                "Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la <br>proporción de penalización</br> para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una <br>proporción de penalización</br> muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la <br>proporción de penalización</br> en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la <br>proporción de penalización</br> prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "optimization criterion": {
            "translated_key": "criterio de optimización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the <br>optimization criterion</br> depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the <br>optimization criterion</br> depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied."
            ],
            "translated_annotated_samples": [
                "Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el <br>criterio de optimización</br> depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el <br>criterio de optimización</br> depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "probabilistic threshold calibration": {
            "translated_key": "calibración de umbrales probabilísticos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include <br>probabilistic threshold calibration</br> which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "More elaborate methods include <br>probabilistic threshold calibration</br> which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11]."
            ],
            "translated_annotated_samples": [
                "Métodos más elaborados incluyen la <br>calibración de umbrales probabilísticos</br> que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la <br>calibración de umbrales probabilísticos</br> que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "rocchio-style method": {
            "translated_key": "método de estilo Rocchio",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "cross-corpus parameter optimization": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic <br>cross-corpus parameter optimization</br> with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through <br>cross-corpus parameter optimization</br>.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the <br>cross-corpus parameter optimization</br> results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 <br>cross-corpus parameter optimization</br> How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to <br>cross-corpus parameter optimization</br>.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Using systematic <br>cross-corpus parameter optimization</br> with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through <br>cross-corpus parameter optimization</br>.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the <br>cross-corpus parameter optimization</br> results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 <br>cross-corpus parameter optimization</br> How much the strong performance of our systems depends on parameter tuning is an important question.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to <br>cross-corpus parameter optimization</br>."
            ],
            "translated_annotated_samples": [
                "Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11.",
                "En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la <br>optimización de parámetros entre corpus</br>.",
                "EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de <br>optimización de parámetros entre corpus</br>, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004.",
                "Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La <br>optimización de parámetros entre corpus cruzados</br>. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante.",
                "CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la <br>optimización de parámetros entre corpus cruzados</br>."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la <br>optimización de parámetros entre corpus</br>. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de <br>optimización de parámetros entre corpus</br>, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La <br>optimización de parámetros entre corpus cruzados</br>. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la <br>optimización de parámetros entre corpus cruzados</br>. ",
            "candidates": [],
            "error": [
                [
                    "optimización de parámetros entre corpus",
                    "optimización de parámetros entre corpus",
                    "optimización de parámetros entre corpus cruzados",
                    "optimización de parámetros entre corpus cruzados"
                ]
            ]
        },
        "posterior probability": {
            "translated_key": "probabilidad posterior",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the <br>posterior probability</br> of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the <br>posterior probability</br> of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic."
            ],
            "translated_annotated_samples": [
                "También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la <br>probabilidad posterior</br> de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la <br>probabilidad posterior</br> de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "sigmoid function": {
            "translated_key": "función sigmoide",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a <br>sigmoid function</br> )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a <br>sigmoid function</br> )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic."
            ],
            "translated_annotated_samples": [
                "También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una <br>función sigmoide</br> )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una <br>función sigmoide</br> )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "bias": {
            "translated_key": "sesgo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling <br>bias</br> (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable <br>bias</br> to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling <br>bias</br> (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable <br>bias</br> to LR."
            ],
            "translated_annotated_samples": [
                "Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el <br>sesgo</br> de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes.",
                "Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un <br>sesgo</br> no deseado en LR."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el <br>sesgo</br> de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un <br>sesgo</br> no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "gaussian": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, <br>gaussian</br>-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a <br>gaussian</br> prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the <br>gaussian</br> prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, <br>gaussian</br>-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "The second term in the objective function is for regularization, equivalent to adding a <br>gaussian</br> prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the <br>gaussian</br> prior would introduce undesirable bias to LR."
            ],
            "translated_annotated_samples": [
                "Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad <br>gaussiana-exponencial</br>, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local.",
                "El segundo término en la función objetivo es para regularización, equivalente a agregar una priori <br>gaussiana</br> a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad.",
                "Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la <br>distribución gaussiana</br> introduciría un sesgo no deseado en LR."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad <br>gaussiana-exponencial</br>, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori <br>gaussiana</br> a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la <br>distribución gaussiana</br> introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    "gaussiana-exponencial",
                    "gaussiana",
                    "distribución gaussiana"
                ]
            ]
        },
        "regularization": {
            "translated_key": "regularización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for <br>regularization</br>, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more <br>regularization</br> is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "The second term in the objective function is for <br>regularization</br>, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more <br>regularization</br> is needed."
            ],
            "translated_annotated_samples": [
                "El segundo término en la función objetivo es para <br>regularización</br>, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad.",
                "En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más <br>regularización</br>."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para <br>regularización</br>, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más <br>regularización</br>. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "lr": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (<br>lr</br>) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that <br>lr</br> performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (<br>lr</br>).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and <br>lr</br> in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and <br>lr</br> in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and <br>lr</br> approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (<br>lr</br>) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for <br>lr</br> is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of <br>lr</br> to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard <br>lr</br> if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is <br>lr</br> with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and <br>lr</br> have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in <br>lr</br>.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or <br>lr</br>), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for <br>lr</br>, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that <br>lr</br> significantly outperformed Rocchio on all the corpora, even in the runs of standard <br>lr</br> without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for <br>lr</br> on TREC11 although our results of <br>lr</br> (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of <br>lr</br>.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in <br>lr</br> a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of <br>lr</br> did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than <br>lr</br> models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to <br>lr</br>.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of <br>lr</br> with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 <br>lr</br>(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of <br>lr</br> is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized <br>lr</br> is stable, the same as or improved slightly over the performance of standard <br>lr</br>.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard <br>lr</br> because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and <br>lr</br>, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and <br>lr</br> on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • <br>lr</br> with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental <br>lr</br>, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in <br>lr</br>.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in <br>lr</br> to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of <br>lr</br> on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive <br>lr</br> was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc <br>lr</br> % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc <br>lr</br>(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental <br>lr</br> in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found <br>lr</br> more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (<br>lr</br>) and incremental Rocchio for adaptive filtering.",
                "We found that <br>lr</br> performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (<br>lr</br>).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and <br>lr</br> in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and <br>lr</br> in our crossbenchmark evaluation for robustness testing."
            ],
            "translated_annotated_samples": [
                "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo.",
                "Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo.",
                "En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR).",
                "Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11.",
                "Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "adaptive filter": {
            "translated_key": "filtrado adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for <br>adaptive filter</br>ing.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make <br>adaptive filter</br>ing a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the <br>adaptive filter</br>ing literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in <br>adaptive filter</br>ing and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for <br>adaptive filter</br>ing in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for <br>adaptive filter</br>ing in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to <br>adaptive filter</br>ing in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for <br>adaptive filter</br>ing evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of <br>adaptive filter</br>ing methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC <br>adaptive filter</br>ing), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for <br>adaptive filter</br>ing with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for <br>adaptive filter</br>ing were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in <br>adaptive filter</br>ing, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the <br>adaptive filter</br>ing in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in <br>adaptive filter</br>ing, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for <br>adaptive filter</br>ing.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for <br>adaptive filter</br>ing.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for <br>adaptive filter</br>ing.",
                "These conditions make <br>adaptive filter</br>ing a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "The first two problems have been studied in the <br>adaptive filter</br>ing literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "It was recently evaluated in <br>adaptive filter</br>ing and was found to have relatively strong performance (Section 5.1).",
                "TREC10 was the evaluation benchmark for <br>adaptive filter</br>ing in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7]."
            ],
            "translated_annotated_samples": [
                "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el <br>filtrado adaptativo</br>.",
                "Estas condiciones hacen que el <br>filtrado adaptativo</br> sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes.",
                "Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local.",
                "Recientemente fue evaluado en <br>filtrado adaptativo</br> y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1).",
                "TREC10 fue el punto de referencia de evaluación para el <br>filtrado adaptativo</br> en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos)."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el <br>filtrado adaptativo</br>. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el <br>filtrado adaptativo</br> sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en <br>filtrado adaptativo</br> y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el <br>filtrado adaptativo</br> en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "topic track": {
            "translated_key": "seguimiento de temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or <br>topic track</br>ing in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for <br>topic track</br>ing is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for <br>topic track</br>ing, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for <br>topic track</br>ing without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or <br>topic track</br>ing in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for <br>topic track</br>ing is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for <br>topic track</br>ing, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for <br>topic track</br>ing without relevance feedback information."
            ],
            "translated_annotated_samples": [
                "A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o <br>seguimiento de temas</br> en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios.",
                "Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el <br>seguimiento de temas</br> se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente.",
                "Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el <br>seguimiento de temas</br>, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1).",
                "La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el <br>seguimiento de temas</br> sin información de retroalimentación de relevancia."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o <br>seguimiento de temas</br> en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el <br>seguimiento de temas</br> se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el <br>seguimiento de temas</br>, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el <br>seguimiento de temas</br> sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "rocchio": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental <br>rocchio</br> for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while <br>rocchio</br> is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental <br>rocchio</br>, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental <br>rocchio</br> and regularized logistic regression (LR).",
                "<br>rocchio</br>-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of <br>rocchio</br> and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include <br>rocchio</br> and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the <br>rocchio</br> and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental <br>rocchio</br> for AF We employed a common version of <br>rocchio</br>-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply <br>rocchio</br> and the second case PRF <br>rocchio</br> where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental <br>rocchio</br> is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt <br>rocchio</br>-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of <br>rocchio</br>-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of <br>rocchio</br> as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of <br>rocchio</br> has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of <br>rocchio</br> are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in <br>rocchio</br>, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our <br>rocchio</br> (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the <br>rocchio</br> method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF <br>rocchio</br>.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF <br>rocchio</br> (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both <br>rocchio</br> and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in <br>rocchio</br>, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (<br>rocchio</br> or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive <br>rocchio</br> Figure 5 compares the performance curves in TDT5SU for <br>rocchio</br> on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for <br>rocchio</br> in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the <br>rocchio</br> method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of <br>rocchio</br>-based methods on these corpora, which are our own results of <br>rocchio</br> on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed <br>rocchio</br> on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using <br>rocchio</br> prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in <br>rocchio</br>, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while <br>rocchio</br> classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a <br>rocchio</br> prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that <br>rocchio</br> prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a <br>rocchio</br> prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best <br>rocchio</br> 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the <br>rocchio</br> prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both <br>rocchio</br> and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated <br>rocchio</br> and LR on TDT with the following settings: • Basic <br>rocchio</br>, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive <br>rocchio</br> with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF <br>rocchio</br>, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive <br>rocchio</br> and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF <br>rocchio</br>.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both <br>rocchio</br> and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/<br>rocchio</br> model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental <br>rocchio</br> and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than <br>rocchio</br>; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found <br>rocchio</br> performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and <br>rocchio</br> applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental <br>rocchio</br> for adaptive filtering.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while <br>rocchio</br> is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental <br>rocchio</br>, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental <br>rocchio</br> and regularized logistic regression (LR).",
                "<br>rocchio</br>-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13]."
            ],
            "translated_annotated_samples": [
                "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y <br>Rocchio</br> incremental para el filtrado adaptativo.",
                "Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo.",
                "Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando <br>Rocchio incremental</br>, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local.",
                "En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR).",
                "Los clasificadores de <br>estilo Rocchio</br> han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]."
            ],
            "translated_text": "Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y <br>Rocchio</br> incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando <br>Rocchio incremental</br>, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de <br>estilo Rocchio</br> han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. ",
            "candidates": [],
            "error": [
                [
                    "Rocchio",
                    "Rocchio incremental",
                    "estilo Rocchio"
                ]
            ]
        }
    }
}