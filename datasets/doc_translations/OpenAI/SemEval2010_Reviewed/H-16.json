{
    "id": "H-16",
    "original_text": "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1. INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers. As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data. In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial. The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal. Caching can be applied at different levels with increasing response latencies or processing requirements. For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network. The decision of what to cache is either off-line (static) or online (dynamic). A static cache is based on historical information and is periodically updated. A dynamic cache replaces entries according to the sequence of requests. When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss. Such online decisions are based on a cache policy, and several different policies have been studied in the past. For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries. Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms. Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing. Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists. On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers. Caching of posting lists has additional challenges. As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later. Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient. Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time. Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture. In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change. In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1. We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms. More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation. We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU. We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching. The remainder of this paper is organized as follows. Sections 2 and 3 summarize related work and characterize the data sets we use. Section 4 discusses the limitations of dynamic caching. Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively. Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2. RELATED WORK There is a large body of work devoted to query optimization. Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined. More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15]. Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching. They may be considered separate and complementary to a cache-based approach. Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries. Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies. Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8]. Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7]. Different from our work, they consider caching and prefetching of pages of results. As systems are often hierarchical, there has also been some effort on multi-level architectures. Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13]. Their goal for such systems has been to improve response time for hierarchical engines. In their architecture, both levels use an LRU eviction policy. They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput. Baeza-Yates and Saint-Jean propose a three-level index organization [2]. Long and Suel propose a caching system structured according to three different levels [9]. The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists. These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy. Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache. Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting. To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3. DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006. In our logs, 50% of the total volume of queries are unique. The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log. The distribution of document frequencies of terms in the UK-2006 dataset (upper curve). Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve). The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.) The y-axis is Table 1: Statistics of the UK-2006 sample. UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term). As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively. In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space. The query terms (middle curve) have been normalized for case, as have the terms in the document collection. The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB. The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2). The statistics of the collection are shown in Table 1. We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424. A scatter plot for a random sample of terms is shown in Figure 3. In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4. CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests. That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective. In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume. Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%. This is because only 56% of all the queries comprise queries that have multiple occurrences. It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses. A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/. URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time. This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses. If we consider a cache with infinite memory, then the hit ratio is 50%. Note that for an infinite cache there are no capacity misses. As we mentioned before, another possibility is to cache the posting lists of terms. Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query. On the other hand, they need more space. As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller. In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms. We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective. Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins. That is, we plot the normalized number of elements that appear in a day. This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log. Total queries and Total terms correspond to the total volume of queries and terms, respectively. Unique queries and Unique terms correspond to the arrival rate of unique queries and terms. Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique. In Figure 4, as expected, the volume of terms is much higher than the volume of queries. The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries. This observation implies that terms repeat significantly more than queries. If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition. We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine. We found that it follows closely the arrival rate for terms shown in Figure 4. To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache. On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries. In this graph, the most frequent queries are not the same queries that were most frequent before the cache. It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries. The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries. If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency. When discussing the effectiveness of dynamically caching, an important metric is cache miss rate. To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14]. A working set, informally, is the set of references that an application or an operating system is currently working with. The model uses such sets in a strategy that tries to capture the temporal locality of references. The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size. Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons. First, it captures the amount of locality of queries and terms in a sequence of queries. Locality in this case refers to the frequency of queries and terms in a window of time. If many queries appear multiple times in a window, then locality is high. Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints. Third, working sets capture aspects of efficient caching algorithms such as LRU. LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14]. Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms. The working set sizes are normalized against the total number of queries in the query log. In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01. Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries. Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller. The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values. The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries. This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries. This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query. We analyze these issues more carefully later in this paper. It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph. It reports the distribution of distances between repetitions of the same frequent query. The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times. From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high. Thus, caching the posting lists of terms has the potential to improve the hit ratio. This is what we explore next. 5. CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers. In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available. The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists. We consider both dynamic and static caching. For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account. Before discussing the static caching strategies, we introduce some notation. We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears. The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t). We call this algorithm Qtf. We observe that there is a trade-off between fq(t) and fd(t). Terms with high fq(t) are useful to keep in the cache because they are queried often. On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space. In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value. In our case, value corresponds to fq(t) and size corresponds to fd(t). Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) . We call this algorithm QtfDf. We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added. In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio. The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8. Performance is measured with hit rate. The cache size is measured as a fraction of the total space required to store the posting lists of all terms. For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries. For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream. As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists. The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms. An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries. However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6. ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists. Our analysis takes into account the impact of caching between two levels of the data-access hierarchy. It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction. Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers). Assume that all posting lists are of the same length L, measured in answer units. We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists. In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache. Thus, Np = Nc/L. Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries). For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit. For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units. Of course TR2 > TR1. Now we want to compare the time to answer a stream of Q queries in both cases. Let Vc(Nc) be the volume of the most frequent Nc queries. Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)). Similarly, for case (B), let Vp(Np) be the number of computable queries. Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). We want to check under which conditions we have TP L < TCA. We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. Figure 9 shows the values of Vp and Vc for our data. We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers. As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα . Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β . In the worst case, for a large cache, β → 1. That is, both techniques will cache a constant fraction of the overall query volume. Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1. If we use compression, we have L < L and TR1 > TR1. According to the experiments that we show later, compression is always better. For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data. In this case there will always be a point where TP L > TCA for a large number of queries. In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists. In such a case, there will be some queries that could be answered by both parts of the cache. As the answer cache is faster, it will be the first choice for answering those queries. Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively. Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L. Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically. In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section. We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM. We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming. The posting lists in the inverted file consist of pairs of document identifier and term frequency. We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size. Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case). Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16]. The size of the inverted file is 1,189Mb. A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes. From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26. We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way. Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory. The average time is Tc = 0.069ms. T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system). For each query, we remove stop words, if there are at least three remaining terms. The stop words correspond to the terms with a frequency higher than the number of documents in the index. We use a document-at-a-time approach to retrieve documents containing all query terms. The only disk access required during query processing is for reading compressed posting lists from the inverted file. We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users. In the partial evaluation of queries, we terminate the processing after matching 10,000 documents. The estimated ratios TR are presented in Table 2. Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists. The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t). Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists. The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites. Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site. The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker. Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible). The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors. Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms. Hence, TRL = TR + 0.615ms/0.069ms = TR + 9. In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms. Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists. To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists. We perform simulations and compute the average response time as a function of x. Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists. For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached. In Figure 11, we plot the simulated response time for a centralized system as a function of x. For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB. In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries. We obtained similar trends in the results for the LAN setting. Figure 12 shows the simulated workload for a distributed system across a WAN. In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists. According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially. When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers. This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7. EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time. To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June. We found that a very small percentage of queries are new queries. The majority of queries that appear in a given week repeat in the following weeks for the next six months. We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13). We report hit rate hourly for 7 days, starting from 5pm. We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum. After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week. The static cache of posting lists can be periodically recomputed. To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream. We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14). We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream. This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies. To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate. As Figure 14 shows, the hit rate decreases by less than 2%. The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream. Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8. CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization. We present results on both dynamic and static caching. Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries. Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy. Caching terms is more effective with respect to miss rate, achieving values as low as 12%. We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies. We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures. Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9. REFERENCES [1] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean. A three level search engine index based in query log distribution. In SPIRE, 2003. [3] C. Buckley and A. F. Lewit. Optimization of inverted vector searches. In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke. A document-centric approach to static index pruning in text retrieval systems. In ACM CIKM, 2006. [5] P. Cao and S. Irani. Cost-aware WWW proxy caching algorithms. In USITS, 1997. [6] P. Denning. Working sets past and present. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando. Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data. ACM Trans. Inf. Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran. Predictive caching and prefetching of query results in search engines. In WWW, 2003. [9] X. Long and T. Suel. Three-level caching for efficient query processing in large web search engines. In WWW, 2005. [10] E. P. Markatos. On caching search engine query results. Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A High Performance and Scalable Information Retrieval Platform. In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever. On the reuse of past optimal queries. In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto. Rank-preserving two-level caching for scalable search engines. In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger. A note on the calculation of average working set size. Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft. Optimization strategies for complex queries. In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat. Managing Gigabytes: Compressing and Indexing Documents and Images. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. On-line file caching. Algorithmica, 33(3):371-383, 2002.",
    "original_translation": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002.",
    "original_sentences": [
        "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
        "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
        "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
        "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
        "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
        "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
        "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
        "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
        "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
        "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
        "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
        "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
        "Caching can be applied at different levels with increasing response latencies or processing requirements.",
        "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
        "The decision of what to cache is either off-line (static) or online (dynamic).",
        "A static cache is based on historical information and is periodically updated.",
        "A dynamic cache replaces entries according to the sequence of requests.",
        "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
        "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
        "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
        "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
        "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
        "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
        "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
        "Caching of posting lists has additional challenges.",
        "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
        "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
        "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
        "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
        "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
        "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
        "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
        "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
        "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
        "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
        "The remainder of this paper is organized as follows.",
        "Sections 2 and 3 summarize related work and characterize the data sets we use.",
        "Section 4 discusses the limitations of dynamic caching.",
        "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
        "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
        "RELATED WORK There is a large body of work devoted to query optimization.",
        "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
        "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
        "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
        "They may be considered separate and complementary to a cache-based approach.",
        "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
        "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
        "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
        "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
        "Different from our work, they consider caching and prefetching of pages of results.",
        "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
        "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
        "Their goal for such systems has been to improve response time for hierarchical engines.",
        "In their architecture, both levels use an LRU eviction policy.",
        "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
        "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
        "Long and Suel propose a caching system structured according to three different levels [9].",
        "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
        "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
        "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
        "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
        "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
        "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
        "In our logs, 50% of the total volume of queries are unique.",
        "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
        "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
        "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
        "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
        "The y-axis is Table 1: Statistics of the UK-2006 sample.",
        "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
        "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
        "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
        "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
        "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
        "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
        "The statistics of the collection are shown in Table 1.",
        "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
        "A scatter plot for a random sample of terms is shown in Figure 3.",
        "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
        "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
        "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
        "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
        "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
        "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
        "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
        "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
        "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
        "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
        "If we consider a cache with infinite memory, then the hit ratio is 50%.",
        "Note that for an infinite cache there are no capacity misses.",
        "As we mentioned before, another possibility is to cache the posting lists of terms.",
        "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
        "On the other hand, they need more space.",
        "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
        "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
        "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
        "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
        "That is, we plot the normalized number of elements that appear in a day.",
        "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
        "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
        "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
        "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
        "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
        "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
        "This observation implies that terms repeat significantly more than queries.",
        "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
        "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
        "We found that it follows closely the arrival rate for terms shown in Figure 4.",
        "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
        "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
        "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
        "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
        "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
        "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
        "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
        "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
        "A working set, informally, is the set of references that an application or an operating system is currently working with.",
        "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
        "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
        "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
        "First, it captures the amount of locality of queries and terms in a sequence of queries.",
        "Locality in this case refers to the frequency of queries and terms in a window of time.",
        "If many queries appear multiple times in a window, then locality is high.",
        "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
        "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
        "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
        "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
        "The working set sizes are normalized against the total number of queries in the query log.",
        "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
        "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
        "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
        "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
        "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
        "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
        "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
        "We analyze these issues more carefully later in this paper.",
        "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
        "It reports the distribution of distances between repetitions of the same frequent query.",
        "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
        "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
        "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
        "This is what we explore next. 5.",
        "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
        "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
        "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
        "We consider both dynamic and static caching.",
        "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
        "Before discussing the static caching strategies, we introduce some notation.",
        "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
        "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
        "We call this algorithm Qtf.",
        "We observe that there is a trade-off between fq(t) and fd(t).",
        "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
        "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
        "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
        "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
        "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
        "We call this algorithm QtfDf.",
        "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
        "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
        "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
        "Performance is measured with hit rate.",
        "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
        "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
        "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
        "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
        "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
        "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
        "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
        "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
        "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
        "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
        "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
        "Assume that all posting lists are of the same length L, measured in answer units.",
        "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
        "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
        "Thus, Np = Nc/L.",
        "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
        "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
        "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
        "Of course TR2 > TR1.",
        "Now we want to compare the time to answer a stream of Q queries in both cases.",
        "Let Vc(Nc) be the volume of the most frequent Nc queries.",
        "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
        "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
        "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
        "We want to check under which conditions we have TP L < TCA.",
        "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
        "Figure 9 shows the values of Vp and Vc for our data.",
        "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
        "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
        "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
        "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
        "In the worst case, for a large cache, β → 1.",
        "That is, both techniques will cache a constant fraction of the overall query volume.",
        "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
        "If we use compression, we have L < L and TR1 > TR1.",
        "According to the experiments that we show later, compression is always better.",
        "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
        "In this case there will always be a point where TP L > TCA for a large number of queries.",
        "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
        "In such a case, there will be some queries that could be answered by both parts of the cache.",
        "As the answer cache is faster, it will be the first choice for answering those queries.",
        "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
        "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
        "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
        "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
        "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
        "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
        "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
        "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
        "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
        "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
        "The size of the inverted file is 1,189Mb.",
        "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
        "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
        "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
        "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
        "The average time is Tc = 0.069ms.",
        "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
        "For each query, we remove stop words, if there are at least three remaining terms.",
        "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
        "We use a document-at-a-time approach to retrieve documents containing all query terms.",
        "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
        "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
        "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
        "The estimated ratios TR are presented in Table 2.",
        "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
        "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
        "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
        "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
        "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
        "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
        "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
        "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
        "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
        "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
        "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
        "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
        "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
        "We perform simulations and compute the average response time as a function of x.",
        "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
        "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
        "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
        "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
        "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
        "We obtained similar trends in the results for the LAN setting.",
        "Figure 12 shows the simulated workload for a distributed system across a WAN.",
        "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
        "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
        "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
        "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
        "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
        "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
        "We found that a very small percentage of queries are new queries.",
        "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
        "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
        "We report hit rate hourly for 7 days, starting from 5pm.",
        "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
        "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
        "The static cache of posting lists can be periodically recomputed.",
        "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
        "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
        "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
        "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
        "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
        "As Figure 14 shows, the hit rate decreases by less than 2%.",
        "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
        "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
        "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
        "We present results on both dynamic and static caching.",
        "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
        "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
        "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
        "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
        "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
        "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
        "REFERENCES [1] V. N. Anh and A. Moffat.",
        "Pruned query evaluation using pre-computed impacts.",
        "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
        "A three level search engine index based in query log distribution.",
        "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
        "Optimization of inverted vector searches.",
        "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
        "A document-centric approach to static index pruning in text retrieval systems.",
        "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
        "Cost-aware WWW proxy caching algorithms.",
        "In USITS, 1997. [6] P. Denning.",
        "Working sets past and present.",
        "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
        "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
        "ACM Trans.",
        "Inf.",
        "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
        "Predictive caching and prefetching of query results in search engines.",
        "In WWW, 2003. [9] X.",
        "Long and T. Suel.",
        "Three-level caching for efficient query processing in large web search engines.",
        "In WWW, 2005. [10] E. P. Markatos.",
        "On caching search engine query results.",
        "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
        "He, C. Macdonald, and C. Lioma.",
        "Terrier: A High Performance and Scalable Information Retrieval Platform.",
        "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
        "On the reuse of past optimal queries.",
        "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
        "Rank-preserving two-level caching for scalable search engines.",
        "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
        "A note on the calculation of average working set size.",
        "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
        "Optimization strategies for complex queries.",
        "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
        "Managing Gigabytes: Compressing and Indexing Documents and Images.",
        "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
        "On-line file caching.",
        "Algorithmica, 33(3):371-383, 2002."
    ],
    "translated_text_sentences": [
        "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
        "En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web.",
        "Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones.",
        "Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas.",
        "Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores.",
        "También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones.",
        "Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo.",
        "Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1.",
        "INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas.",
        "A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos.",
        "En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché.",
        "El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo.",
        "El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento.",
        "Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia.",
        "La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico).",
        "Una caché estática se basa en información histórica y se actualiza periódicamente.",
        "Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes.",
        "Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché.",
        "Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado.",
        "Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras.",
        "Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados.",
        "A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas.",
        "Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché.",
        "Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché.",
        "La caché de listas de publicaciones tiene desafíos adicionales.",
        "Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante.",
        "El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio.",
        "Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo.",
        "Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida.",
        "En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros.",
        "En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1.",
        "Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta.",
        "Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta.",
        "Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU.",
        "Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático.",
        "El resto de este documento está organizado de la siguiente manera.",
        "Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos.",
        "La sección 4 discute las limitaciones del almacenamiento en caché dinámico.",
        "Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente.",
        "La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales.",
        "TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas.",
        "Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo.",
        "Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15].",
        "Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché.",
        "Podrían considerarse como separados y complementarios a un enfoque basado en caché.",
        "Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares.",
        "Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché.",
        "Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8].",
        "Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7].",
        "A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados.",
        "Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel.",
        "Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13].",
        "Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos.",
        "En su arquitectura, ambos niveles utilizan una política de expulsión LRU.",
        "Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general.",
        "Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2].",
        "Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9].",
        "El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes.",
        "Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria.",
        "Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché.",
        "Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico.",
        "Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3.",
        "CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006.",
        "En nuestros registros, el 50% del volumen total de consultas son únicas.",
        "La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas.",
        "La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior).",
        "La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia).",
        "El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y).",
        "El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006.",
        "Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico.",
        "Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente.",
        "En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco.",
        "Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos.",
        "La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB.",
        "La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2).",
        "Las estadísticas de la colección se muestran en la Tabla 1.",
        "Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424.",
        "Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3.",
        "En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4.",
        "El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes.",
        "Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva.",
        "En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total.",
        "Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%.",
        "Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias.",
        "Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios.",
        "Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/.",
        "URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez.",
        "Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché.",
        "Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%.",
        "Ten en cuenta que para una caché infinita no hay fallos de capacidad.",
        "Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos.",
        "Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta.",
        "Por otro lado, necesitan más espacio.",
        "A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor.",
        "En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta.",
        "Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo.",
        "La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores.",
        "Es decir, representamos el número normalizado de elementos que aparecen en un día.",
        "Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas.",
        "El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente.",
        "Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos.",
        "Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas.",
        "En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas.",
        "La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas.",
        "Esta observación implica que los términos se repiten significativamente más que las consultas.",
        "Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición.",
        "También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda.",
        "Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4.",
        "Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU.",
        "En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas.",
        "En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché.",
        "Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché.",
        "La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes.",
        "Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta.",
        "Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché.",
        "Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14].",
        "Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente.",
        "El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias.",
        "La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana.",
        "Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones.",
        "Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas.",
        "La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo.",
        "Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta.",
        "Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria.",
        "Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU.",
        "LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14].",
        "La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos.",
        "Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas.",
        "En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01.",
        "Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas.",
        "En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor.",
        "La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos.",
        "El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas.",
        "Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas.",
        "Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario.",
        "Analizaremos estos temas con más detalle más adelante en este documento.",
        "Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error.",
        "Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente.",
        "La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces.",
        "A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta.",
        "Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos.",
        "Esto es lo que exploramos a continuación. 5.",
        "La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas.",
        "En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible.",
        "Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones.",
        "Consideramos tanto el almacenamiento en caché dinámico como estático.",
        "Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones.",
        "Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones.",
        "Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t.",
        "La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t).",
        "Llamamos a este algoritmo Qtf.",
        "Observamos que hay un equilibrio entre fq(t) y fd(t).",
        "Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia.",
        "Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio.",
        "De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total.",
        "En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t).",
        "Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t).",
        "Llamamos a este algoritmo QtfDf.",
        "Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida.",
        "Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t).",
        "El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8.",
        "El rendimiento se mide con la tasa de aciertos.",
        "El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos.",
        "Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas.",
        "Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas.",
        "Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones.",
        "La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos.",
        "Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas.",
        "Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6.",
        "ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones.",
        "Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos.",
        "Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción.",
        "Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas).",
        "Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta.",
        "Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones.",
        "En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché.",
        "Por lo tanto, Np = Nc/L.",
        "Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales).",
        "Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo.",
        "Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo.",
        "Por supuesto, TR2 > TR1.",
        "Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos.",
        "Sea Vc(Nc) el volumen de las consultas más frecuentes Nc.",
        "Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
        "De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables.",
        "Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
        "Queremos verificar bajo qué condiciones tenemos TP L < TCA.",
        "Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
        "La Figura 9 muestra los valores de Vp y Vc para nuestros datos.",
        "Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas.",
        "Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα.",
        "Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
        "Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β.",
        "En el peor de los casos, para una caché grande, β → 1.",
        "Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas.",
        "Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1.",
        "Si usamos compresión, tenemos L < L y TR1 > TR1.",
        "Según los experimentos que mostramos más adelante, la compresión siempre es mejor.",
        "Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos.",
        "En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas.",
        "En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones.",
        "En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché.",
        "Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas.",
        "Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente.",
        "Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L.",
        "Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente.",
        "En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior.",
        "Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM.",
        "Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos.",
        "Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término.",
        "Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño.",
        "Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido).",
        "Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16].",
        "El tamaño del archivo invertido es de 1,189Mb.",
        "Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes.",
        "De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26.",
        "Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera.",
        "Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria.",
        "El tiempo promedio es Tc = 0.069ms.",
        "T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema).",
        "Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes.",
        "Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice.",
        "Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta.",
        "El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido.",
        "Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios.",
        "En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes.",
        "Las razones estimadas TR se presentan en la Tabla 2.",
        "La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas.",
        "El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t).",
        "Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas.",
        "El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios.",
        "Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio.",
        "El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor.",
        "Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante).",
        "La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas.",
        "Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms.",
        "Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9.",
        "En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms.",
        "Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones.",
        "Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones.",
        "Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x.",
        "Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones.",
        "Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché.",
        "En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x.",
        "Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB.",
        "En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas.",
        "Obtuvimos tendencias similares en los resultados para la configuración de LAN.",
        "La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN.",
        "En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones.",
        "Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta.",
        "Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas.",
        "Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario.",
        "EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo.",
        "Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio.",
        "Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas.",
        "La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses.",
        "Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13).",
        "Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm.",
        "Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo.",
        "Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana.",
        "La caché estática de listas de publicaciones puede ser recalculada periódicamente.",
        "Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual.",
        "Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14).",
        "Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas.",
        "Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta.",
        "Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos.",
        "Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%.",
        "La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro.",
        "De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%.",
        "CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red.",
        "Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático.",
        "El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes.",
        "Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo.",
        "El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%.",
        "También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias.",
        "Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas.",
        "Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9.",
        "REFERENCIAS [1] V. N. Anh y A. Moffat.",
        "Evaluación de consulta podada utilizando impactos precalculados.",
        "En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean.",
        "Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas.",
        "En SPIRE, 2003. [3] C. Buckley y A. F. Lewit.",
        "Optimización de búsquedas de vectores invertidos.",
        "En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke.",
        "Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto.",
        "En ACM CIKM, 2006. [5] P. Cao y S. Irani.",
        "Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo.",
        "En USITS, 1997. [6] P. Denning.",
        "Conjuntos de trabajo pasados y presentes.",
        "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando.",
        "Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico.",
        "ACM Trans.",
        "I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish?",
        "Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran.",
        "Caché predictivo y precarga de resultados de consultas en motores de búsqueda.",
        "En WWW, 2003. [9] X.",
        "Largo y T. Suel.",
        "Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes.",
        "En WWW, 2005. [10] E. P. Markatos.",
        "Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda.",
        "Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
        "Él, C. Macdonald y C. Lioma.",
        "Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable.",
        "En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever.",
        "Sobre la reutilización de consultas óptimas pasadas.",
        "En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto.",
        "Caché de dos niveles que preserva el orden para motores de búsqueda escalables.",
        "En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger.",
        "Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo.",
        "Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft.",
        "Estrategias de optimización para consultas complejas.",
        "En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat.",
        "Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes.",
        "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
        "Caché de archivos en línea.",
        "Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002."
    ],
    "error_count": 3,
    "keys": {
        "efficient caching system": {
            "translated_key": "sistemas de almacenamiento en caché eficientes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing <br>efficient caching system</br>s for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing <br>efficient caching system</br>s for Web search engines."
            ],
            "translated_annotated_samples": [
                "En este artículo estudiamos los compromisos en el diseño de <br>sistemas de almacenamiento en caché eficientes</br> para motores de búsqueda web."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de <br>sistemas de almacenamiento en caché eficientes</br> para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web search engine": {
            "translated_key": "motores de búsqueda web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of <br>web search engine</br>s: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large <br>web search engine</br>s.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "Boosting the performance of <br>web search engine</br>s: Caching and prefetching query results by exploiting historical usage data.",
                "Three-level caching for efficient query processing in large <br>web search engine</br>s."
            ],
            "translated_annotated_samples": [
                "Mejorando el rendimiento de los <br>motores de búsqueda web</br>: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico.",
                "Caché de tres niveles para un procesamiento eficiente de consultas en <br>motores de búsqueda web</br> grandes."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los <br>motores de búsqueda web</br>: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en <br>motores de búsqueda web</br> grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "static caching": {
            "translated_key": "almacenamiento en caché estático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for <br>static caching</br> of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of <br>static caching</br>, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "<br>static caching</br> of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a <br>static caching</br> policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker <br>static caching</br> posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between <br>static caching</br> of query answers and posting lists; • <br>static caching</br> of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on <br>static caching</br>.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of <br>static caching</br>, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on <br>static caching</br>, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our <br>static caching</br> algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for <br>static caching</br> of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and <br>static caching</br>.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the <br>static caching</br> strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF <br>static caching</br> In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the <br>static caching</br> algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and <br>static caching</br>.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for <br>static caching</br> of posting lists that outperforms previous <br>static caching</br> algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the <br>static caching</br> of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "We propose a new algorithm for <br>static caching</br> of posting lists, which outperforms previous methods.",
                "Finally, we measure how the changes in the query log affect the effectiveness of <br>static caching</br>, given our observation that the distribution of the queries changes slowly over time.",
                "<br>static caching</br> of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a <br>static caching</br> policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker <br>static caching</br> posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture."
            ],
            "translated_annotated_samples": [
                "Proponemos un nuevo algoritmo para el <br>almacenamiento en caché estático</br> de listas de publicaciones, que supera a los métodos anteriores.",
                "Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del <br>almacenamiento en caché estático</br>, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo.",
                "El <br>almacenamiento en caché estático</br> de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio.",
                "Finalmente, antes de decidir adoptar una política de <br>almacenamiento en caché estática</br>, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo.",
                "Nivel de <br>almacenamiento en caché estático</br> de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el <br>almacenamiento en caché estático</br> de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del <br>almacenamiento en caché estático</br>, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El <br>almacenamiento en caché estático</br> de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de <br>almacenamiento en caché estática</br>, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de <br>almacenamiento en caché estático</br> de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. ",
            "candidates": [],
            "error": [
                [
                    "almacenamiento en caché estático",
                    "almacenamiento en caché estático",
                    "almacenamiento en caché estático",
                    "almacenamiento en caché estática",
                    "almacenamiento en caché estático"
                ]
            ]
        },
        "dynamic caching": {
            "translated_key": "almacenamiento en caché dinámico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. <br>dynamic caching</br>, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than <br>dynamic caching</br> with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of <br>dynamic caching</br>.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and <br>dynamic caching</br> policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level <br>dynamic caching</br> system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For <br>dynamic caching</br>, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for <br>dynamic caching</br>: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "<br>dynamic caching</br> of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "We explore the impact of different approaches, such as static vs. <br>dynamic caching</br>, and caching query results vs. caching posting lists.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than <br>dynamic caching</br> with, for example, LRU.",
                "Section 4 discusses the limitations of <br>dynamic caching</br>.",
                "Fagni et al. follow Markatos work by showing that combining static and <br>dynamic caching</br> policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level <br>dynamic caching</br> system [13]."
            ],
            "translated_annotated_samples": [
                "Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones.",
                "Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el <br>almacenamiento en caché dinámico</br> con, por ejemplo, LRU.",
                "La sección 4 discute las limitaciones del <br>almacenamiento en caché dinámico</br>.",
                "Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7].",
                "Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de <br>almacenamiento en caché dinámico</br> de dos niveles [13]."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el <br>almacenamiento en caché dinámico</br> con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del <br>almacenamiento en caché dinámico</br>. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de <br>almacenamiento en caché dinámico</br> de dos niveles [13]. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "caching query result": {
            "translated_key": "resultado de consulta en caché",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and <br>caching query result</br>s vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between <br>caching query result</br>s and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of <br>caching query result</br>s and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "We explore the impact of different approaches, such as static vs. dynamic caching, and <br>caching query result</br>s vs. caching posting lists.",
                "We present a framework for the analysis of the trade-off between <br>caching query result</br>s and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of <br>caching query result</br>s and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9."
            ],
            "translated_annotated_samples": [
                "Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el <br>almacenamiento en caché de resultados de consultas</br> vs. el almacenamiento en caché de listas de publicaciones.",
                "Presentamos un marco para el análisis del equilibrio entre <br>almacenar en caché los resultados de consultas</br> y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas.",
                "Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de <br>resultados de consultas en caché</br> y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el <br>almacenamiento en caché de resultados de consultas</br> vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre <br>almacenar en caché los resultados de consultas</br> y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de <br>resultados de consultas en caché</br> y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                [
                    "almacenamiento en caché de resultados de consultas",
                    "almacenar en caché los resultados de consultas",
                    "resultados de consultas en caché"
                ]
            ]
        },
        "caching posting list": {
            "translated_key": "lista de publicaciones en caché",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. <br>caching posting list</br>s.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that <br>caching posting list</br>s can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static <br>caching posting list</br>s Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for <br>caching posting list</br>s, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that <br>caching posting list</br>s can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for <br>caching posting list</br>s.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and <br>caching posting list</br>s. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then <br>caching posting list</br>s makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and <br>caching posting list</br>s.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for <br>caching posting list</br>s.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and <br>caching posting list</br>s, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. <br>caching posting list</br>s.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that <br>caching posting list</br>s can achieve higher hit rates than caching query answers.",
                "Broker Static <br>caching posting list</br>s Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "Sections 5 and 6 introduce algorithms for <br>caching posting list</br>s, and a theoretical framework for the analysis of static caching, respectively.",
                "CACHING POSTING LISTS The previous section shows that <br>caching posting list</br>s can obtain a higher hit rate compared to caching query answers."
            ],
            "translated_annotated_samples": [
                "Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el <br>almacenamiento en caché de listas de publicaciones</br>.",
                "Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que <br>almacenar en caché listas de publicaciones</br> puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas.",
                "Nivel de <br>almacenamiento en caché</br> estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de <br>almacenamiento en caché</br>, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida.",
                "Las secciones 5 y 6 presentan algoritmos para <br>almacenar en caché listas de publicaciones</br>, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente.",
                "La sección anterior muestra que almacenar en caché las <br>listas de publicaciones</br> puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el <br>almacenamiento en caché de listas de publicaciones</br>. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que <br>almacenar en caché listas de publicaciones</br> puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de <br>almacenamiento en caché</br> estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de <br>almacenamiento en caché</br>, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para <br>almacenar en caché listas de publicaciones</br>, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las <br>listas de publicaciones</br> puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. ",
            "candidates": [],
            "error": [
                [
                    "almacenamiento en caché de listas de publicaciones",
                    "almacenar en caché listas de publicaciones",
                    "almacenamiento en caché",
                    "almacenamiento en caché",
                    "almacenar en caché listas de publicaciones",
                    "listas de publicaciones"
                ]
            ]
        },
        "static cache": {
            "translated_key": "caché estática",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the <br>static cache</br> between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A <br>static cache</br> is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a <br>static cache</br>, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the <br>static cache</br> corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a <br>static cache</br> is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a <br>static cache</br> of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the <br>static cache</br> is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a <br>static cache</br> holding 128,000 answers during the period of a week.",
                "The <br>static cache</br> of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the <br>static cache</br> we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "We also study the problem of finding the optimal way to split the <br>static cache</br> between answers and posting lists.",
                "A <br>static cache</br> is based on historical information and is periodically updated.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a <br>static cache</br>, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "In fact, the problem of selecting the best posting lists for the <br>static cache</br> corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "An important benefit a <br>static cache</br> is that it requires no eviction and it is hence more efficient when evaluating queries."
            ],
            "translated_annotated_samples": [
                "También estudiamos el problema de encontrar la manera óptima de dividir la <br>caché estática</br> entre respuestas y listas de publicaciones.",
                "Una <br>caché estática</br> se basa en información histórica y se actualiza periódicamente.",
                "Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una <br>caché estática</br>, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático.",
                "De hecho, el problema de seleccionar las mejores listas de publicaciones para la <br>caché estática</br> corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total.",
                "Un beneficio importante de una <br>caché estática</br> es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la <br>caché estática</br> entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una <br>caché estática</br> se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una <br>caché estática</br>, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la <br>caché estática</br> corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una <br>caché estática</br> es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "answer and posting list": {
            "translated_key": "lista de respuestas y publicaciones",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "query log": {
            "translated_key": "registro de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a <br>query log</br> spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the <br>query log</br> affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a <br>query log</br> spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the <br>query log</br>.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the <br>query log</br> to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the <br>query log</br> we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the <br>query log</br>, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our <br>query log</br>, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the <br>query log</br>.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the <br>query log</br>.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the <br>query log</br>, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the <br>query log</br> and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the <br>query log</br> and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the <br>query log</br> as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training <br>query log</br> but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our <br>query log</br>, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in <br>query log</br> distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "Using a <br>query log</br> spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "Finally, we measure how the changes in the <br>query log</br> affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "We use a <br>query log</br> spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the <br>query log</br>.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the <br>query log</br> to be 0.424."
            ],
            "translated_annotated_samples": [
                "Utilizando un <br>registro de consultas</br> que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas.",
                "Finalmente, medimos cómo los cambios en el <br>registro de consultas</br> afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo.",
                "Utilizamos un <br>registro de consultas</br> que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta.",
                "La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el <br>registro de consultas</br>.",
                "Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el <br>registro de consultas</br> como 0.424."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un <br>registro de consultas</br> que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el <br>registro de consultas</br> afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un <br>registro de consultas</br> que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el <br>registro de consultas</br>. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el <br>registro de consultas</br> como 0.424. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "effectiveness of static caching": {
            "translated_key": "almacenamiento en caché estático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the <br>effectiveness of static caching</br>, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "Finally, we measure how the changes in the query log affect the <br>effectiveness of static caching</br>, given our observation that the distribution of the queries changes slowly over time."
            ],
            "translated_annotated_samples": [
                "Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del <br>almacenamiento en caché estático</br>, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del <br>almacenamiento en caché estático</br>, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "static caching effectiveness": {
            "translated_key": "efectividad del almacenamiento en caché estático",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "distribution of the query": {
            "translated_key": "distribución de la consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed <br>distribution of the query</br> stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed <br>distribution of the query</br> stream, as shown later."
            ],
            "translated_annotated_samples": [
                "Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "the query distribution": {
            "translated_key": "la distribución de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of <br>the query distribution</br> over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in <br>the query distribution</br> on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As <br>the query distribution</br> is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, <br>the query distribution</br> and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of <br>the query distribution</br> over time have little impact on static caching.",
                "Section 7 discusses the impact of changes in <br>the query distribution</br> on static caching, and Section 8 provides concluding remarks. 2.",
                "As <br>the query distribution</br> is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "EFFECT OF THE QUERY DYNAMICS For our query log, <br>the query distribution</br> and query-term distribution change slowly over time."
            ],
            "translated_annotated_samples": [
                "Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en <br>la distribución de consultas</br> con el tiempo tienen poco impacto en el almacenamiento en caché estático.",
                "La sección 7 discute el impacto de los cambios en <br>la distribución de consultas</br> en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales.",
                "Dado que <br>la distribución de consultas</br> es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα.",
                "EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la <br>distribución de consultas</br> y la distribución de términos de consulta cambian lentamente con el tiempo."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en <br>la distribución de consultas</br> con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en <br>la distribución de consultas</br> en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que <br>la distribución de consultas</br> es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la <br>distribución de consultas</br> y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                [
                    "la distribución de consultas",
                    "la distribución de consultas",
                    "la distribución de consultas",
                    "distribución de consultas"
                ]
            ]
        },
        "data-access hierarchy": {
            "translated_key": "jerarquía de acceso a los datos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the <br>data-access hierarchy</br>, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the <br>data-access hierarchy</br>.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "Our results and observations are applicable to different levels of the <br>data-access hierarchy</br>, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Our analysis takes into account the impact of caching between two levels of the <br>data-access hierarchy</br>."
            ],
            "translated_annotated_samples": [
                "Nuestros resultados y observaciones son aplicables a diferentes niveles de la <br>jerarquía de acceso a los datos</br>, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker.",
                "Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la <br>jerarquía de acceso a datos</br>."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la <br>jerarquía de acceso a los datos</br>, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la <br>jerarquía de acceso a datos</br>. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                [
                    "jerarquía de acceso a los datos",
                    "jerarquía de acceso a datos"
                ]
            ]
        },
        "disk layer": {
            "translated_key": "capa de memoria/disco",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/<br>disk layer</br> or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/<br>disk layer</br> or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/<br>disk layer</br> or a broker/remote server layer.",
                "It can either be applied at the memory/<br>disk layer</br> or at a server/remote server layer as in the architecture we discussed in the introduction."
            ],
            "translated_annotated_samples": [
                "Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una <br>capa de memoria/disco</br> o una capa de servidor remoto/broker.",
                "Puede aplicarse en la <br>capa de memoria/disco</br> o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una <br>capa de memoria/disco</br> o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la <br>capa de memoria/disco</br> o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "remote server layer": {
            "translated_key": "capa de servidor remoto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/<br>remote server layer</br>.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/<br>remote server layer</br> as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/<br>remote server layer</br>.",
                "It can either be applied at the memory/disk layer or at a server/<br>remote server layer</br> as in the architecture we discussed in the introduction."
            ],
            "translated_annotated_samples": [
                "Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una <br>capa de servidor remoto</br>/broker.",
                "Puede aplicarse en la capa de memoria/disco o en la <br>capa de servidor/servidor remoto</br>, como en la arquitectura que discutimos en la introducción."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una <br>capa de servidor remoto</br>/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la <br>capa de servidor/servidor remoto</br>, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. \n\nJohn Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. \n\nAlgoritmic, 33(3):371-383, 2002. ",
            "candidates": [],
            "error": [
                [
                    "capa de servidor remoto",
                    "capa de servidor/servidor remoto"
                ]
            ]
        },
        "cache": {
            "translated_key": "caché",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static <br>cache</br> between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a <br>cache</br> is crucial.",
                "The primary use of a <br>cache</br> memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to <br>cache</br> is either off-line (static) or online (dynamic).",
                "A static <br>cache</br> is based on historical information and is periodically updated.",
                "A dynamic <br>cache</br> replaces entries according to the sequence of requests.",
                "When a new request arrives, the <br>cache</br> system decides whether to evict some entry from the <br>cache</br> in the case of a cache miss.",
                "Such online decisions are based on a <br>cache</br> policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a <br>cache</br> memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the <br>cache</br> is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to <br>cache</br> one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each <br>cache</br> level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static <br>cache</br>, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a <br>cache</br>-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level <br>cache</br> can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to <br>cache</br>.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a <br>cache</br> memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be <br>cache</br> hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the <br>cache</br> receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the <br>cache</br> uses.",
                "If we consider a <br>cache</br> with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite <br>cache</br> there are no capacity misses.",
                "As we mentioned before, another possibility is to <br>cache</br> the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the <br>cache</br> content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic <br>cache</br> on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU <br>cache</br>. after going through an LRU cache.",
                "On a <br>cache</br> miss, an LRU <br>cache</br> decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the <br>cache</br>.",
                "It is possible that queries that are most frequent after the <br>cache</br> have different characteristics, and tuning the search engine to queries frequent before the <br>cache</br> may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the <br>cache</br>, thus showing that the <br>cache</br> is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-<br>cache</br> frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is <br>cache</br> miss rate.",
                "To analyze the <br>cache</br> miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the <br>cache</br> memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers <br>cache</br> to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the <br>cache</br> because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static <br>cache</br> corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the <br>cache</br> the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The <br>cache</br> size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the <br>cache</br> with terms in order of fq(t) and we let the <br>cache</br> warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate <br>cache</br> size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static <br>cache</br> is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the <br>cache</br> often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to <br>cache</br> query answers or <br>cache</br> posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the <br>cache</br> measured in answer units (the <br>cache</br> can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a <br>cache</br> that stores only precomputed answers, and (B) a <br>cache</br> that stores only posting lists.",
                "In the first case, Nc = M answers fit in the <br>cache</br>, while in the second case Np = M/L posting lists fit in the <br>cache</br>.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the <br>cache</br> can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the <br>cache</br> then the results can be computed in TR1 time units, while if the posting lists are not in the <br>cache</br> then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large <br>cache</br>, β → 1.",
                "That is, both techniques will <br>cache</br> a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small <br>cache</br>, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the <br>cache</br> only with answers or only with posting lists, a better strategy will be to divide the total <br>cache</br> space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the <br>cache</br>.",
                "As the answer <br>cache</br> is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the <br>cache</br> in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal <br>cache</br> trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: <br>cache</br> saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the <br>cache</br> the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to <br>cache</br> posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the <br>cache</br> in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the <br>cache</br> when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the <br>cache</br> at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static <br>cache</br> of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static <br>cache</br> is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static <br>cache</br> holding 128,000 answers during the period of a week.",
                "The static <br>cache</br> of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static <br>cache</br> we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the <br>cache</br> too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to <br>cache</br>, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to <br>cache</br> query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate <br>cache</br> size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "We also study the problem of finding the optimal way to split the static <br>cache</br> between answers and posting lists.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a <br>cache</br> is crucial.",
                "The primary use of a <br>cache</br> memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "The decision of what to <br>cache</br> is either off-line (static) or online (dynamic).",
                "A static <br>cache</br> is based on historical information and is periodically updated."
            ],
            "translated_annotated_samples": [
                "También estudiamos el problema de encontrar la manera óptima de dividir la <br>caché</br> estática entre respuestas y listas de publicaciones.",
                "En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una <br>caché</br>.",
                "El uso principal de una <br>memoria caché</br> es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo.",
                "La decisión de qué <br>cachear</br> puede ser fuera de línea (estático) u en línea (dinámico).",
                "Una <br>caché</br> estática se basa en información histórica y se actualiza periódicamente."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la <br>caché</br> estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una <br>caché</br>. El uso principal de una <br>memoria caché</br> es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué <br>cachear</br> puede ser fuera de línea (estático) u en línea (dinámico). Una <br>caché</br> estática se basa en información histórica y se actualiza periódicamente. ",
            "candidates": [],
            "error": [
                [
                    "caché",
                    "caché",
                    "memoria caché",
                    "cachear",
                    "caché"
                ]
            ]
        },
        "web search": {
            "translated_key": "búsqueda en la web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for <br>web search</br> engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to <br>web search</br> engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for <br>web search</br> engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of <br>web search</br> engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large <br>web search</br> engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for <br>web search</br> engines.",
                "INTRODUCTION Millions of queries are submitted daily to <br>web search</br> engines, and users have high expectations of the quality and speed of the answers.",
                "Saraiva et al. propose a new architecture for <br>web search</br> engines using a two-level dynamic caching system [13].",
                "Boosting the performance of <br>web search</br> engines: Caching and prefetching query results by exploiting historical usage data.",
                "Three-level caching for efficient query processing in large <br>web search</br> engines."
            ],
            "translated_annotated_samples": [
                "En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para <br>motores de búsqueda web</br>.",
                "INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de <br>búsqueda en la web</br>, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas.",
                "Saraiva et al. proponen una nueva arquitectura para motores de <br>búsqueda web</br> utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13].",
                "Mejorando el rendimiento de los motores de <br>búsqueda web</br>: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico.",
                "Caché de tres niveles para un procesamiento eficiente de consultas en motores de <br>búsqueda web</br> grandes."
            ],
            "translated_text": "El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para <br>motores de búsqueda web</br>. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de <br>búsqueda en la web</br>, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de <br>búsqueda web</br> utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.\n\nTraducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de <br>búsqueda web</br>: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de <br>búsqueda web</br> grandes. ",
            "candidates": [],
            "error": [
                [
                    "motores de búsqueda web",
                    "búsqueda en la web",
                    "búsqueda web",
                    "búsqueda web",
                    "búsqueda web"
                ]
            ]
        },
        "information retrieval system": {
            "translated_key": "sistema de recuperación de información",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}