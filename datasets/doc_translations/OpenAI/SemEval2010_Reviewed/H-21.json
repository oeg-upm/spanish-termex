{
    "id": "H-21",
    "original_text": "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo! Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine. We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1. INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. One thing, however, has remained constant: people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information. Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient. Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results. For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries. In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes. Given such classifications, one can directly use them to provide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear. Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages. Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification. Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time. Another important aspect of our work lies in the choice of queries. The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on. However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such aggregation is to classify the queries into a topical taxonomy. Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial. Early studies in query interpretation focused on query augmentation through external dictionaries [22]. More recent studies [18, 21] also attempted to gather some additional knowledge from the Web. However, these studies had a number of shortcomings, which we overcome in this paper. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11]. They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18]. The main contributions of this paper are as follows. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development. The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages). We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2. METHODOLOGY Our methodology has two main phases. In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries. However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified. In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1). Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1. Given a taxonomy of this size, the computational efficiency of classification is a major issue. Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples. Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers. A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields. Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine. The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document. Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q). Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query. We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query. This is the case for the majority of queries that are unambiguous. Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words. In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds. Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q). The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1). While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query. This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance. Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q. This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q. In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula. This relevance function is an adaptation of the traditional word-based retrieval rules. For example, we may let categories be the words in the vocabulary. We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj. With such choices, the method given by (1) becomes the standard TFIDF retrieval rule. If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). That is, the ads are ranked according to P(q|a). This relevance model has been employed in various statistical language modeling techniques for information retrieval. The intuition can be described as follows. We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj. For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad. It should be mentioned that in our case, each query and ad can have multiple categories. For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj. We use P(Cj|q) to denote the probability of q belonging to category Cj. Here the sum Cj ∈C P(Cj|q) may not equal to one. We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known). Thus, we only need to obtain estimates of P(Cj|q) for each query q. Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q. In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search. That is, top results ranked by search engines should also be ranked high by this formula. Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking. Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)). Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads. Nor does it affect query classification with appropriately chosen thresholds. In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter. The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale. In the experiment, we will simply take uniform weights wi. A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x. In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q. For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine. Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data. That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear. Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method. In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3. EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application. Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity. For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node. The ads appropriate for these two queries are, however, very different. To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics. Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9. Figure 1 shows the distribution of categories by taxonomy levels. Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising. Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query. All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency. These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query. A discussion of bidding and placement mechanisms is beyond the scope of this paper [13]. However, many searches do not explicitly use phrases that someone bids on. Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase. In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc. These transformations are based on rules and dictionaries. As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries. Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries). The first set of queries can be matched to at least one ad using broad match as described above. Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them. In a sense, these are even more rare queries and further away from common queries. As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2. The queries in the two sets differ in their classification difficulty. In fact, queries in Set 2 are difficult to interpret even for human evaluators. Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words. Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets. As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words. Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2. Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators. These evaluators were trained editorial staff who possessed knowledge about the taxonomy. The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query. About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation. To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect. We used standard evaluation metrics: precision, recall and F1. In what follows, we plot precision-recall graphs for all the experiments. For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1). Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge. Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach. This baseline classifier is actually a production version of the query classifier running in a major US search engine. In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results. In what follows, we start with the general assessment of the effect of using Web search results. We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs. We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result. For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify. We use top search engine results for collecting background knowledge for queries. We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages. Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy. Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used. Engine Context Prec. F1 Prec. F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge. First, individual results can be classified separately, with subsequent voting among individual classifications. Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier. Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin. However, in the case of summaries, bundling together is found to be consistently better than individual classification. This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results. Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together. The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification. This observation differs from findings by Shen et al. [20], who found summaries to be more useful. We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes. Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings. As can be readily seen, all three variants produce very similar results. However, the precision-recall curve for the 1-class experiment has higher fluctuations. Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth. Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query. Figure 5 and Table 2 present the results of this experiment. In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50). This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise. Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline. We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model. As we have seen, the voting method works quite well. In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine. We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries. The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d). Method B requires a training/testing split. Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods. For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation. The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG. The decaying choice of log2(i + 1) is conventional, which does not have particular importance. The overall DCG of a system is the averaged DCG over queries. We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output. Therefore as a single metric, it is convenient for comparing the methods. Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers. Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3. The oracle method is the best ranking of categories for each query after seeing human judgments. It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance. The simple voting method performs very well in our experiments. The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5). However, both methods are computationally more costly, and the potential gain is minor enough to be neglected. This means that as a simple method, voting is quite effective. We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement. This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results. It may be possible to improve this method by including other page-features that can differentiate top-ranked search results. However, the effectiveness will require further investigation which we did not test. We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries). One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4. RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information. One important direction in enhancing queries is through query expansion. This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results. Early work in information retrieval concentrated on manually reviewing the returned results [16, 15]. However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14]. More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval. Studies in the field pursue different approaches for obtaining additional information about the queries. Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2]. Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global. The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21]. The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier. Several teams used the Web to enrich the queries and provide more context for classification. The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications. The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines. To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier. The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes. A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification. Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2. This simplifies the process and removes the need for mapping between taxonomies. This also streamlines taxonomy maintenance and development. Using this approach, we were able to achieve good performance in a very large scale taxonomy. We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query. In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies. In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy. For this, an intermediate taxonomy with a training set (ODP) is used. Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy. As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy. While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5. CONCLUSIONS Query classification is an important information retrieval task. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching. Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy. To address this problem, we proposed a methodology for using search results as a source of external knowledge. To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query. Classifying these results then allows us to classify the original query with substantially higher accuracy. The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification. Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy. Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works. We also experimented with different values of parameters that characterize our method. When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge. Overall, query classification performance was the best when using the full crawled pages (Table 1). These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries. Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries. In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output. Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages. We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications. For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous. When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole. We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages. The best results were obtained when using 40 top search hits. In this work, we first classify search results, and then use their classifications directly to classify the original query. Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier. In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages. We plan to further investigate this direction in our future work. It is also essential to note that implementing our methodology incurs little overhead. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting. To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries. This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements. In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6. REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz. Automatic web query classification using labeled and unlabeled training data. In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz. Improving automatic query classification via semi-supervised learning. In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart. Pattern Classification and Scene Analysis. John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron. UCLA-Okapi at TREC-2: Query expansion experiments. In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch. Feature generation for text categorization using world knowledge. In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein. Categorizing web queries according to geographical locality. In CIKM03, 2003. [7] E. Han and G. Karypis. Centroid-based document classification: Analysis and experimental results. In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi. The ferrety algorithm for the KDD Cup 2005 problem. In SIGKDD Explorations, volume 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann. Analyzing the effect of query class on document retrieval performance. In Proc. Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai. KDD CUP-2005 report: Facing a great challenge. In SIGKDD Explorations, volume 7, pages 91-99. ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley. Improving automatic query expansion. In SIGIR98, pages 206-214, 1998. [13] M. Moran and B. Hunt. Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In TREC-3, 1995. [15] J. Rocchio. Relevance feedback in information retrieval. In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323. Prentice Hall, 1971. [16] G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy. The Statistical Analysis of Discrete Data. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and Q. Yang. Q2C@UST: Our winning solution to query classification in KDDCUP 2005. In SIGKDD Explorations, volume 7, pages 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and Q. Yang. Query enrichment for web-query classification. ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building bridges for web query classification. In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer. Classifying search engine queries using the web as background knowledge. In SIGKDD Explorations, volume 7. ACM, 2005. [22] E. Voorhees. Query expansion using lexical-semantic relations. In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft. Improving the effectiveness of information retrieval with local context analysis. ACM TOIS, 18(1):79-112, 2000.",
    "original_translation": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000.",
    "original_sentences": [
        "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
        "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
        "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
        "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
        "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
        "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
        "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
        "One thing, however, has remained constant: people use very short queries.",
        "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
        "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
        "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
        "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
        "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
        "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
        "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
        "The problem of query classification is extremely difficult owing to the brevity of queries.",
        "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
        "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
        "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
        "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
        "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
        "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
        "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
        "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
        "We crawl the Web pages pointed by these URLs, and classify these pages.",
        "Finally, we use these result-page classifications to classify the original query.",
        "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
        "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
        "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
        "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
        "Another important aspect of our work lies in the choice of queries.",
        "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
        "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
        "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
        "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
        "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
        "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
        "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
        "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
        "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
        "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
        "However, these studies had a number of shortcomings, which we overcome in this paper.",
        "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
        "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
        "The main contributions of this paper are as follows.",
        "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
        "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
        "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
        "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
        "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
        "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
        "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
        "METHODOLOGY Our methodology has two main phases.",
        "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
        "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
        "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
        "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
        "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
        "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
        "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
        "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
        "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
        "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
        "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
        "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
        "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
        "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
        "This is the case for the majority of queries that are unambiguous.",
        "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
        "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
        "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
        "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
        "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
        "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
        "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
        "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
        "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
        "This relevance function is an adaptation of the traditional word-based retrieval rules.",
        "For example, we may let categories be the words in the vocabulary.",
        "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
        "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
        "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
        "That is, the ads are ranked according to P(q|a).",
        "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
        "The intuition can be described as follows.",
        "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
        "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
        "It should be mentioned that in our case, each query and ad can have multiple categories.",
        "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
        "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
        "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
        "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
        "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
        "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
        "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
        "That is, top results ranked by search engines should also be ranked high by this formula.",
        "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
        "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
        "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
        "Nor does it affect query classification with appropriately chosen thresholds.",
        "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
        "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
        "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
        "In the experiment, we will simply take uniform weights wi.",
        "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
        "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
        "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
        "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
        "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
        "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
        "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
        "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
        "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
        "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
        "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
        "The ads appropriate for these two queries are, however, very different.",
        "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
        "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
        "Figure 1 shows the distribution of categories by taxonomy levels.",
        "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
        "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
        "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
        "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
        "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
        "However, many searches do not explicitly use phrases that someone bids on.",
        "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
        "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
        "These transformations are based on rules and dictionaries.",
        "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
        "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
        "The first set of queries can be matched to at least one ad using broad match as described above.",
        "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
        "In a sense, these are even more rare queries and further away from common queries.",
        "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
        "The queries in the two sets differ in their classification difficulty.",
        "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
        "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
        "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
        "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
        "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
        "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
        "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
        "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
        "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
        "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
        "We used standard evaluation metrics: precision, recall and F1.",
        "In what follows, we plot precision-recall graphs for all the experiments.",
        "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
        "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
        "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
        "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
        "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
        "In what follows, we start with the general assessment of the effect of using Web search results.",
        "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
        "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
        "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
        "We use top search engine results for collecting background knowledge for queries.",
        "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
        "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
        "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
        "Engine Context Prec.",
        "F1 Prec.",
        "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
        "First, individual results can be classified separately, with subsequent voting among individual classifications.",
        "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
        "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
        "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
        "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
        "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
        "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
        "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
        "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
        "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
        "As can be readily seen, all three variants produce very similar results.",
        "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
        "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
        "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
        "Figure 5 and Table 2 present the results of this experiment.",
        "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
        "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
        "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
        "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
        "As we have seen, the voting method works quite well.",
        "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
        "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
        "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
        "Method B requires a training/testing split.",
        "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
        "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
        "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
        "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
        "The overall DCG of a system is the averaged DCG over queries.",
        "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
        "Therefore as a single metric, it is convenient for comparing the methods.",
        "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
        "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
        "The oracle method is the best ranking of categories for each query after seeing human judgments.",
        "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
        "The simple voting method performs very well in our experiments.",
        "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
        "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
        "This means that as a simple method, voting is quite effective.",
        "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
        "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
        "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
        "However, the effectiveness will require further investigation which we did not test.",
        "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
        "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
        "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
        "Consequently, many researchers studied possible ways to enhance queries with additional information.",
        "One important direction in enhancing queries is through query expansion.",
        "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
        "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
        "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
        "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
        "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
        "Studies in the field pursue different approaches for obtaining additional information about the queries.",
        "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
        "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
        "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
        "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
        "Several teams used the Web to enrich the queries and provide more context for classification.",
        "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
        "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
        "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
        "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
        "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
        "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
        "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
        "This simplifies the process and removes the need for mapping between taxonomies.",
        "This also streamlines taxonomy maintenance and development.",
        "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
        "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
        "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
        "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
        "For this, an intermediate taxonomy with a training set (ODP) is used.",
        "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
        "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
        "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
        "CONCLUSIONS Query classification is an important information retrieval task.",
        "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
        "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
        "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
        "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
        "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
        "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
        "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
        "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
        "We also experimented with different values of parameters that characterize our method.",
        "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
        "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
        "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
        "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
        "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
        "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
        "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
        "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
        "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
        "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
        "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
        "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
        "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
        "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
        "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
        "The best results were obtained when using 40 top search hits.",
        "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
        "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
        "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
        "We plan to further investigate this direction in our future work.",
        "It is also essential to note that implementing our methodology incurs little overhead.",
        "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
        "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
        "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
        "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
        "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
        "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
        "Automatic web query classification using labeled and unlabeled training data.",
        "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
        "Improving automatic query classification via semi-supervised learning.",
        "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
        "Pattern Classification and Scene Analysis.",
        "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
        "UCLA-Okapi at TREC-2: Query expansion experiments.",
        "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
        "Feature generation for text categorization using world knowledge.",
        "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
        "Categorizing web queries according to geographical locality.",
        "In CIKM03, 2003. [7] E. Han and G. Karypis.",
        "Centroid-based document classification: Analysis and experimental results.",
        "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
        "IR evaluation methods for retrieving highly relevant documents.",
        "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
        "The ferrety algorithm for the KDD Cup 2005 problem.",
        "In SIGKDD Explorations, volume 7.",
        "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
        "Analyzing the effect of query class on document retrieval performance.",
        "In Proc.",
        "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
        "KDD CUP-2005 report: Facing a great challenge.",
        "In SIGKDD Explorations, volume 7, pages 91-99.",
        "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
        "Improving automatic query expansion.",
        "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
        "Hunt.",
        "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
        "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
        "Okapi at TREC-3.",
        "In TREC-3, 1995. [15] J. Rocchio.",
        "Relevance feedback in information retrieval.",
        "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
        "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
        "Improving retrieval performance by relevance feedback.",
        "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
        "The Statistical Analysis of Discrete Data.",
        "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
        "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
        "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
        "In SIGKDD Explorations, volume 7, pages 100-110.",
        "ACM, 2005. [19] D. Shen, R. Pan, J.",
        "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
        "Query enrichment for web-query classification.",
        "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
        "Sun, Q. Yang, and Z. Chen.",
        "Building bridges for web query classification.",
        "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
        "Classifying search engine queries using the web as background knowledge.",
        "In SIGKDD Explorations, volume 7.",
        "ACM, 2005. [22] E. Voorhees.",
        "Query expansion using lexical-semantic relations.",
        "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
        "Improving the effectiveness of information retrieval with local context analysis.",
        "ACM TOIS, 18(1):79-112, 2000."
    ],
    "translated_text_sentences": [
        "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo!",
        "Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial.",
        "Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta.",
        "Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda.",
        "La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente.",
        "Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1.",
        "INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web.",
        "Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas.",
        "Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información.",
        "Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes!",
        "Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario.",
        "Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda.",
        "Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares.",
        "En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos.",
        "Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados.",
        "El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas.",
        "Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido.",
        "Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo.",
        "Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara.",
        "Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento.",
        "Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta.",
        "Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta.",
        "Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta.",
        "Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas.",
        "Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas.",
        "Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original.",
        "Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas.",
        "Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación.",
        "Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación.",
        "Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución.",
        "Otro aspecto importante de nuestro trabajo radica en la elección de las consultas.",
        "El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces.",
        "Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas.",
        "Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más.",
        "Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta.",
        "Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas.",
        "Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática.",
        "Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes.",
        "Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa.",
        "Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22].",
        "Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web.",
        "Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo.",
        "Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11].",
        "También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18].",
        "Las principales contribuciones de este artículo son las siguientes.",
        "Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía.",
        "La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores.",
        "La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente.",
        "Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables.",
        "También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas).",
        "Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes.",
        "Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2.",
        "METODOLOGÍA Nuestra metodología tiene dos fases principales.",
        "En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas.",
        "Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas.",
        "En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1).",
        "Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1.",
        "Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante.",
        "Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento.",
        "Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7].",
        "Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja.",
        "Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda.",
        "El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento.",
        "La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q).",
        "Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta.",
        "Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
        "Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta.",
        "Este es el caso para la mayoría de las consultas que son inequívocas.",
        "Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes.",
        "En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría.",
        "Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
        "La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1).",
        "Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta.",
        "Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda).",
        "Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q.",
        "Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada.",
        "En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia.",
        "Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras.",
        "Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario.",
        "Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj.",
        "Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF.",
        "Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
        "Es decir, los anuncios se clasifican según P(q|a).",
        "Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información.",
        "La intuición se puede describir de la siguiente manera.",
        "Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj.",
        "Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio.",
        "Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías.",
        "Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj.",
        "Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj.",
        "Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno.",
        "Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce).",
        "Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q.",
        "La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q.",
        "Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda.",
        "Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula.",
        "Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación.",
        "Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)).",
        "Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios.",
        "Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos.",
        "En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste.",
        "La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
        "Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala.",
        "En el experimento, simplemente tomaremos pesos uniformes wi.",
        "Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x.",
        "En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q.",
        "Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente.",
        "Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos.",
        "Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal.",
        "Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
        "Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal.",
        "En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3.",
        "EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web.",
        "Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación.",
        "Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo.",
        "Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes.",
        "Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes.",
        "Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9.",
        "La Figura 1 muestra la distribución de las categorías por niveles taxonómicos.",
        "Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web.",
        "La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen.",
        "Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad.",
        "Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta).",
        "Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13].",
        "Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje.",
        "Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada.",
        "En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc.",
        "Estas transformaciones se basan en reglas y diccionarios.",
        "Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas.",
        "Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares).",
        "El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba.",
        "Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas.",
        "En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes.",
        "Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2.",
        "Las consultas en los dos conjuntos difieren en su dificultad de clasificación.",
        "De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos.",
        "Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras.",
        "Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas.",
        "Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras.",
        "Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2.",
        "Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta.",
        "Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía.",
        "Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta.",
        "Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación.",
        "Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas.",
        "Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1.",
        "En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos.",
        "Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1).",
        "Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo.",
        "Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano.",
        "Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos.",
        "En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda.",
        "En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web.",
        "Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas.",
        "También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda.",
        "Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar.",
        "Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas.",
        "Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas.",
        "La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación.",
        "Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes.",
        "Contexto del motor de precisión.",
        "F1 Prec.",
        "F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional.",
        "Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales.",
        "Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos.",
        "La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen.",
        "Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual.",
        "Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda.",
        "Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos.",
        "Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta.",
        "Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles.",
        "Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases.",
        "La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente.",
        "Como se puede ver fácilmente, las tres variantes producen resultados muy similares.",
        "Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones.",
        "Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave.",
        "Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta.",
        "La Figura 5 y la Tabla 2 presentan los resultados de este experimento.",
        "De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50).",
        "Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional.",
        "Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline.",
        "Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia.",
        "Como hemos visto, el método de votación funciona bastante bien.",
        "En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda.",
        "Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención.",
        "La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d).",
        "El método B requiere una división de entrenamiento/prueba.",
        "Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos.",
        "Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda.",
        "La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG.",
        "La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular.",
        "El DCG general de un sistema es el DCG promediado sobre las consultas.",
        "Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados.",
        "Por lo tanto, como una única métrica, es conveniente para comparar los métodos.",
        "Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG.",
        "Los resultados de nuestros experimentos se muestran en la Tabla 3.",
        "El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos.",
        "No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG.",
        "El método de votación simple funciona muy bien en nuestros experimentos.",
        "Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5).",
        "Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada.",
        "Esto significa que como método simple, votar es bastante efectivo.",
        "Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable.",
        "Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales.",
        "Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados.",
        "Sin embargo, la efectividad requerirá una investigación adicional que no realizamos.",
        "También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas).",
        "Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4.",
        "Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras.",
        "Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional.",
        "Una dirección importante para mejorar las consultas es a través de la expansión de consultas.",
        "Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación.",
        "Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15].",
        "Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14].",
        "Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada.",
        "De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos.",
        "Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas.",
        "Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2].",
        "Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global.",
        "La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21].",
        "La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas.",
        "Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación.",
        "Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos.",
        "La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda.",
        "Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP.",
        "La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales.",
        "Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico.",
        "Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas.",
        "En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2.",
        "Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías.",
        "Esto también simplifica el mantenimiento y desarrollo de la taxonomía.",
        "Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala.",
        "También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta.",
        "En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías.",
        "En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente.",
        "Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP).",
        "Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo.",
        "En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia.",
        "Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5.",
        "CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información.",
        "La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios.",
        "Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada.",
        "Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo.",
        "Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta.",
        "Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor.",
        "Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación.",
        "Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo.",
        "Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores.",
        "También experimentamos con diferentes valores de parámetros que caracterizan nuestro método.",
        "Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo.",
        "En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1).",
        "Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves.",
        "Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados.",
        "Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras.",
        "En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados.",
        "Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados.",
        "Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas.",
        "También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales.",
        "Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional.",
        "Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija).",
        "Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea.",
        "Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad.",
        "Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal.",
        "Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes.",
        "Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda.",
        "En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original.",
        "Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel.",
        "En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas.",
        "Planeamos investigar más en esta dirección en nuestro trabajo futuro.",
        "También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo.",
        "Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación.",
        "Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web.",
        "Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web.",
        "Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados.",
        "En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6.",
        "REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz.",
        "Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados.",
        "En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz.",
        "Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado.",
        "En Actas de ICDM05, 2005. [3] R. Duda y P. Hart.",
        "Clasificación de patrones y análisis de escenas.",
        "John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron.",
        "UCLA-Okapi en TREC-2: Experimentos de expansión de consultas.",
        "En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch.",
        "Generación de características para la categorización de texto utilizando conocimiento del mundo.",
        "En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein.",
        "Clasificación de consultas web según la localidad geográfica.",
        "En CIKM03, 2003. [7] E. Han y G. Karypis.",
        "Clasificación de documentos basada en el centroide: Análisis y resultados experimentales.",
        "En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen.",
        "Métodos de evaluación IR para recuperar documentos altamente relevantes.",
        "En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi.",
        "El algoritmo ferrety para el problema de la Copa KDD 2005.",
        "En SIGKDD Explorations, volumen 7.",
        "ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann.",
        "Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos.",
        "En Proc.",
        "Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai.",
        "Informe de la KDD CUP-2005: Enfrentando un gran desafío.",
        "En SIGKDD Explorations, volumen 7, páginas 91-99.",
        "ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley.",
        "Mejorando la expansión automática de consultas.",
        "En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B.",
        "Cazar.",
        "Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa.",
        "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford.",
        "Okapi en TREC-3.",
        "En TREC-3, 1995. [15] J. Rocchio.",
        "Retroalimentación de relevancia en la recuperación de información.",
        "En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323.",
        "Prentice Hall, 1971. [16] G. Salton y C. Buckley.",
        "Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia.",
        "JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy.",
        "El Análisis Estadístico de Datos Discretos.",
        "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
        "Sun, J. Pan, K. Wu, J. Yin y Q. Yang.",
        "Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005.",
        "En SIGKDD Explorations, volumen 7, páginas 100-110.",
        "ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J.",
        "Sun, J. Pan, K. Wu, J. Yin y Q. Yang.",
        "Enriquecimiento de consultas para la clasificación de consultas web.",
        "ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J.",
        "Sun, Q. Yang y Z. Chen.",
        "Construyendo puentes para la clasificación de consultas web.",
        "En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer.",
        "Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo.",
        "En SIGKDD Explorations, volumen 7.",
        "ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees.",
        "Expansión de consulta utilizando relaciones léxico-semánticas.",
        "En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft.",
        "Mejorando la efectividad de la recuperación de información con análisis de contexto local.",
        "ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000."
    ],
    "error_count": 0,
    "keys": {
        "query classification": {
            "translated_key": "clasificación de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust <br>query classification</br> system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for <br>query classification</br>, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of <br>query classification</br> is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of <br>query classification</br>.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve <br>query classification</br> is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small <br>query classification</br> taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in <br>query classification</br> [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform <br>query classification</br>. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 <br>query classification</br> by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "<br>query classification</br> is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect <br>query classification</br> with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving <br>query classification</br>. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final <br>query classification</br>.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in <br>query classification</br>. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for <br>query classification</br>. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve <br>query classification</br>, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web <br>query classification</br> inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for <br>query classification</br>.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for <br>query classification</br> based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS <br>query classification</br> is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of <br>query classification</br> does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, <br>query classification</br> performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and <br>query classification</br> can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web <br>query classification</br> using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic <br>query classification</br> via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to <br>query classification</br> in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-<br>query classification</br>.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web <br>query classification</br>.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust <br>query classification</br> system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "In this study we present a methodology for <br>query classification</br>, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "The problem of <br>query classification</br> is extremely difficult owing to the brevity of queries.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of <br>query classification</br>.",
                "This additional overhead is minimal, and therefore the use of search results to improve <br>query classification</br> is entirely feasible in run-time."
            ],
            "translated_annotated_samples": [
                "Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de <br>clasificación de consultas</br> robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial.",
                "En este estudio presentamos una metodología para la <br>clasificación de consultas</br>, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos.",
                "El problema de la <br>clasificación de consultas</br> es extremadamente difícil debido a la brevedad de las consultas.",
                "Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la <br>clasificación de consultas</br>.",
                "Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la <br>clasificación de consultas</br> es totalmente factible en tiempo de ejecución."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de <br>clasificación de consultas</br> robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la <br>clasificación de consultas</br>, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la <br>clasificación de consultas</br> es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la <br>clasificación de consultas</br>. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la <br>clasificación de consultas</br> es totalmente factible en tiempo de ejecución. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "search engine": {
            "translated_key": "motor de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web <br>search engine</br>.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of <br>search engine</br> traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web <br>search engine</br>, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial <br>search engine</br>, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US <br>search engine</br> (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a <br>search engine</br>.",
                "The <br>search engine</br> can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major <br>search engine</br>, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the <br>search engine</br> results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying <br>search engine</br>.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a <br>search engine</br> and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the <br>search engine</br> used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US <br>search engine</br>; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US <br>search engine</br>.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top <br>search engine</br> results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that <br>search engine</br> A performs consistently better with full-page text, while <br>search engine</br> B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from <br>search engine</br> results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a <br>search engine</br>.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in <br>search engine</br> evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a <br>search engine</br> to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the <br>search engine</br> did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the <br>search engine</br>, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a <br>search engine</br>, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the <br>search engine</br>, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the <br>search engine</br>, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a <br>search engine</br>, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the <br>search engine</br> classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "<br>search engine</br> Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying <br>search engine</br> queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web <br>search engine</br>.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of <br>search engine</br> traffic.",
                "For the purpose of this study we first dispatch the given query to a general web <br>search engine</br>, and collect a number of the highest-scoring URLs.",
                "Note that in a practical implementation of our methodology within a commercial <br>search engine</br>, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US <br>search engine</br> (see Section 3.1)."
            ],
            "translated_annotated_samples": [
                "Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un <br>motor de búsqueda</br> web comercial.",
                "Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de <br>motores de búsqueda</br>.",
                "Con el propósito de este estudio, primero enviamos la consulta proporcionada a un <br>motor de búsqueda web</br> general y recopilamos una serie de URL con las puntuaciones más altas.",
                "Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un <br>motor de búsqueda</br> comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación.",
                "En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante <br>motor de búsqueda</br> de EE. UU. (ver Sección 3.1)."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un <br>motor de búsqueda</br> web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de <br>motores de búsqueda</br>. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un <br>motor de búsqueda web</br> general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un <br>motor de búsqueda</br> comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante <br>motor de búsqueda</br> de EE. UU. (ver Sección 3.1). ",
            "candidates": [],
            "error": [
                [
                    "motor de búsqueda",
                    "motores de búsqueda",
                    "motor de búsqueda web",
                    "motor de búsqueda",
                    "motor de búsqueda"
                ]
            ]
        },
        "search advertising": {
            "translated_key": "publicidad en buscadores",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of <br>search advertising</br>, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "Motivated by the needs of <br>search advertising</br>, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic."
            ],
            "translated_annotated_samples": [
                "Motivados por las necesidades de la <br>publicidad en buscadores</br>, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la <br>publicidad en buscadores</br>, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "machine learning": {
            "translated_key": "aprendizaje automático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of <br>machine learning</br>, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few <br>machine learning</br> algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of <br>machine learning</br>, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Few <br>machine learning</br> algorithms can efficiently handle so many different classes, each having hundreds of training examples."
            ],
            "translated_annotated_samples": [
                "Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del <br>aprendizaje automático</br>, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda.",
                "Pocos algoritmos de <br>aprendizaje automático</br> pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del <br>aprendizaje automático</br>, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de <br>aprendizaje automático</br> pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "relevance feedback": {
            "translated_key": "retroalimentación de relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- <br>relevance feedback</br>, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo <br>relevance feedback</br> paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via <br>relevance feedback</br> techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind <br>relevance feedback</br>, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "<br>relevance feedback</br> in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by <br>relevance feedback</br>.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- <br>relevance feedback</br>, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "To this end, we employ the pseudo <br>relevance feedback</br> paradigm, and assume the top search results to be relevant to the query.",
                "This can be done either using electronic dictionaries and thesauri [22], or via <br>relevance feedback</br> techniques that make use of a few top-scoring search results.",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind <br>relevance feedback</br>, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "<br>relevance feedback</br> in information retrieval."
            ],
            "translated_annotated_samples": [
                "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - <br>retroalimentación de relevancia</br>, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1.",
                "Con este fin, empleamos el paradigma de <br>retroalimentación de relevancia pseudo</br> y asumimos que los resultados de búsqueda principales son relevantes para la consulta.",
                "Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de <br>retroalimentación de relevancia</br> que hacen uso de algunos de los resultados de búsqueda con mayor puntuación.",
                "Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la <br>retroalimentación de relevancia</br> ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14].",
                "Retroalimentación de relevancia en la recuperación de información."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - <br>retroalimentación de relevancia</br>, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de <br>retroalimentación de relevancia pseudo</br> y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de <br>retroalimentación de relevancia</br> que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la <br>retroalimentación de relevancia</br> ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. ",
            "candidates": [],
            "error": [
                [
                    "retroalimentación de relevancia",
                    "retroalimentación de relevancia pseudo",
                    "retroalimentación de relevancia",
                    "retroalimentación de relevancia"
                ]
            ]
        },
        "voting scheme": {
            "translated_key": "esquema de votación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate <br>voting scheme</br> among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "We attribute our observations to using a more elaborate <br>voting scheme</br> among the classifications of individual search results, as well as to using a more difficult set of rare queries."
            ],
            "translated_annotated_samples": [
                "Atribuimos nuestras observaciones al uso de un <br>esquema de votación</br> más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un <br>esquema de votación</br> más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "crawling": {
            "translated_key": "rastreo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any <br>crawling</br> or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that <br>crawling</br> the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually <br>crawling</br> the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "Thus, at run-time we only need to run the voting procedure, without doing any <br>crawling</br> or classification.",
                "We found that <br>crawling</br> the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually <br>crawling</br> the returned URLs."
            ],
            "translated_annotated_samples": [
                "Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún <br>rastreo</br> o clasificación.",
                "Descubrimos que <br>rastrear</br> los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes.",
                "Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de <br>rastrear</br> las URL devueltas."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún <br>rastreo</br> o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que <br>rastrear</br> los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de <br>rastrear</br> las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    "rastreo",
                    "rastrear",
                    "rastrear"
                ]
            ]
        },
        "topical taxonomy": {
            "translated_key": "taxonomía temática",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a <br>topical taxonomy</br>.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "A natural choice for such aggregation is to classify the queries into a <br>topical taxonomy</br>."
            ],
            "translated_annotated_samples": [
                "Una elección natural para tal agregación es clasificar las consultas en una <br>taxonomía temática</br>."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una <br>taxonomía temática</br>. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "affinity score": {
            "translated_key": "puntajes de afinidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such <br>affinity score</br>s used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such <br>affinity score</br>s used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document."
            ],
            "translated_annotated_samples": [
                "El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales <br>puntajes de afinidad</br> utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales <br>puntajes de afinidad</br> utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "conditional probability": {
            "translated_key": "probabilidad condicional",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the <br>conditional probability</br> of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The <br>conditional probability</br> of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually <br>conditional probability</br> g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "Our goal is to estimate the <br>conditional probability</br> of each possible class using the search results initially returned by the query.",
                "The <br>conditional probability</br> of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually <br>conditional probability</br> g(P(Cj|q))."
            ],
            "translated_annotated_samples": [
                "Nuestro objetivo es estimar la <br>probabilidad condicional</br> de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta.",
                "La <br>probabilidad condicional</br> de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1).",
                "Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la <br>probabilidad condicional</br> real g(P(Cj|q))."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la <br>probabilidad condicional</br> de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La <br>probabilidad condicional</br> de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la <br>probabilidad condicional</br> real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "adaptation": {
            "translated_key": "adaptación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an <br>adaptation</br> of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "This relevance function is an <br>adaptation</br> of the traditional word-based retrieval rules."
            ],
            "translated_annotated_samples": [
                "Esta función de relevancia es una <br>adaptación</br> de las reglas tradicionales de recuperación basadas en palabras."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una <br>adaptación</br> de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for <br>information retrieval</br>.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in <br>information retrieval</br> concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important <br>information retrieval</br> task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in <br>information retrieval</br>.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of <br>information retrieval</br> with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "This relevance model has been employed in various statistical language modeling techniques for <br>information retrieval</br>.",
                "Early work in <br>information retrieval</br> concentrated on manually reviewing the returned results [16, 15].",
                "CONCLUSIONS Query classification is an important <br>information retrieval</br> task.",
                "Relevance feedback in <br>information retrieval</br>.",
                "Improving the effectiveness of <br>information retrieval</br> with local context analysis."
            ],
            "translated_annotated_samples": [
                "Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la <br>recuperación de información</br>.",
                "Los primeros trabajos en <br>recuperación de información</br> se centraron en revisar manualmente los resultados devueltos [16, 15].",
                "CONCLUSIONES La clasificación de consultas es una tarea importante de <br>recuperación de información</br>.",
                "Retroalimentación de relevancia en la <br>recuperación de información</br>.",
                "Mejorando la efectividad de la <br>recuperación de información</br> con análisis de contexto local."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la <br>recuperación de información</br>. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en <br>recuperación de información</br> se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la retroalimentación de relevancia ciega, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de <br>recuperación de información</br>. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la <br>recuperación de información</br>. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la <br>recuperación de información</br> con análisis de contexto local. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web search": {
            "translated_key": "búsqueda web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial <br>web search</br> engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the <br>web search</br> results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, <br>web search</br> had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of <br>web search</br>, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general <br>web search</br> engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using <br>web search</br> results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses <br>web search</br> results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of <br>web search</br> engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using <br>web search</br> results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using <br>web search</br> engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as <br>web search</br> and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using <br>web search</br> results holds considerable promise for substantially improving the accuracy of <br>web search</br> queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial <br>web search</br> engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the <br>web search</br> results retrieved by the query.",
                "INTRODUCTION In its 12 year lifetime, <br>web search</br> had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of <br>web search</br>, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For the purpose of this study we first dispatch the given query to a general <br>web search</br> engine, and collect a number of the highest-scoring URLs."
            ],
            "translated_annotated_samples": [
                "Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un <br>motor de búsqueda web</br> comercial.",
                "Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de <br>búsqueda web</br> recuperados por la consulta.",
                "INTRODUCCIÓN En sus 12 años de existencia, la <br>búsqueda en la web</br> ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web.",
                "Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la <br>búsqueda en la web</br>, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda.",
                "Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de <br>búsqueda web</br> general y recopilamos una serie de URL con las puntuaciones más altas."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un <br>motor de búsqueda web</br> comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de <br>búsqueda web</br> recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la <br>búsqueda en la web</br> ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la <br>búsqueda en la web</br>, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de <br>búsqueda web</br> general y recopilamos una serie de URL con las puntuaciones más altas. ",
            "candidates": [],
            "error": [
                [
                    "motor de búsqueda web",
                    "búsqueda web",
                    "búsqueda en la web",
                    "búsqueda en la web",
                    "búsqueda web"
                ]
            ]
        },
        "blind relevance feedback": {
            "translated_key": "retroalimentación de relevancia ciega",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on <br>blind relevance feedback</br>, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on <br>blind relevance feedback</br>, which basically assumes top returned results to be relevant [23, 12, 4, 14]."
            ],
            "translated_annotated_samples": [
                "Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la <br>retroalimentación de relevancia ciega</br>, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]."
            ],
            "translated_text": "Clasificación robusta de consultas raras utilizando conocimiento web de Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang de Yahoo! Investigación, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com RESUMEN Proponemos una metodología para construir un sistema de clasificación de consultas robusto y práctico que pueda identificar miles de clases de consultas con una precisión razonable, mientras maneja en tiempo real el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivados por las necesidades de la publicidad en buscadores, nos enfocamos principalmente en consultas raras, que son las más difíciles desde el punto de vista del aprendizaje automático, pero que en conjunto representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la reportada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de los anuncios en línea con consultas poco comunes y, en general, a una mejor experiencia del usuario. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - retroalimentación de relevancia, proceso de búsqueda Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN En sus 12 años de existencia, la búsqueda en la web ha crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo en una industria de ocho mil millones de dólares impulsada por la publicidad en la web. Sin embargo, una cosa ha permanecido constante: las personas utilizan consultas muy cortas. Varios estudios estiman que la longitud promedio de una consulta de búsqueda es de entre 2.4 y 2.7 palabras, lo que, según todos los informes, solo puede contener una pequeña cantidad de información. Los motores de búsqueda comerciales hacen un trabajo notablemente bueno al interpretar estas cadenas cortas, ¡pero aún no son omniscientes! Por lo tanto, utilizar conocimientos externos adicionales para complementar las consultas puede ser de gran ayuda para mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar el sustento económico de la búsqueda en la web, es decir, la publicidad en línea, a través del mecanismo de búsqueda patrocinada que coloca anuncios relevantes junto a los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se refiere a cámaras mientras que nc4200 se refiere a computadoras portátiles puede llevar claramente a anuncios más enfocados, incluso si ningún anunciante ha pujado específicamente por estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, se pueden utilizar directamente para ofrecer mejores resultados de búsqueda y anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Sin embargo, observe que en muchos casos, una persona que observa una consulta de búsqueda y los resultados de la búsqueda lo hace notablemente bien al darle sentido. Por supuesto, el simple volumen de consultas de búsqueda no se presta para la supervisión humana, por lo que necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre cámaras Canon, mientras que nc4200 trae páginas sobre computadoras portátiles Compaq, por lo tanto, para un humano la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y por lo tanto pueden ser vistos como repositorios muy completos de conocimiento. Siguiendo la heurística descrita anteriormente, proponemos utilizar los propios resultados de búsqueda para obtener información adicional para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de relevancia pseudo y asumimos que los resultados de búsqueda principales son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes, por lo que utilizamos esquemas de votación elaborados para obtener conocimiento confiable sobre la consulta. Con el propósito de este estudio, primero enviamos la consulta proporcionada a un motor de búsqueda web general y recopilamos una serie de URL con las puntuaciones más altas. Rastreamos las páginas web señaladas por estos URLs y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda en la web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Ten en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas pueden ser preclasificadas utilizando el procesamiento de texto normal y el pipeline de indexación. Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin realizar ningún rastreo o clasificación. Este gasto adicional es mínimo, por lo tanto, el uso de los resultados de búsqueda para mejorar la clasificación de consultas es totalmente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de las consultas. El volumen de consultas en los motores de búsqueda de hoy en día sigue la conocida ley de potencias, donde unas pocas consultas aparecen con mucha frecuencia mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de dichas consultas brinda una oportunidad sustancial de ingresos a través de la publicidad en línea. Las plataformas de búsqueda y publicidad pueden ser entrenadas para obtener buenos resultados en consultas frecuentes, incluyendo datos auxiliares como mapas, accesos directos a información estructurada relacionada, anuncios exitosos, y más. Sin embargo, las consultas de cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico a nivel de consulta. Por lo tanto, necesitamos agrupar estas consultas de alguna manera y razonar a nivel de clusters de consultas agregadas. Una elección natural para tal agregación es clasificar las consultas en una taxonomía temática. Conocer qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a brindar el mismo tipo de soporte para consultas raras que para consultas frecuentes. Por consiguiente, en este trabajo nos enfocamos en la clasificación de consultas raras, cuya correcta clasificación probablemente sea particularmente beneficiosa. Los primeros estudios en la interpretación de consultas se centraron en la ampliación de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron recopilar conocimientos adicionales de la Web. Sin embargo, estos estudios tenían varias deficiencias, las cuales superamos en este artículo. Específicamente, trabajos anteriores en el campo utilizaban taxonomías de clasificación de consultas muy pequeñas, con solo unas pocas docenas de nodos, lo que no permite la suficiente especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para los documentos web, por lo que se tuvo que emplear un nivel adicional de indirección para establecer la correspondencia entre la taxonomía auxiliar y la principal [18]. Las principales contribuciones de este artículo son las siguientes. Primero, construimos el clasificador de consultas directamente para la taxonomía objetivo, en lugar de utilizar una estructura auxiliar secundaria; esto simplifica enormemente el mantenimiento y desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grande que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para utilizar conocimiento externo logra mayores mejoras que las reportadas previamente. Dado que nuestra taxonomía es considerablemente más grande, el problema de clasificación al que nos enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos sobre los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes niveles de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas completas rastreadas). Descubrimos que rastrear los resultados de búsqueda proporciona un conocimiento más profundo y conduce a mayores mejoras que simples resúmenes. Este resultado contrasta con hallazgos previos en la clasificación de consultas [20], pero está respaldado por investigaciones en la clasificación de texto convencional [5]. 2. METODOLOGÍA Nuestra metodología tiene dos fases principales. En la primera fase, en los ejemplos anteriores, SD450 y nc4200 representan modelos de dispositivos bastante antiguos, por lo tanto, hay anunciantes colocando anuncios en estas consultas. Sin embargo, en este documento nos ocupamos principalmente de consultas raras que son extremadamente difíciles de asociar con anuncios relevantes. Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán las consultas. En la segunda fase, desarrollamos un clasificador de consultas que invoca al clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas. 2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizada en un importante motor de búsqueda de EE. UU. (ver Sección 3.1). Los editores humanos poblaron los nodos de la taxonomía con ejemplos etiquetados que utilizamos como instancias de entrenamiento para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar eficientemente tantas clases diferentes, cada una con cientos de ejemplos de entrenamiento. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como métodos de formación de prototipos como Rocchio [15] o clasificadores basados en centroides [7]. Un estudio reciente [5] demostró que los clasificadores basados en centroides son efectivos y eficientes para taxonomías a gran escala y, en consecuencia, utilizamos un clasificador de centroides en este trabajo. 2.2 Clasificación de consultas por búsqueda. Después de haber desarrollado un clasificador de documentos para la taxonomía de consultas, ahora abordamos el problema de obtener una clasificación para una consulta dada basada en los resultados iniciales de la búsqueda que arroja. Supongamos que hay un conjunto de documentos D = d1 . . . dm indexados por un motor de búsqueda. El motor de búsqueda puede ser representado por una función f = similitud(q, d) que cuantifica la afinidad entre una consulta q y un documento d. Ejemplos de tales puntajes de afinidad utilizados en este documento son: rango: el rango del documento en la lista ordenada de resultados de búsqueda; puntaje estático: la calidad de la página independientemente de la consulta (por ejemplo, PageRank); y puntaje dinámico: la cercanía entre la consulta y el documento. La clasificación de la consulta se determina primero evaluando las probabilidades condicionales de todas las clases posibles P(Cj|q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = arg maxCj ∈C P(Cj|q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda devueltos inicialmente por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). Suponemos que P(q|Cj, d) ≈ P(q|d), es decir, la probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso para la mayoría de las consultas que son inequívocas. Contraejemplos son consultas como jaguar (animal y marca de automóvil) o apple (fruta y fabricante de computadoras), pero tales consultas ambiguas no pueden ser clasificadas por definición, y generalmente consisten en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras poco comunes, ser más largas y coincidir con menos documentos; por lo tanto, en nuestro contexto esta suposición se cumple en su mayoría. Usando esta suposición, podemos escribir P(Cj|q) = d∈D P(Cj|d)· P(d|q). La probabilidad condicional de una clasificación para un documento dado P(Cj|d) se estima utilizando la salida del clasificador de documentos (sección 2.1). Si bien P(d|q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este tema se explora más a fondo en la siguiente sección. Modelo de relevancia basado en la clasificación 2.3 Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para utilizar la clasificación para determinar la relevancia de los anuncios (o búsqueda). Sea a un anuncio y q una consulta, denotamos por R(a, q) la relevancia de a para q. Este número indica qué tan relevante es el anuncio a para la consulta q, y puede ser utilizado para clasificar los anuncios a para una consulta q dada. En este documento, consideramos la siguiente aproximación de la función de relevancia: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar anuncios, donde s(c, a) es una función de puntuación que especifica qué tan probable es a en la clase c, y s(c, q) es una función de puntuación que especifica qué tan probable es q en la clase c. El valor w(c) es un término de ponderación para la categoría c, indicando la importancia de la categoría c en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas tradicionales de recuperación basadas en palabras. Por ejemplo, podríamos dejar que las categorías sean las palabras en el vocabulario. Tomamos s(Cj, a) como el recuento de palabras de Cj en a, s(Cj, q) como el recuento de palabras de Cj en q, y w(Cj) como el peso de término IDF para la palabra Cj. Con tales opciones, el método proporcionado por (1) se convierte en la regla estándar de recuperación TFIDF. Si tomamos s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), y w(Cj) = 1/P(Cj), y asumimos que q y a son generados de forma independiente dado un concepto oculto C, entonces tenemos RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). Es decir, los anuncios se clasifican según P(q|a). Este modelo de relevancia ha sido empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio a través de la construcción de una consulta q: la persona primero elige un concepto Cj de acuerdo a los pesos P(Cj|a), y luego construye una consulta q con probabilidad P(q|Cj) basada en el concepto Cj. Para este proceso de generación de consultas, los anuncios pueden ser clasificados según la probabilidad de que la consulta observada sea generada a partir de cada anuncio. Debería mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por Cj una variable aleatoria que indica si q pertenece a la categoría Cj. Usamos P(Cj|q) para denotar la probabilidad de que q pertenezca a la categoría Cj. Aquí la suma Cj ∈C P(Cj|q) puede no ser igual a uno. Luego consideramos la siguiente fórmula de clasificación: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) Suponemos que la estimación de P(Cj|a) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P(Cj|q) para cada consulta q. La ecuación (2) es el modelo de relevancia de anuncio que consideramos en este artículo, con parámetros desconocidos P(Cj|q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., donde asumimos que la fórmula de clasificación en (2) proporciona una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deberían ser clasificados en lo alto por esta fórmula. Por lo tanto, dado una consulta q, y las páginas principales de resultados d1(q), . . . , dK (q) de un motor de búsqueda importante, ajustamos los parámetros P(Cj|q) de manera que RC (di(q), q) tengan puntajes altos para i = 1, . . . , K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P(Cj|q), pero no la escala, ya que la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma g(P(Cj|q)) para alguna función monótona g(·) de la probabilidad condicional real g(P(Cj|q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad de usar la fórmula para clasificar anuncios. Tampoco afecta la clasificación de consultas con umbrales adecuadamente elegidos. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P(Cj|q). 2.4 El método de votación Nos gustaría calcular P(Cj|q) de manera que RC (di(q), q) sea alta para i = 1, . . . , K y RC (d, q) sea baja para un documento aleatorio d. Supongamos que el vector [P(Cj|d)]Cj ∈C es aleatorio para un documento promedio, entonces la condición de que Cj ∈C P(Cj|q)2 sea pequeña implica que RC (d, q) también es pequeña en promedio sobre d. Por lo tanto, un método natural es maximizar K i=1 wiRC (di(q), q) sujeto a que Cj ∈C P(Cj|q)2 sea pequeño, donde wi son pesos asociados con cada rango i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , donde asumimos que K i=1 wi = 1, y λ > 0 es un parámetro de regularización de ajuste. La solución óptima es P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Dado que tanto P(Cj|di(q)) como P(Cj|q) pertenecen a [0, 1], podemos simplemente tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes wi. Una estrategia más compleja es permitir que w dependa de d también: P(Cj|q) = d w(d, q)g(P(Cj|d)), donde g(x) es una cierta transformación de x. En esta formulación general, w(d, q) puede depender de factores distintos al rango de d en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de r(d, q) donde r(d, q) es la puntuación de relevancia devuelta por el motor de búsqueda subyacente. Además, si se nos proporciona un conjunto de pares de categoría/consulta de entrenamiento etiquetados a mano (C, q), entonces tanto los pesos w(d, q) como la transformación g(·) pueden ser aprendidos utilizando técnicas estándar de clasificación. 2.5 Clasificación discriminativa Podemos tratar el problema de estimar P(Cj|q) como un problema de clasificación, donde para cada q, etiquetamos di(q) para i = 1, . . . , K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi(q) = 1 para di(q) cuando i ≤ K, y la etiqueta yi(q) = −1 para di(q) cuando i > K. En este escenario, la regla de puntuación de clasificación para un documento di(q) es lineal. Sea xi(q) = [P(Cj|di(q))], y w = [P(Cj|q)], entonces Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). Los valores P(Cj|d) son las características para el clasificador lineal, y [P(Cj|d)] es el vector de pesos, que puede ser calculado utilizando cualquier método de clasificación lineal. En este documento, consideramos estimar w utilizando regresión logística [17] de la siguiente manera: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Númerodecategorías Nivel de taxonomía Figura 1: Número de categorías por nivel 3. EVALUACIÓN En esta sección, evaluamos nuestra metodología que utiliza los resultados de búsqueda en la Web para mejorar la clasificación de consultas. 3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad en la Web. Dado que queremos que las clases sean útiles para emparejar anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un solo nodo probablemente resultará en una mala coincidencia de anuncios, ya que tanto las consultas sobre pies doloridos como sobre la gripe terminarán en el mismo nodo. Los anuncios apropiados para estas dos consultas son, sin embargo, muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una suficiente discriminación entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con una profundidad mediana de 5 y una profundidad máxima de 9. La Figura 1 muestra la distribución de las categorías por niveles taxonómicos. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aprox. 150 consultas por nodo), las cuales se utilizaron como conjunto de entrenamiento; una pequeña fracción de consultas ha sido asignada a más de una categoría. 3.2 Digresión: los conceptos básicos de la búsqueda patrocinada. Para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de la publicidad en la web. La publicidad de búsqueda patrocinada (o de pago) consiste en colocar anuncios de texto en las páginas de resultados de los motores de búsqueda web, con los anuncios siendo impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo! y MSN) admiten este tipo de anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios de texto se caracterizan por una o más frases de oferta que representan aquellas consultas en las que los anunciantes desean que se muestre su anuncio. (El nombre de frase de oferta proviene del hecho de que los anunciantes ofrecen diversas cantidades para asegurar su posición en la torre de anuncios asociada a una consulta). Una discusión sobre los mecanismos de oferta y colocación está fuera del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases por las que alguien puje. Por consiguiente, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En la concordancia amplia, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de la oferta, por ejemplo, eliminar o agregar palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. Dado que los anunciantes tienden a cubrir consultas de alto volumen y alto ingreso, las consultas de coincidencia amplia caen en la cola de la distribución en cuanto a volumen e ingresos. 3.3 Conjuntos de datos Utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no pueden ser emparejadas directamente con anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede asociar con al menos un anuncio utilizando la concordancia amplia como se describe arriba. Las consultas en el segundo conjunto no pueden ser coincidentes ni siquiera con la coincidencia amplia, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra publicidad para ellas. En cierto sentido, estas son consultas aún más raras y más alejadas de las consultas comunes. Como medida de la rareza de la consulta, estimamos su frecuencia en un mes de registros de consulta para un importante motor de búsqueda de EE. UU.; la frecuencia mediana fue de 1 para las consultas en el Conjunto 1 y 0 para las consultas en el Conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Conjunto 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el Conjunto 1 tienen en promedio 3.50 palabras, siendo la más larga de 11 palabras; las consultas en el Conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la longitud promedio de las consultas en la web es de poco menos de 3 palabras, lo cual es menor que en nuestros conjuntos de pruebas. Como otra medida de la dificultad de la consulta, medimos la fracción de consultas que contienen comillas, ya que estas ayudan a interpretar la consulta al agrupar significativamente las palabras. Solo el 8% de las consultas en el Conjunto 1 y el 14% en el Conjunto 2 contenían comillas. 3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas fueron clasificados en la taxonomía objetivo utilizando las técnicas presentadas en la sección 2. Basándose en los valores de confianza asignados, se presentaron a los evaluadores humanos las 3 clases principales para cada consulta. Estos evaluadores eran personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron cada par de consulta-clase y los clasificaron en una escala del 1 al 4, donde 1 significa que la clasificación es altamente relevante y 4 significa que es irrelevante para la consulta. Aproximadamente el 2.4% de las consultas en el Conjunto 1 y el 5.4% de las consultas en el Conjunto 2 fueron consideradas como no clasificables (por ejemplo, cadenas aleatorias de caracteres) y, por lo tanto, fueron excluidas de la evaluación. Para calcular las métricas de evaluación, consideramos que las clasificaciones con calificaciones de 1 y 2 son correctas, y las que tienen calificaciones de 3 y 4 son incorrectas. Utilizamos métricas de evaluación estándar: precisión, exhaustividad y F1. En lo que sigue, trazamos gráficos de precisión-recuperación para todos los experimentos. Para comparar con otros estudios publicados, también informamos los valores de precisión y F1 correspondientes a un recuerdo completo (R = 1). Debido a la falta de espacio, solo mostramos gráficos para la consulta Set 1; sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas. 3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador base expandió las consultas utilizando técnicas estándar de expansión de consultas, agrupó sus términos utilizando un reconocedor de frases, potenció ciertas frases en la consulta basándose en sus propiedades estadísticas, y realizó la clasificación utilizando el enfoque de vecino más cercano. Este clasificador base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de Estados Unidos. En nuestros experimentos, variamos los valores de parámetros pertinentes que caracterizan la forma exacta de utilizar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto de utilizar los resultados de búsqueda en la Web. Luego procedemos a explorar técnicas más refinadas, como utilizar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con la variación en el número de clasificaciones consideradas para cada resultado de búsqueda. Por falta de espacio, solo mostramos gráficos para las consultas del Conjunto 1 y omitimos los gráficos de las consultas del Conjunto 2, que exhiben fenómenos similares. 3.5.1 El efecto del conocimiento externo. Las consultas por sí solas son muy cortas y difíciles de clasificar. Utilizamos los resultados principales de los motores de búsqueda para recopilar conocimientos previos para las consultas. Utilizamos dos motores de búsqueda principales de EE. UU. y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreadas. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Interesantemente, descubrimos que el motor de búsqueda A funciona de manera consistente mejor con texto de página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor de precisión. F1 Prec. F1 Conjunto 1 Conjunto 1 Conjunto 2 Conjunto 2 Una página completa 0.72 0.84 0.509 0.721 B página completa 0.706 0.827 0.497 0.665 A resumen 0.586 0.744 0.396 0.572 B resumen 0.645 0.788 0.467 0.638 Línea base 0.534 0.696 0.365 0.536 Tabla 1: El efecto de usar conocimiento externo 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales pueden ser clasificados por separado, con una votación posterior entre las clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales pueden agruparse como un metadocumento y clasificarse como tal utilizando el clasificador de documentos. La Figura 3 presenta los resultados de estos dos enfoques. Cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de los resultados de búsqueda claramente supera al enfoque de agrupamiento por un amplio margen. Sin embargo, en el caso de los resúmenes, se ha encontrado que agruparlos juntos es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí solos son demasiado cortos para ser clasificados correctamente de forma individual, pero cuando se agrupan juntos son mucho más estables. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Línea base Página completa agrupada Página completa votada Resumen agrupado Resumen votado Figura 3: Votación vs. Agrupamiento 3.5.3 Texto de página completa vs. resumen Para resumir las dos secciones anteriores, el conocimiento previo para cada consulta se obtiene utilizando tanto el texto de página completa como solo los resúmenes de los principales resultados de búsqueda. Se encontró que el texto de página completa estaba más en consonancia con la clasificación votada, mientras que los resúmenes resultaron útiles cuando se agrupaban juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificados individualmente, con votación posterior utilizada para determinar la clasificación final de la consulta. Esta observación difiere de los hallazgos de Shen et al. [20], quienes encontraron que los resúmenes eran más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son de cola, que son raras y difíciles de clasificar. 3.5.4 Variación en el número de clases por resultado de búsqueda. También variamos el número de clasificaciones por resultado de búsqueda, es decir, cada resultado podía tener 1, 3 o 5 clases. La Figura 4 muestra las gráficas de precisión-recuperación correspondientes tanto para la configuración de página completa como para la de resumen únicamente. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de precisión-recuperación para el experimento de una sola clase tiene mayores fluctuaciones. Usar 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado la curva de precisión-recuperación es muy suave. Por lo tanto, al aumentar el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas. 3.5.5 Variación en el número de resultados de búsqueda obtenidos. También experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. De acuerdo con nuestra intuición, observamos que la precisión de la clasificación aumenta constantemente a medida que incrementamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera disminución al seguir utilizando aún más resultados (50). Esto se debe a que usar muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que usar demasiados resultados introduce ruido adicional. Utilizando la prueba t pareada, evaluamos la significancia estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación Baseline 1 clase página completa 3 clases página completa 5 clases página completa 1 clase resumen 3 clases resumen 5 clases resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precisión Recuperación 10 20 30 40 50 Baseline Figura 5: Variando el número de resultados por consulta de las mejoras debido a nuestra metodología versus el baseline. Encontramos que los resultados son altamente significativos (p < 0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas. 3.6 Votación versus métodos alternativos Como se explicó en la Sección 2.2, se pueden utilizar varios métodos para clasificar las consultas de los resultados de los motores de búsqueda basados en nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de votar por los diez mejores resultados de búsqueda con los siguientes dos métodos: • A: Aprendizaje discriminativo de clasificación de consultas basado en regresión logística, descrito en la Sección 2.5. • B: Aprendizaje de pesos basado en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos la puntuación de calidad s(d, q) de un par de consulta/documento en {alta, media, baja}, y aprendemos los tres pesos w en un conjunto de consultas de entrenamiento, y evaluamos el rendimiento en consultas de retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P(Cj|q) = d w(s(d, q))P(Cj|d). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división; sin embargo, para mantener la consistencia, realizamos divisiones de entrenamiento/prueba al azar de 50-50 diez veces, y reportamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y exhaustividad, utilizamos DCG-k (k = 1, 5), popular en la evaluación de motores de búsqueda. La métrica DCG (ganancia acumulada descontada), descrita en [8], es una medida de clasificación en la que se pide al sistema que clasifique un conjunto de candidatos (en nuestro caso, categorías juzgadas para cada consulta), y calcula para cada consulta q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), donde Ci(q) es la i-ésima categoría para la consulta q clasificada por el sistema, y g(Ci) es la calificación de Ci: asignamos calificaciones de 10, 5, 1, 0 a la escala de juicio de 4 puntos descrita anteriormente para calcular DCG. La elección en descomposición de log2(i + 1) es convencional, la cual no tiene una importancia particular. El DCG general de un sistema es el DCG promediado sobre las consultas. Utilizamos esta métrica en lugar de precisión/recuperación en este experimento porque puede manejar directamente salidas de múltiples grados. Por lo tanto, como una única métrica, es conveniente para comparar los métodos. Ten en cuenta que las curvas de precisión/recuperación utilizadas en las secciones anteriores proporcionan algunas ideas adicionales que no son inmediatamente evidentes a partir de los números de DCG. Los resultados de nuestros experimentos se muestran en la Tabla 3. El método del oráculo es la mejor clasificación de categorías para cada consulta después de ver los juicios humanos. No puede lograrse mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden llevar a una ganancia de rendimiento moderada (especialmente el método A, que utiliza entrenamiento discriminativo en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente pequeña como para ser despreciada. Esto significa que como método simple, votar es bastante efectivo. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que asignar pesos iguales (votación) funciona de manera similar a asignar pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de menor calidad (método B), al menos para los resultados de búsqueda principales. Podría ser posible mejorar este método al incluir otras características de la página que puedan diferenciar los resultados de búsqueda mejor clasificados. Sin embargo, la efectividad requerirá una investigación adicional que no realizamos. También podemos observar que el rendimiento en el Conjunto 2 es inferior al del Conjunto 1, lo que significa que las consultas en el Conjunto 2 son más difíciles que las del Conjunto 1. 3.7 Análisis de fallos Examinamos los casos en los que el conocimiento externo no mejoró la clasificación de consultas e identificamos tres causas principales para tal falta de mejora. (1) Consultas que contienen cadenas aleatorias, como números de teléfono; estas consultas no arrojan resultados de búsqueda coherentes, por lo que estos últimos no pueden ayudar en la clasificación (alrededor del 5% de las consultas eran de este tipo). (2) Consultas que no arrojan resultados de búsqueda en absoluto; hubo un 8% de tales consultas en el Conjunto 1 y un 15% en el Conjunto 2. (3) Consultas correspondientes a eventos recientes, para los cuales el motor de búsqueda aún no tenía una cobertura suficiente (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias; si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de la búsqueda sean de poca utilidad. 4. Aunque la longitud promedio de las consultas de búsqueda está aumentando constantemente con el tiempo, una consulta típica sigue siendo de menos de 3 palabras. Por consiguiente, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de consultas. Esto se puede hacer ya sea utilizando diccionarios electrónicos y tesauros [22], o a través de técnicas de retroalimentación de relevancia que hacen uso de algunos de los resultados de búsqueda con mayor puntuación. Los primeros trabajos en recuperación de información se centraron en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el volumen abrumador de consultas en la actualidad no se presta para una supervisión manual, por lo que trabajos posteriores se centraron en la <br>retroalimentación de relevancia ciega</br>, que básicamente asume que los resultados principales devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios sobre la ampliación de consultas se han centrado en la clasificación de consultas, asumiendo que dichas clasificaciones son beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al. [10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al. [1] utilizaron aprendizaje semisupervisado, así como datos no etiquetados [2]. Gravano et al. [6] clasificaron las consultas con respecto a la localidad geográfica para determinar si su intención es local o global. La competencia KDD Cup de 2005 sobre clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer las consultas utilizando motores de búsqueda web y directorios [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó el desafío de utilizar estos datos de entrenamiento para construir un clasificador de consultas. Varios equipos utilizaron la Web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones a la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en las clasificaciones de documentos. La solución ganadora del KDD Cup [18] propuso utilizar un conjunto de clasificadores en conjunto con la búsqueda en múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto de Directorio Abierto (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía de la ODP fue luego mapeada en la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se construyó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos mapeados al nodo objetivo específico. Por lo tanto, los documentos web fueron clasificados primero con respecto a la jerarquía de ODP, y sus clasificaciones fueron posteriormente mapeadas a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo utilizando las consultas para producir un clasificador de documentos, como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también simplifica el mantenimiento y desarrollo de la taxonomía. Utilizando este enfoque, logramos obtener un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar las clasificaciones individuales de documentos al clasificar realmente la consulta. En un artículo de seguimiento [19], Shen et al. propusieron un marco de trabajo para la clasificación de consultas basado en la conexión entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para los resultados web se resuelve utilizando un conjunto de entrenamiento disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de entrenamiento (ODP). Entonces se prueban varios esquemas que establecen una correspondencia entre las taxonomías o permiten mapear el conjunto de entrenamiento desde la taxonomía intermedia a la taxonomía objetivo. En contraposición a esto, construimos un clasificador de documentos para la taxonomía objetivo directamente, sin utilizar documentos de una taxonomía intermedia. Aunque no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho más grande), nuestros resultados de precisión y recall son consistentemente más altos incluso sobre el conjunto de consultas más difíciles. 5. CONCLUSIONES La clasificación de consultas es una tarea importante de recuperación de información. La clasificación precisa de las consultas de búsqueda probablemente beneficiará a una serie de tareas de nivel superior, como la búsqueda web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí solas suelen contener información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para utilizar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de mayor rango son relevantes para la consulta. Clasificar estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio de conocimiento mundial aporta información valiosa sobre la consulta y ayuda en su correcta clasificación. Notablemente, nuestro método muestra una precisión significativamente mayor que los métodos descritos en estudios anteriores. En comparación con estudios previos, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consultas directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud mayor que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al utilizar los resultados de búsqueda, uno puede optar por utilizar solo los resúmenes de los resultados proporcionados por el motor de búsqueda, o rastrear realmente las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor al utilizar las páginas completas rastreadas (Tabla 1). Estos resultados son consistentes con estudios previos [5], los cuales encontraron que el uso de páginas completas rastreadas es superior para la clasificación de documentos que el uso de solo resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al. [19], quienes encontraron que los resúmenes producían mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos motores de búsqueda principales, A y B. Curiosamente, encontramos distinciones notables en la calidad de sus resultados. Notablemente, para el motor A los resultados generales fueron mejores al utilizar las páginas completas rastreadas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo al resumir las páginas. También descubrimos que los mejores resultados se obtuvieron al utilizar páginas completas rastreadas y realizar votaciones entre sus clasificaciones individuales. Para un clasificador que es externo al motor de búsqueda, recuperar páginas completas puede resultar prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para ganar eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación de página completa es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas al clasificarlas una vez en la taxonomía (fija). Entonces, las clasificaciones de páginas se obtienen como parte de los metadatos asociados con cada resultado de búsqueda, y la clasificación de consultas puede ser casi instantánea. Al utilizar resúmenes, parece que se obtienen mejores resultados al primero concatenar los resúmenes individuales en un meta-documento y luego utilizar su clasificación en su totalidad. Creemos que la razón de esta observación es que los resúmenes son cortos y inherentemente más ruidosos, por lo que su agregación ayuda a identificar correctamente el tema principal. Coherente con nuestra intuición, usar muy pocos resultados de búsqueda proporciona conocimiento útil pero insuficiente, y usar demasiados resultados de búsqueda lleva a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar los 40 mejores resultados de búsqueda. En este trabajo, primero clasificamos los resultados de búsqueda y luego utilizamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, se pueden utilizar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, realizamos algunos experimentos preliminares en esta dirección y encontramos que aprender un clasificador secundario de este tipo no proporcionaba ventajas considerablemente significativas. Planeamos investigar más en esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que implementar nuestra metodología conlleva poco sobrecargo. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en el momento de la consulta solo necesitamos recuperar estas clasificaciones y realizar la votación. Para concluir, creemos que nuestra metodología para utilizar los resultados de búsqueda en la Web tiene un gran potencial para mejorar sustancialmente la precisión de las consultas de búsqueda en la Web. Esto es particularmente importante para consultas raras, para las cuales se puede hacer poco aprendizaje por consulta, y en este estudio demostramos que tal escasez de información podría ser abordada aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para obtener anuncios mejor relacionados. En nuestra investigación futura también planeamos hacer uso de la información de la sesión para aprovechar el conocimiento sobre consultas anteriores y clasificar mejor las siguientes. 6. REFERENCIAS [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de entrenamiento etiquetados y no etiquetados. En Actas de SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejorando la clasificación automática de consultas a través del aprendizaje semisupervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de escenas. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-Okapi en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando conocimiento del mundo. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Clasificación de consultas web según la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en el centroide: Análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo ferrety para el problema de la Copa KDD 2005. En SIGKDD Explorations, volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizando el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conferencia Australiana sobre Inteligencia Artificial, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe de la KDD CUP-2005: Enfrentando un gran desafío. En SIGKDD Explorations, volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejorando la expansión automática de consultas. En SIGIR98, páginas 206-214, 1998. [13] M. Moran y B. Cazar. Search Engine Marketing, Inc.: Atrayendo tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En TREC-3, 1995. [15] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el Sistema de Recuperación SMART: Experimentos en Procesamiento Automático de Documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288-297, 1990. [17] T. Santner y D. Duffy. El Análisis Estadístico de Datos Discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. \n\nSpringer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Nuestra solución ganadora para la clasificación de consultas en KDDCUP 2005. En SIGKDD Explorations, volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. \n\nACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consultas para la clasificación de consultas web. ACM TOIS, 24:320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En SIGIR06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas en motores de búsqueda utilizando la web como conocimiento de fondo. En SIGKDD Explorations, volumen 7. ACM, 2005. [22] E. Voorhees. \n\nACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En SIGIR94, 1994. [23] J. Xu y W. Bruce Croft. Mejorando la efectividad de la recuperación de información con análisis de contexto local. ACM TOIS, 18(1):79-112, 2000. \n\nACM TOIS, 18(1):79-112, 2000. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}