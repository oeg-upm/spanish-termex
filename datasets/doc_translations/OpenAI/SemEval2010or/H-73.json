{
    "id": "H-73",
    "original_text": "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst. School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst. School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval. This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases. With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications. Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents). This new model provides a more solid framework for distributed information retrieval. Empirical studies show that it is at least as effective as other state-of-the-art algorithms. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1. INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing. Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database. For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines. Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines. Therefore, it is very important to search this type of valuable information. The architecture of distributed search solution is highly influenced by different environmental characteristics. In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines. Early distributed information retrieval research focused on this type of cooperative environments [1,8]. On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required. Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required. Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases. In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines. There are three important sub-problems in distributed information retrieval. First, information about the contents of each individual database must be acquired (resource representation) [1,8,21]. Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21]. Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22]. Many types of solutions exist for distributed information retrieval. Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics. A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries. It is composed of the resource description and the resource selection components. This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically. Distributed document retrieval is a more sophisticated task. It selects relevant information sources for users queries as the database recommendation system does. Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users. The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal. On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal. Prior research indicated that these two goals are related but not identical [4,21]. However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21]. This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals. First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21]. A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance. Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document. The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score. Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates. For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal. For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance. This selection criterion meets the high-precision goal of distributed document retrieval application. Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list. The unified utility framework makes very few assumptions and works in uncooperative environments. Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases. Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model. The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases. This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15]. The unified utility framework is not only more theoretically solid but also very effective. Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations. The next section discusses related work. Section 3 describes the new unified utility maximization model. Section 4 explains our experimental methodology. Sections 5 and 6 present our experimental results for resource selection and document retrieval. Section 7 concludes. 2. PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval. We survey the most related work in this section. The first problem of distributed information retrieval is resource representation. The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8]. However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not. Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other. Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions. As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic. Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1]. Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade. The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]). As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work. The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly. Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21]. The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores. The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models. These linear models are used to convert the database-specific document scores into the approximated centralized document scores. The SSL algorithm has been shown to be effective [22]. It serves as an important component of our unified utility maximization framework (Section 3). In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system. But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal. This type of inconsistency has been observed in previous research [4,21]. The research in [21] tried to solve the problem with a heuristic method. The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15]. This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality. However, two points distinguish our research from the DTF model. First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework. Second, the DTF builds a model for each database to calculate the probabilities of relevance. This requires human relevance judgments for the results retrieved from each database. In contrast, our approach only builds one logistic model for the centralized sample database. The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated. This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3. UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment. In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model. We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision. In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases. This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling. Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21]. At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries. For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database. The SSL results merging algorithm [20,22] is used to merge the results. Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database. The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15]. Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model. These two parameters are estimated by maximizing the probabilities of relevance of the training queries. The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated. However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents. This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics. We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database. The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5. Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database. Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on. Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1. The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ . It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves. However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents. Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100). Specifically, a logistic model is built for each of these databases. The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model. For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated. Together with the scores of the top two sampled documents, these parameters can be estimated. After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)). The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above. The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results. From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases. After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1. Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework. In the task of database recommendation, the system needs to decide how to rank databases. In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database. We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task. Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases. The database selection decision is made based on the complete lists of probabilities of relevance for all the databases. The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database. If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ . Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists. Finally, to each selection action d and a set of complete lists of Figure 1. Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ . Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation. In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation. The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise. Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple. We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal. We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval. It is measured by the Precision at the top part of the final merged document list. This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking. Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem. The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected. We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database). A more complex situation is to vary the number of retrieved documents from each selected database. More specifically, we allow different selected databases to return different numbers of documents. For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.) This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.) This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly). For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved. Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14. However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution. The basic steps of this dynamic programming method are described in Figure 2. As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm. After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database. The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm. It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4. EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications. The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers. In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web. However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web. It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all. These types of web pages are contained in the WT2g/WT10g datasets. Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts. Another choice is the TREC news/government data [1,15,17, 18,21]. TREC news/government data is concentrated on relatively narrow topics. Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data. By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents. As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3]. As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work. Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21]. It was chosen in this work. Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments. Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3. They were organized by source and publication date [1]. The sizes of the databases are not skewed. Details are in Table 1. Three testbeds built in [21] were based on the trec123-100colbysource testbed. Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together. Input: Complete lists of probabilities of relevance for all the |DB| databases. Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates. And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept. Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb). Figure 2. The dynamic programming optimization procedure for Equation 16. Table1: Testbed statistics. Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics. Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order. Two large databases were created by merging 20 small databases with the round-robin method. Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases. Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall. The other 60 collections were left unchanged. The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases. Thus, the two large databases have many more relevant documents than the small databases. Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall. The other 80 collections were left unchanged. The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150. The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine. To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20]. All these algorithms were implemented with the Lemur toolkit [12]. These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5. RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system. The resource descriptions were created using query-based sampling. About 80 queries were sent to each database to download 300 unique documents. The database size statistics were estimated by the sample-resample method [21]. Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1). Another 50 queries (51-100) were used as test data. Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21]. Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm. And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases. The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR. The UUM/HR algorithm is described in Section 3.3. It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm. The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds. This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm. It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm. We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough. More training data or a more sophisticated model may help to solve this minor puzzle. Collections Selected. Collections Selected. Trec123-100Col Testbed. Representative Testbed. Collection Selected. Collection Selected. Relevant Testbed. Nonrelevant Testbed. Figure 3. Resource selection experiments on the four testbeds. 38 6. RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list. In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm. This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores. It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22]. This is a common scenario in operational environments and was the case for our experiments. Document retrieval effectiveness was measured by Precision at the top part of the final document list. The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms. The last three algorithms were proposed in Section 3. All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database. The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10. The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5. Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6. Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3. Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4. Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms. Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed. On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4). The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms. One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves. This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance. Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval. The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do. It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms. This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16. On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6). The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm. On this testbed the three UUM algorithms are about equally effective. Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases. This explains why they are about equally effective for document retrieval. In real operational environments, databases may return no document scores and report only ranked lists of results. As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores. The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22]. The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7. The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids. It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives. Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms. The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval. The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7. CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets. Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application. We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical. This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment. In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework. In this framework, the selection decisions are obtained by optimizing different objective functions. As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner. The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database. A single logistic model was trained on the centralized Table 7. Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model. Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database. This framework is not only more theoretically solid but also very effective. One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework. Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web. Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores. Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better. Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks. The unified utility maximization framework is open for different extensions. When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs. Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments. All of these are the directions of future research. ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor. REFERENCES [1] J. Callan. (2000). Distributed information retrieval. In W.B. Croft, editor, Advances in Information Retrieval. Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. Croft, and J. Broglio. (1995). TREC and TIPSTER experiments with INQUERY. Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002). Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment. Distributed search over the hidden web: Hierarchical database sampling and selection. In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000). Methods for distributed information retrieval. Ph. D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999). Merging results from isolated search engines. In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000). A comparison of techniques for selecting text collections. In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999). A Decision-Theoretic approach to database selection in networked IR. ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997). STARTS: Stanford proposal for internet metasearching. In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003). QProber: A System for Automatic Classification of Hidden-Web Databases. ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002). Distributed search over the hidden web: Hierarchical database sampling and selection. In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003). Content-based information retrieval in peer-to-peer networks. In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T. Yu and K.L. Liu. (2002) Building efficient and effective metasearch engines. ACM Comput. Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003). Evaluating different method of estimating retrieval quality for resource selection. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003). The MIND architecture for heterogeneous multimedia federated digital libraries. ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L. Powell, J.C. French, J. Callan, M. Connell, and C.L. Viles. (2000). The impact of database selection on distributed searching. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L. Powell and J.C. French. (2003). Comparing the performance of database selection algorithms. ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). Search for the invisible web. Guardian Unlimited. [20] L. Si and J. Callan. (2002). Using sampled data and regression to merge search engine results. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003). Relevant document distribution estimation method for resource selection. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003). A Semi-Supervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41",
    "original_translation": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La recuperación distribuida de documentos es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de recuperación de documentos distribuidos, tratándolos como objetivos de optimización diferentes. Primero, se construye una base de datos de muestra centralizada mediante el muestreo aleatorio de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas; también se estiman las estadísticas del tamaño de la base de datos. Un modelo de transformación logística se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia. Segundo, después de que se envía una nueva consulta, la consulta se puede utilizar para buscar en la base de datos de muestras centralizada que produce una puntuación para cada documento muestreado. La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada puede estimarse aplicando el modelo logístico al puntaje de cada documento. Entonces, las probabilidades de relevancia de todos los documentos (en su mayoría no vistos) entre las bases de datos disponibles pueden ser estimadas utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos. Para la tarea de selección de recursos para un sistema de recomendación de bases de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alto recall. Para la selección de recursos para un sistema distribuido de recuperación de documentos, se prefieren las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia. Este criterio de selección cumple con el objetivo de alta precisión de la aplicación de recuperación de documentos distribuidos. Además, se aplica el algoritmo de aprendizaje semisupervisado (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada. El marco de utilidad unificado hace muy pocas suposiciones y funciona en entornos no cooperativos. Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos correspondientes; y ii) Muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos. Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades correspondientes de relevancia y utiliza la base de datos de muestra centralizada como puente entre las bases de datos individuales y el modelo logístico. El esfuerzo humano (juicio de relevancia) necesario para entrenar el modelo logístico centralizado único no aumenta con el número de bases de datos. Esta es una gran ventaja sobre investigaciones anteriores, las cuales requerían que la cantidad de esfuerzo humano fuera lineal con el número de bases de datos [7,15]. El marco de utilidad unificada no solo es más sólido teóricamente, sino también muy efectivo. Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de vanguardia en una variedad de configuraciones. La siguiente sección discute el trabajo relacionado. La sección 3 describe el nuevo modelo unificado de maximización de utilidad. La sección 4 explica nuestra metodología experimental. Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos. La sección 7 concluye. 2. Investigación previa Ha habido una considerable investigación sobre todos los subproblemas de la recuperación de información distribuida. Exploramos los trabajos más relacionados en esta sección. El primer problema de la recuperación de información distribuida es la representación de recursos. El protocolo STARTS es una solución para adquirir descripciones de recursos en entornos cooperativos [8]. Sin embargo, en entornos no cooperativos, aunque las bases de datos estén dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no. Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí. Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en consultas, que genera y envía consultas de forma aleatoria a motores de búsqueda individuales y recupera algunos documentos para construir las descripciones. Dado que los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico. Los experimentos han demostrado que descripciones de recursos bastante precisas pueden ser construidas enviando alrededor de 80 consultas y descargando alrededor de 300 documentos [1]. Muchos algoritmos de selección de recursos como gGlOSS/vGlOSS [8] y CORI [1] han sido propuestos en la última década. El algoritmo CORI representa cada base de datos por sus términos, las frecuencias de los documentos y un pequeño número de estadísticas del corpus (detalles en [1]). Como investigaciones previas en diferentes conjuntos de datos han demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo utilizamos como algoritmo base en este trabajo. El algoritmo de selección de recursos de estimación de distribución de documentos relevantes (ReDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia. Aunque se ha demostrado que el algoritmo ReDDE es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21]. El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar puntuaciones de documentos específicas de la base de datos en puntuaciones de documentos independientes de la base de datos comparables. El algoritmo de fusión de resultados de aprendizaje semisupervisado (SSL) [20,22] utiliza los documentos adquiridos mediante muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos y de la consulta. Estos modelos lineales se utilizan para convertir las puntuaciones de documentos específicas de la base de datos en las puntuaciones de documentos centralizadas aproximadas. El algoritmo SSL ha demostrado ser efectivo [22]. Sirve como un componente importante de nuestro marco unificado de maximización de utilidad (Sección 3). Para lograr resultados precisos en la recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos en sistemas de recomendación de bases de datos. Pero como se señaló anteriormente, un algoritmo de selección de recursos optimizado para un alto recuerdo puede no funcionar bien para la recuperación de documentos, que tiene como objetivo la alta precisión. Este tipo de inconsistencia ha sido observada en investigaciones previas [4,21]. La investigación en [21] intentó resolver el problema con un método heurístico. La investigación más similar a lo que proponemos aquí es el marco teórico de la toma de decisiones (DTF) [7,15]. Este marco de trabajo calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y se han propuesto varios métodos [15] para estimar la calidad de recuperación. Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF. Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de bases de datos y recuperación distribuida de documentos) en el mismo marco unificado. Segundo, el DTF construye un modelo para cada base de datos para calcular las probabilidades de relevancia. Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos. Por el contrario, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada. La base de datos de muestra centralizada puede servir como puente para conectar las bases de datos individuales con el modelo logístico centralizado, de esta manera se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos. Esta estrategia puede ahorrar una gran cantidad de esfuerzo en juicio humano y es una gran ventaja del marco de maximización de utilidad unificada sobre el DTF, especialmente cuando hay un gran número de bases de datos. MARCO DE MAXIMIZACIÓN DE UTILIDAD UNIFICADA El marco de Maximización de Utilidad Unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría no vistos) disponibles en el entorno de búsqueda distribuida. En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el modelo de Maximización de Utilidad Unificado. También describimos cómo el modelo puede ser optimizado para el objetivo de alto recuerdo de un sistema de recomendación de base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido. 3.1 Estimación de Probabilidades de Relevancia Como se señaló anteriormente, el propósito de la selección de recursos es el alto recuerdo y el propósito de la recuperación de documentos es la alta precisión. Para cumplir con estos objetivos diversos, el problema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos. Este es un problema difícil porque solo podemos observar una muestra de los contenidos de cada base de datos utilizando muestreo basado en consultas. Nuestra estrategia es aprovechar al máximo toda la información disponible para calcular las estimaciones de probabilidad. 3.1.1 Aprendizaje de Probabilidades de Relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se construye mediante muestreo basado en consultas y los tamaños de la base de datos se estiman utilizando el método de muestreo y remuestreo [21]. Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (Inquery [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de entrenamiento. Para cada consulta de entrenamiento, se aplica el algoritmo de selección de recursos CORI [1] para seleccionar un cierto número (por ejemplo, 10) de bases de datos y recuperar 50 identificadores de documentos de cada base de datos. El algoritmo de fusión de resultados SSL [20,22] se utiliza para combinar los resultados. Luego, podemos descargar los 50 documentos principales de la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando Inquery y las estadísticas del corpus de la base de datos de muestra centralizada. Las puntuaciones centralizadas se normalizan aún más (dividiéndolas por la puntuación centralizada máxima para cada consulta), ya que este método ha sido sugerido para mejorar la precisión de la estimación en investigaciones anteriores [15]. El juicio humano se adquiere para esos documentos y se construye un modelo logístico para transformar las puntuaciones de documentos centralizados normalizados en probabilidades de relevancia de la siguiente manera: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) donde )( _ dSc es la puntuación de documento centralizada normalizada y ac y bc son los dos parámetros del modelo logístico. Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de entrenamiento. El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de las puntuaciones de documentos centralizadas. 3.1.2 Estimación de las puntuaciones de documentos centralizadas Cuando el usuario envía una nueva consulta, se calculan las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar las puntuaciones de los documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados. Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas del tamaño de la base de datos. Definimos el factor de escala de la base de datos para la base de datos i como la razón entre el tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: SF_i = ^N_db / _N_db_samp_i donde ^N_db es el tamaño estimado de la base de datos y _N_db_samp_i es el número de documentos de la base de datos i en la base de datos de muestra centralizada. La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podríamos suponer que hay alrededor de 50 documentos en esa base de datos que tienen puntuaciones de alrededor de 0.5. De hecho, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntuación del documento centralizado para cada base de datos. Formalmente, clasificamos todos los documentos muestreados de la base de datos i-ésima por sus puntajes de documento centralizado 34 para obtener la lista de puntajes de documento centralizado muestreado {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} para la base de datos i; asumimos que si pudiéramos calcular los puntajes de documento centralizado para todos los documentos en esta base de datos y obtener la lista completa de puntajes de documento centralizado, el documento superior en la lista muestreada tendría un rango de SFdbi/2, el segundo documento en la lista muestreada tendría un rango de SFdbi3/2, y así sucesivamente. Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. La interpolación lineal por tramos se aplica para estimar la curva de puntuación del documento centralizado, como se ilustra en la Figura 1. La lista completa de puntuaciones de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documentos centralizados como: ],1[,)(S ^^ c idbij Njd ∈ . Se puede observar en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntuación del documento centralizado. Sin embargo, para bases de datos con grandes proporciones de escala de base de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos mejor clasificados (por ejemplo, [1, SFdbi/2]). Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizados de los documentos mejor clasificados para bases de datos con ratios a gran escala (por ejemplo, mayores de 100). Específicamente, se construye un modelo logístico para cada una de estas bases de datos. El modelo logístico se utiliza para estimar la puntuación del documento centralizado superior 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con las puntuaciones centralizadas más altas. 0iα , 1iα y 2iα son los parámetros del modelo logístico. Para cada consulta de entrenamiento, se descarga el documento mejor recuperado de cada base de datos y se calcula la puntuación del documento centralizado correspondiente. Junto con las puntuaciones de los dos documentos muestreados principales, estos parámetros pueden ser estimados. Después de estimar la puntuación centralizada del documento principal, se ajusta una función exponencial para la parte superior ([1, SFdbi/2]) de la curva de puntuación del documento centralizado como: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) Los dos parámetros 0iβ y 1iβ se ajustan para asegurarse de que la función exponencial pase por los dos puntos (1, ^ 1)( ic dS ) y (SFdbi/2, Sc(dsi1)). La función exponencial se utiliza únicamente para ajustar la parte superior de la curva de puntuación del documento centralizado, mientras que la parte inferior de la curva sigue siendo ajustada con el método de interpolación lineal descrito anteriormente. El ajuste mediante la función exponencial de los documentos mejor clasificados ha demostrado empíricamente producir resultados más precisos. A partir de las curvas de puntuación de documentos centralizadas, podemos estimar las listas completas de puntuación de documentos centralizados correspondientes para todas las bases de datos disponibles. Después de que las puntuaciones estimadas de los documentos centralizados se normalizan, las listas completas de probabilidades de relevancia pueden ser construidas a partir de las listas completas de puntuaciones de documentos centralizados mediante la Ecuación 1. Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El Modelo Unificado de Maximización de Utilidad En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco. En la tarea de recomendación de bases de datos, el sistema necesita decidir cómo clasificar las bases de datos. En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada. Generalizamos el proceso de selección de recomendaciones de bases de datos, que implícitamente recomienda todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos. Formalmente, denotamos di como el número de documentos que nos gustaría recuperar de la base de datos i y ,.....},{ 21 ddd = como una acción de selección para todas las bases de datos. La decisión de selección de la base de datos se toma en base a las listas completas de probabilidades de relevancia para todas las bases de datos. Las listas completas de probabilidades de relevancia se infieren a partir de toda la información disponible, específicamente sR, que representa las descripciones de recursos adquiridas mediante muestreo basado en consultas y las estimaciones del tamaño de la base de datos adquiridas mediante muestreo-resampleo; cS representa las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Si el método de estimación de puntajes de documentos centralizados y probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈. El vector aleatorio   denota un conjunto arbitrario de listas completas de probabilidades de relevancia y ),|( cs SRP θ como la probabilidad de generar este conjunto de listas. Finalmente, a cada acción de selección d y un conjunto de listas completas de la Figura 1. Construcción de la lista completa de puntuación de documentos centralizada mediante interpolación lineal (el factor de escala de la base de datos es 50). Para 35 probabilidades de relevancia θ, asociamos una función de utilidad ),( dU θ que indica el beneficio de realizar la selección d cuando las verdaderas listas completas de probabilidades de relevancia son θ. Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7). Un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en los valores de parámetros más probables en lugar de calcular toda la expectativa. En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recomendación de bases de datos. El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de N bases de datos de Nsdb) que contengan tantos documentos relevantes como sea posible, lo cual puede definirse formalmente como: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) es la función indicadora, que es 1 cuando se selecciona la i-ésima base de datos y 0 en caso contrario. Inserta esta ecuación en el modelo básico de la Ecuación 8 y asocia la restricción del número de base de datos seleccionado para obtener lo siguiente: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) La solución de este problema de optimización es muy simple. Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) Las bases de datos Nsdb con el mayor número esperado de documentos relevantes pueden ser seleccionadas para cumplir con el objetivo de alto recall. Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recuperación distribuida de documentos. Se mide mediante la Precisión en la parte superior de la lista final de documentos fusionados. Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la Precisión de los documentos recuperados de las bases de datos seleccionadas. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Tenga en cuenta que la diferencia clave entre la Ecuación 12 y la Ecuación 9 es que la Ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos, mientras que la Ecuación 12 solo considera una parte mucho más pequeña de la clasificación. Específicamente, podemos calcular la decisión de selección óptima mediante: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Diferentes tipos de restricciones causadas por las diferentes características de las tareas de recuperación de documentos pueden estar asociadas con el problema de optimización anterior. La más común es seleccionar un número fijo (Nsdb) de bases de datos y recuperar un número fijo (Nrdoc) de documentos de cada base de datos seleccionada, definido formalmente como: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) Este problema de optimización puede resolverse fácilmente calculando el número de documentos relevantes esperados en la parte superior de la lista completa de probabilidades de relevancia de cada base de datos: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Luego, las bases de datos pueden ser clasificadas por estos valores y seleccionadas. Llamamos a este algoritmo UUM/HP-FL (Maximización Unificada de Utilidad para Alta Precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada). Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada. Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes cantidades de documentos. Para simplificar, se requiere que las longitudes de la lista de resultados sean múltiplos de un número base 10. (Este valor también puede variar, pero para simplificar se establece en 10 en este documento). Esta restricción está establecida para simular el comportamiento de los motores de búsqueda comerciales en la web. (Motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 identificadores de documentos por página de resultados). Este procedimiento ahorra tiempo de cálculo al calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea de 10 en lugar de 1 (más detalles se discuten posteriormente). Para una mayor simplificación, restringimos la selección a un máximo de 100 documentos de cada base de datos (di<=100). Luego, el problema de optimización de la selección se formaliza de la siguiente manera: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc es el número total de documentos a recuperar. Desafortunadamente, no hay una solución simple para este problema de optimización como la hay para las Ecuaciones 10 y 14. Sin embargo, se puede aplicar un algoritmo de programación dinámica de 36 para calcular la solución óptima. Los pasos básicos de este método de programación dinámica se describen en la Figura 2. Dado que este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se le llama algoritmo UUM/HP-VL. Después de que se toman las decisiones de selección, se buscan las bases de datos seleccionadas y se recuperan los identificadores de documentos correspondientes de cada base de datos. El paso final de la recuperación de documentos es fusionar los resultados devueltos en una única lista clasificada con el algoritmo de aprendizaje semisupervisado. Se señaló anteriormente que el algoritmo SSL mapea las puntuaciones específicas de la base de datos en las puntuaciones de documentos centralizadas y construye la lista clasificada final en consecuencia, lo cual es consistente con todos nuestros procedimientos de selección donde se seleccionan los documentos con mayores probabilidades de relevancia (y por ende, puntuaciones de documentos centralizadas más altas). 4. METODOLOGÍA EXPERIMENTAL 4.1 Bancos de pruebas Es deseable evaluar algoritmos de recuperación de información distribuida con bancos de pruebas que simulen de cerca las aplicaciones del mundo real. Las colecciones web TREC WT2g o WT10g proporcionan una forma de dividir los documentos por diferentes servidores web. De esta manera, se podrían crear un gran número (O(1000)) de bases de datos con contenidos bastante diversos, lo que podría convertir a este banco de pruebas en un buen candidato para simular entornos operativos como la web oculta de dominio abierto. Sin embargo, dos debilidades de este banco de pruebas son: i) Cada base de datos contiene solo una pequeña cantidad de documentos (259 documentos en promedio para WT2g) [4]; y ii) El contenido de WT2g o WT10g se extrae arbitrariamente de la web. No es probable que una base de datos web oculta proporcione páginas personales o páginas web que indiquen que las páginas están en construcción y no contengan información útil en absoluto. Estos tipos de páginas web están contenidos en los conjuntos de datos WT2g/WT10g. Por lo tanto, los datos ruidosos de la Web no son similares a los contenidos de alta calidad de las bases de datos ocultas de la Web, que generalmente están organizados por expertos en el dominio. Otra opción es los datos de noticias/gobierno de TREC [1,15,17,18,21]. Los datos gubernamentales/noticias de TREC se centran en temas relativamente específicos. Comparado con los datos web de TREC: i) Los documentos de noticias/gobierno son mucho más similares a los contenidos proporcionados por una base de datos orientada a temas que a una página web arbitraria, ii) Una base de datos en este banco de pruebas es más grande que la de los datos web de TREC. En promedio, una base de datos contiene miles de documentos, lo cual es más realista que una base de datos de datos web de TREC con alrededor de 250 documentos. Dado que los contenidos y tamaños de las bases de datos en el banco de pruebas de noticias/gobierno de TREC son más similares a los de una base de datos orientada a temas, es un buen candidato para simular los entornos de recuperación de información distribuida de grandes organizaciones (empresas) o sitios web ocultos específicos de dominio, como West, que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3]. Dado que la mayoría de los sistemas actuales de recuperación de información distribuida están desarrollados para entornos de grandes organizaciones (empresas) o para la Web oculta de dominios específicos en lugar de la Web oculta de dominio abierto, en este trabajo se eligió el banco de pruebas de noticias/gobierno de TREC. El banco de pruebas Trec123-100col-bysource es uno de los más utilizados en las pruebas de noticias y gobierno de TREC [1,15,17,21]. Fue elegido en este trabajo. Tres bancos de pruebas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes también se utilizaron para proporcionar una simulación más exhaustiva para entornos reales. Se crearon 100 bases de datos a partir de los CDs de TREC 1, 2 y 3. Fueron organizados por fuente y fecha de publicación [1]. Los tamaños de las bases de datos no están sesgados. Los detalles se encuentran en la Tabla 1. Tres bancos de pruebas construidos en [21] se basaron en el banco de pruebas trec123-100colbysource. Cada banco de pruebas contiene muchas bases de datos pequeñas y dos bases de datos grandes creadas al fusionar alrededor de 10 a 20 bases de datos pequeñas. Listas completas de probabilidades de relevancia para todas las bases de datos |DB|. Solución de selección óptima para la Ecuación 16. i) Crear el arreglo tridimensional: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Cada Sel (x, y, z) está asociado con una decisión de selección xyzd, que representa la mejor decisión de selección en la condición: solo se consideran bases de datos del número 1 al número x para la selección; se recuperarán un total de y*10 documentos; solo se seleccionan z bases de datos de los candidatos de la base de datos x. Y Sel (x, y, z) es el valor de utilidad correspondiente al elegir la mejor selección. ii) Inicializar Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) solo con la información de relevancia estimada de la 1ª base de datos. iii) Iterar el candidato actual de la base de datos i desde 2 hasta |DB| Para cada entrada Sel (i, y, z): Encontrar k tal que: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k∗ documentos de la base de datos i-ésima, de lo contrario no debemos seleccionar esta base de datos y se debe mantener la solución anterior mejor Sel (i-1, y, z). Luego establezca el valor de iyzd y Sel (i, y, z) en consecuencia. iv) La mejor solución de selección se da por _ /10| | Toral rdoc sdbDB N Nd y el valor de utilidad correspondiente es Sel (|DB|, NTotal_rdoc/10, Nsdb). Figura 2. El procedimiento de optimización de programación dinámica para la Ecuación 16. Tabla 1: Estadísticas del banco de pruebas. Número de documentos Tamaño (MB) Tamaño del banco de pruebas (GB) Mínimo Promedio Máximo Mínimo Promedio Máximo Trec123 3.2 752 10782 39713 28 32 42 Tabla 2: Estadísticas del conjunto de consultas. Nombre del conjunto de temas TREC Campo del tema TREC Longitud promedio (palabras) Trec123 51-150 Título 3.1 37 Trec123-2ldb-60col (representativo): Las bases de datos en el trec123-100col-bysource se ordenaron en orden alfabético. Dos grandes bases de datos fueron creadas al fusionar 20 bases de datos pequeñas con el método de round-robin. Por lo tanto, las dos bases de datos grandes tienen más documentos relevantes debido a sus tamaños grandes, aunque las densidades de documentos relevantes son aproximadamente iguales a las de las bases de datos pequeñas. Las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, APall y WSJall. Las otras 60 colecciones quedaron sin cambios. Las bases de datos APall y WSJall tienen una mayor densidad de documentos relevantes para las consultas de TREC que las bases de datos pequeñas. Por lo tanto, las dos bases de datos grandes tienen muchos más documentos relevantes que las bases de datos pequeñas. Las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, FRall y DOEall. Las otras 80 colecciones quedaron sin cambios. Las bases de datos FRall y DOEall tienen densidades más bajas de documentos relevantes para las consultas de TREC que las bases de datos pequeñas, a pesar de ser mucho más grandes. Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150. Las consultas 101-150 se utilizaron como consultas de entrenamiento y las consultas 51-100 se utilizaron como consultas de prueba (detalles en la Tabla 2). 4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o en la Web oculta específica de dominio, diferentes bases de datos pueden utilizar diferentes tipos de motores de búsqueda. Para simular el entorno de múltiples motores de búsqueda, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: INQUERY [2], un modelo de lenguaje estadístico de unigrama con suavizado lineal [12,20] y un algoritmo de recuperación TFIDF con peso ltc [12,20]. Todos estos algoritmos fueron implementados con la herramienta Lemur [12]. Estos tres tipos de motores de búsqueda fueron asignados a las bases de datos entre los cuatro bancos de pruebas de manera round-robin. 5. RESULTADOS: SELECCIÓN DE RECURSOS DE LA RECOMENDACIÓN DE BASES DE DATOS Todos los cuatro bancos de pruebas descritos en la Sección 4 fueron utilizados en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de bases de datos. Las descripciones de los recursos fueron creadas utilizando muestreo basado en consultas. Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos. Las estadísticas del tamaño de la base de datos fueron estimadas mediante el método de muestra y remuestra [21]. Cincuenta consultas (101-150) se utilizaron como consultas de entrenamiento para construir el modelo logístico relevante y ajustar las funciones exponenciales de las curvas de puntuación de documentos centralizados para bases de datos de gran proporción (detalles en la Sección 3.1). Otros 50 consultas (51-100) se utilizaron como datos de prueba. Los algoritmos de selección de recursos de los sistemas de recomendación de bases de datos suelen compararse utilizando la métrica de recuperación nR [1,17,18,21]. Que B denote una clasificación base, que a menudo es la RBR (clasificación basada en relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos. Y que Bi y Ei denoten el número de documentos relevantes en la base de datos clasificada i-ésima de B o E. Entonces, Rn se define de la siguiente manera: = = = k i i k i i k B E R 1 1 (17) Por lo general, el objetivo es buscar solo algunas bases de datos, por lo que nuestras cifras solo muestran resultados para la selección de hasta 20 bases de datos. Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, CORI, ReDDE y UUM/HR. El algoritmo UUM/HR se describe en la Sección 3.3. Se puede observar en la Figura 3 que los algoritmos ReDDE y UUM/HR son más efectivos (en los conjuntos de pruebas representativos, relevantes y no relevantes) o igual de efectivos (en el conjunto de pruebas Trec123-100Col) que el algoritmo de selección de recursos CORI. El algoritmo UUM/HR es más efectivo que el algoritmo ReDDE en los conjuntos de pruebas representativos y relevantes y es aproximadamente igual que el algoritmo ReDDE en los conjuntos de pruebas Trec123100Col y no relevantes. Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo ReDDE. Se puede observar que al seleccionar solo algunas bases de datos en el Trec123-100Col o en los conjuntos de pruebas no relevantes, el algoritmo ReDEE tiene una pequeña ventaja sobre el algoritmo UUM/HR. Atribuimos esto a dos causas: i) El algoritmo ReDDE fue ajustado en el banco de pruebas Trec123-100Col; y ii) Aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico para estimar probabilidades de relevancia no es lo suficientemente preciso. Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este pequeño rompecabezas. Colecciones seleccionadas. Colecciones seleccionadas. Plataforma de pruebas Trec123-100Col. Plataforma de pruebas representativa. Colección seleccionada. Colección seleccionada. Plataforma de pruebas relevante. Plataforma de pruebas no relevante. Figura 3. Experimentos de selección de recursos en los cuatro bancos de pruebas. 38 6. RESULTADOS: EFECTIVIDAD DE LA RECUPERACIÓN DE DOCUMENTOS Para la recuperación de documentos, se buscan en las bases de datos seleccionadas y los resultados devueltos se fusionan en una lista final única. En todos los experimentos discutidos en esta sección, los resultados obtenidos de bases de datos individuales fueron combinados por el algoritmo de fusión de resultados de aprendizaje semisupervisado. Esta versión del algoritmo SSL [22] tiene permitido descargar un pequeño número de textos de documentos devueltos sobre la marcha para crear datos de entrenamiento adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicos de la base de datos en puntuaciones de documentos centralizadas estimadas. Se ha demostrado ser muy efectivo en entornos donde solo se obtienen listas de resultados cortas de cada base de datos seleccionada [22]. Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos. La efectividad de la recuperación de documentos se midió mediante la Precisión en la parte superior de la lista final de documentos. Los experimentos en esta sección se llevaron a cabo para estudiar la efectividad de recuperación de documentos de cinco algoritmos de selección, a saber, los algoritmos CORI, ReDDE, UUM/HR, UUM/HP-FL y UUM/HP-VL. Los últimos tres algoritmos fueron propuestos en la Sección 3. Todos los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada. El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos a recuperar de cada base de datos seleccionada; el número recuperado estaba limitado a ser de 10 a 100, y un múltiplo de 10. El Trec123-100Col y los bancos de pruebas representativos fueron seleccionados para la recuperación de documentos, ya que representan dos casos extremos de efectividad en la selección de recursos; en un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante Tabla 5. Precisión en el banco de pruebas representativo cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 documentos 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 documentos 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 documentos 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 documentos 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Tabla 6. Precisión en el banco de pruebas representativo cuando se seleccionaron 5 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 documentos 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 documentos 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 documentos 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 documentos 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Tabla 3. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 documentos 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 documentos 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 documentos 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 documentos 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Tabla 4. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 5 bases de datos. (El primer punto de referencia es CORI; el segundo punto de referencia para los métodos UUM/HP es UUM/HR). La precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 documentos 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 documentos 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 documentos 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 documentos 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 mucho peor que los otros algoritmos. Las Tablas 3 y 4 muestran los resultados en el banco de pruebas Trec123-100Col, y las Tablas 5 y 6 muestran los resultados en el banco de pruebas representativo. En el banco de pruebas Trec123-100Col, la efectividad de recuperación de documentos del algoritmo de selección CORI es aproximadamente la misma o un poco mejor que el algoritmo ReDDE, pero ambos son peores que los otros tres algoritmos (Tablas 3 y 4). El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos CORI y ReDDE. Una de las principales diferencias entre el algoritmo UUM/HR y el algoritmo ReDDE fue señalada anteriormente: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntuación de documentos centralizadas, mientras que el algoritmo ReDDE [21] utiliza un método heurístico, asume que las curvas de puntuación de documentos centralizadas son funciones escalonadas y no hace distinción entre la parte superior de las curvas. Esta diferencia hace que UUM/HR sea mejor que el algoritmo ReDDE para distinguir documentos con altas probabilidades de relevancia de aquéllos con bajas probabilidades de relevancia. Por lo tanto, el UUM/HR refleja mejor el objetivo de recuperación de alta precisión que el algoritmo ReDDE y, por lo tanto, es más efectivo para la recuperación de documentos. El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, como lo hacen los algoritmos UUM/HP-FL y UUM/HP-VL. Se puede observar que en este banco de pruebas, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos. Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de recuperación de documentos en las Ecuaciones 14 y 16. En el banco de pruebas representativo, CORI es mucho menos efectivo que otros algoritmos para la recuperación distribuida de documentos (Tablas 5 y 6). Los resultados de recuperación de documentos del algoritmo ReDDE son mejores que los del algoritmo CORI pero aún peores que los resultados del algoritmo UUM/HR. En este banco de pruebas, los tres algoritmos de UUM son aproximadamente igual de efectivos. Un análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho mayor que los experimentos en el banco de pruebas Trec123-100Col, ya que todos tienden a seleccionar las dos bases de datos grandes. Esto explica por qué son igualmente efectivos para la recuperación de documentos. En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos y reportar solo listas clasificadas de resultados. Dado que el modelo unificado de maximización de utilidad solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos. El único ajuste es que el algoritmo SSL fusiona listas clasificadas sin puntuaciones de documentos asignando a los documentos puntuaciones de pseudo-documentos normalizadas por sus rangos (En una lista clasificada de 50 documentos, el primero tiene una puntuación de 1, el segundo tiene una puntuación de 0.98, etc.), lo cual ha sido estudiado en [22]. Los resultados del experimento en el banco de pruebas trec123-100Col-bysource con 3 bases de datos seleccionadas se muestran en la Tabla 7. La configuración del experimento fue la misma que antes, excepto que las puntuaciones de los documentos fueron eliminadas intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de identificadores de documentos. Se puede observar en los resultados que el UUM/HP-FL y el UUM/HP-VL funcionan bien con bases de datos que no devuelven puntuaciones de documentos y siguen siendo más efectivos que otras alternativas. Otros experimentos con bases de datos que no devuelven puntuaciones de documentos no se informan, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL. Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión de manera explícita en la recuperación de documentos. Los nuevos algoritmos basados en este principio logran resultados mejores o al menos tan buenos como los algoritmos previos de vanguardia en varios entornos. CONCLUSIÓN La recuperación distribuida de información resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e Internet. La mayoría de investigaciones previas utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de bases de datos para la aplicación de recuperación de documentos distribuidos. Sostenemos que el objetivo de alta recuperación de recursos en la recomendación de bases de datos y el objetivo de alta precisión en la recuperación de documentos están relacionados pero no son idénticos. Este tipo de inconsistencia también ha sido observada en trabajos anteriores, pero las soluciones previas utilizaron métodos heurísticos o asumieron la cooperación de bases de datos individuales (por ejemplo, que todas las bases de datos utilizaran el mismo tipo de motores de búsqueda), lo cual frecuentemente no es cierto en un entorno no cooperativo. En este trabajo proponemos un modelo unificado de maximización de utilidad para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado. En este marco, las decisiones de selección se obtienen optimizando diferentes funciones objetivo. Hasta donde sabemos, este es el primer trabajo que intenta visualizar y modelar teóricamente la tarea de recuperación de información distribuida de manera integrada. El nuevo marco continúa una tendencia reciente de investigación que estudia el uso de muestreo basado en consultas y una base de datos de muestras centralizada. Se entrenó un único modelo logístico en la Tabla 7 centralizada. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). (Los motores de búsqueda no devuelven puntajes de documentos) Precisión en la Clasificación de Documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 documentos 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 documentos 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 documentos 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 documentos 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 base de datos de muestra para estimar las probabilidades de relevancia de documentos por sus puntajes de recuperación centralizados, mientras que la base de datos de muestra centralizada sirve como puente para conectar las bases de datos individuales con el modelo logístico centralizado. Por lo tanto, las probabilidades de relevancia para todos los documentos en las bases de datos pueden ser estimadas con una cantidad muy pequeña de juicio de relevancia humano, lo cual es mucho más eficiente que los métodos anteriores que construyen un modelo separado para cada base de datos. Este marco no solo es más sólido teóricamente, sino también muy efectivo. Un algoritmo para la selección de recursos (UUM/HR) y dos algoritmos para la recuperación de documentos (UUM/HP-FL y UUM/HP-VL) se derivan de este marco. Se han realizado estudios empíricos en bancos de pruebas para simular las soluciones de búsqueda distribuida de grandes organizaciones (empresas) o la Web oculta específica de un dominio. Además, los algoritmos de selección de recursos UUM/HP-FL y UUM/HP-VL se amplían con una variante del algoritmo de fusión de resultados SSL para abordar la tarea de recuperación de documentos distribuidos cuando las bases de datos seleccionadas no devuelven puntuaciones de documentos. Los experimentos han demostrado que estos algoritmos logran resultados que son al menos tan buenos como el estado del arte previo, y a veces considerablemente mejores. Un análisis detallado indica que la ventaja de estos algoritmos proviene de optimizar explícitamente los objetivos de las tareas específicas. El marco unificado de maximización de utilidad está abierto a diferentes extensiones. Cuando el costo está asociado con la búsqueda en las bases de datos en línea, el marco de utilidad puede ajustarse para estimar automáticamente el mejor número de bases de datos a buscar, de modo que se puedan recuperar una gran cantidad de documentos relevantes con costos relativamente bajos. Otra extensión del marco es considerar la efectividad de la recuperación de información de las bases de datos en línea, lo cual es un tema importante en los entornos operativos. Todas estas son las direcciones de la investigación futura. AGRADECIMIENTO Esta investigación fue apoyada por las subvenciones de la NSF EIA-9983253 y IIS-0118767. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este documento son del autor y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] J. Callan. (2000). Recuperación de información distribuida. En W.B. Croft, editor, Avances en Recuperación de Información. Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. \n\nEditorial Kluwer Academic. (pp. 127-150). [2] J. Callan, W.B. Croft, y J. Broglio. (1995). Experimentos TREC y TIPSTER con INQUERY. Procesamiento y Gestión de la Información, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson y M. Meziou. (2002). Selección de base de datos utilizando recursos de colección lógica adquiridos y físicos reales en un entorno operativo masivo específico de dominio. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [4] N. Craswell. (2000). Métodos para la recuperación distribuida de información. I'm sorry, but the sentence \"Ph.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate to Spanish? Tesis doctoral, Universidad Nacional Australiana. [5] N. Craswell, D. Hawking y P. Thistlewaite. (1999). Combinando resultados de motores de búsqueda aislados. En Actas de la 10ª Conferencia de Bases de Datos Australasiana. [6] D. DSouza, J. Thom y J. Zobel. (2000). Una comparación de técnicas para seleccionar colecciones de texto. En Actas de la 11ª Conferencia de Bases de Datos Australasiana. [7] N. Fuhr. (1999). Un enfoque de Teoría de la Decisión para la selección de bases de datos en IR en red. ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina y A. Paepcke. (1997). Propuesta de Stanford para la metabusqueda en internet. En Actas de la 20ª Conferencia Internacional ACM-SIGMOD sobre Gestión de Datos. [9] L. Gravano, P. Ipeirotis y M. Sahami. (2003). QProber: Un sistema para la clasificación automática de bases de datos de la web oculta. ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis y L. Gravano. (2002). Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] El kit de herramientas lemur. http://www.cs.cmu.edu/~lemur [13] J. Lu y J. Callan. (2003). Recuperación de información basada en contenido en redes peer-to-peer. En Actas de la 12ª Conferencia Internacional sobre Información y Gestión del Conocimiento. [14] W. Meng, C.T. Yu y K.L. Liu. (2002) Construcción de motores de búsqueda eficientes y efectivos. ACM Comput. Surv. 34(1). [15] H. Nottelmann y N. Fuhr. (2003). Evaluando diferentes métodos para estimar la calidad de recuperación para la selección de recursos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [16] H., Nottelmann y N., Fuhr. (2003). La arquitectura MIND para bibliotecas digitales federadas de multimedia heterogénea. Taller ACM SIGIR 2003 sobre Recuperación de Información Distribuida. [17] A.L. Powell, J.C. French, J. Callan, M. Connell y C.L. Viles. (2000). \n\nViles. (2000). El impacto de la selección de bases de datos en la búsqueda distribuida. En Actas de la 23ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [18] A.L. Powell y J.C. French. (2003). Comparando el rendimiento de los algoritmos de selección de bases de datos. ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). \n\nACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). Busca en la web invisible. Guardian Unlimited. [20] L. Si y J. Callan. (2002). Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [21] L. Si y J. Callan. (2003). Método de estimación de distribución de documentos relevantes para la selección de recursos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [22] L. Si y J. Callan. (2003). Un método de aprendizaje semi-supervisado para fusionar los resultados de un motor de búsqueda. ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41\n\nACM Transactions on Information Systems, 21(4). (pp. 457-491). 41",
    "original_sentences": [
        "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
        "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
        "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
        "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
        "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
        "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
        "This new model provides a more solid framework for distributed information retrieval.",
        "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
        "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
        "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
        "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
        "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
        "Therefore, it is very important to search this type of valuable information.",
        "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
        "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
        "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
        "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
        "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
        "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
        "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
        "There are three important sub-problems in distributed information retrieval.",
        "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
        "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
        "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
        "Many types of solutions exist for distributed information retrieval.",
        "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
        "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
        "It is composed of the resource description and the resource selection components.",
        "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
        "Distributed document retrieval is a more sophisticated task.",
        "It selects relevant information sources for users queries as the database recommendation system does.",
        "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
        "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
        "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
        "Prior research indicated that these two goals are related but not identical [4,21].",
        "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
        "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
        "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
        "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
        "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
        "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
        "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
        "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
        "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
        "This selection criterion meets the high-precision goal of distributed document retrieval application.",
        "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
        "The unified utility framework makes very few assumptions and works in uncooperative environments.",
        "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
        "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
        "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
        "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
        "The unified utility framework is not only more theoretically solid but also very effective.",
        "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
        "The next section discusses related work.",
        "Section 3 describes the new unified utility maximization model.",
        "Section 4 explains our experimental methodology.",
        "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
        "Section 7 concludes. 2.",
        "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
        "We survey the most related work in this section.",
        "The first problem of distributed information retrieval is resource representation.",
        "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
        "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
        "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
        "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
        "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
        "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
        "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
        "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
        "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
        "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
        "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
        "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
        "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
        "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
        "The SSL algorithm has been shown to be effective [22].",
        "It serves as an important component of our unified utility maximization framework (Section 3).",
        "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
        "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
        "This type of inconsistency has been observed in previous research [4,21].",
        "The research in [21] tried to solve the problem with a heuristic method.",
        "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
        "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
        "However, two points distinguish our research from the DTF model.",
        "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
        "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
        "This requires human relevance judgments for the results retrieved from each database.",
        "In contrast, our approach only builds one logistic model for the centralized sample database.",
        "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
        "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
        "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
        "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
        "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
        "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
        "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
        "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
        "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
        "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
        "The SSL results merging algorithm [20,22] is used to merge the results.",
        "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
        "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
        "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
        "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
        "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
        "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
        "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
        "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
        "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
        "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
        "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
        "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
        "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
        "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
        "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
        "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
        "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
        "Specifically, a logistic model is built for each of these databases.",
        "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
        "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
        "Together with the scores of the top two sampled documents, these parameters can be estimated.",
        "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
        "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
        "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
        "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
        "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
        "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
        "In the task of database recommendation, the system needs to decide how to rank databases.",
        "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
        "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
        "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
        "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
        "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
        "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
        "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
        "Finally, to each selection action d and a set of complete lists of Figure 1.",
        "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
        "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
        "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
        "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
        "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
        "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
        "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
        "It is measured by the Precision at the top part of the final merged document list.",
        "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
        "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
        "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
        "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
        "A more complex situation is to vary the number of retrieved documents from each selected database.",
        "More specifically, we allow different selected databases to return different numbers of documents.",
        "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
        "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
        "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
        "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
        "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
        "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
        "The basic steps of this dynamic programming method are described in Figure 2.",
        "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
        "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
        "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
        "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
        "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
        "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
        "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
        "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
        "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
        "These types of web pages are contained in the WT2g/WT10g datasets.",
        "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
        "Another choice is the TREC news/government data [1,15,17, 18,21].",
        "TREC news/government data is concentrated on relatively narrow topics.",
        "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
        "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
        "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
        "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
        "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
        "It was chosen in this work.",
        "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
        "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
        "They were organized by source and publication date [1].",
        "The sizes of the databases are not skewed.",
        "Details are in Table 1.",
        "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
        "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
        "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
        "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
        "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
        "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
        "Figure 2.",
        "The dynamic programming optimization procedure for Equation 16.",
        "Table1: Testbed statistics.",
        "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
        "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
        "Two large databases were created by merging 20 small databases with the round-robin method.",
        "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
        "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
        "The other 60 collections were left unchanged.",
        "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
        "Thus, the two large databases have many more relevant documents than the small databases.",
        "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
        "The other 80 collections were left unchanged.",
        "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
        "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
        "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
        "All these algorithms were implemented with the Lemur toolkit [12].",
        "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
        "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
        "The resource descriptions were created using query-based sampling.",
        "About 80 queries were sent to each database to download 300 unique documents.",
        "The database size statistics were estimated by the sample-resample method [21].",
        "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
        "Another 50 queries (51-100) were used as test data.",
        "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
        "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
        "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
        "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
        "The UUM/HR algorithm is described in Section 3.3.",
        "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
        "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
        "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
        "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
        "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
        "More training data or a more sophisticated model may help to solve this minor puzzle.",
        "Collections Selected.",
        "Collections Selected.",
        "Trec123-100Col Testbed.",
        "Representative Testbed.",
        "Collection Selected.",
        "Collection Selected.",
        "Relevant Testbed.",
        "Nonrelevant Testbed.",
        "Figure 3.",
        "Resource selection experiments on the four testbeds. 38 6.",
        "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
        "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
        "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
        "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
        "This is a common scenario in operational environments and was the case for our experiments.",
        "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
        "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
        "The last three algorithms were proposed in Section 3.",
        "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
        "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
        "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
        "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
        "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
        "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
        "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
        "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
        "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
        "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
        "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
        "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
        "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
        "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
        "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
        "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
        "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
        "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
        "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
        "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
        "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
        "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
        "On this testbed the three UUM algorithms are about equally effective.",
        "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
        "This explains why they are about equally effective for document retrieval.",
        "In real operational environments, databases may return no document scores and report only ranked lists of results.",
        "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
        "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
        "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
        "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
        "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
        "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
        "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
        "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
        "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
        "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
        "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
        "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
        "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
        "In this framework, the selection decisions are obtained by optimizing different objective functions.",
        "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
        "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
        "A single logistic model was trained on the centralized Table 7.",
        "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
        "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
        "This framework is not only more theoretically solid but also very effective.",
        "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
        "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
        "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
        "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
        "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
        "The unified utility maximization framework is open for different extensions.",
        "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
        "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
        "All of these are the directions of future research.",
        "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
        "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
        "REFERENCES [1] J. Callan. (2000).",
        "Distributed information retrieval.",
        "In W.B.",
        "Croft, editor, Advances in Information Retrieval.",
        "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
        "Croft, and J. Broglio. (1995).",
        "TREC and TIPSTER experiments with INQUERY.",
        "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
        "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
        "Distributed search over the hidden web: Hierarchical database sampling and selection.",
        "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
        "Methods for distributed information retrieval.",
        "Ph.",
        "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
        "Merging results from isolated search engines.",
        "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
        "A comparison of techniques for selecting text collections.",
        "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
        "A Decision-Theoretic approach to database selection in networked IR.",
        "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
        "STARTS: Stanford proposal for internet metasearching.",
        "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
        "QProber: A System for Automatic Classification of Hidden-Web Databases.",
        "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
        "Distributed search over the hidden web: Hierarchical database sampling and selection.",
        "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
        "Content-based information retrieval in peer-to-peer networks.",
        "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
        "Yu and K.L.",
        "Liu. (2002) Building efficient and effective metasearch engines.",
        "ACM Comput.",
        "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
        "Evaluating different method of estimating retrieval quality for resource selection.",
        "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
        "The MIND architecture for heterogeneous multimedia federated digital libraries.",
        "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
        "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
        "Viles. (2000).",
        "The impact of database selection on distributed searching.",
        "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
        "Powell and J.C. French. (2003).",
        "Comparing the performance of database selection algorithms.",
        "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
        "Search for the invisible web.",
        "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
        "Using sampled data and regression to merge search engine results.",
        "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
        "Relevant document distribution estimation method for resource selection.",
        "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
        "A Semi-Supervised learning method to merge search engine results.",
        "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
    ],
    "translated_text_sentences": [
        "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si.",
        "Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje.",
        "Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida.",
        "Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto.",
        "Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones.",
        "Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos).",
        "Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información.",
        "Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1.",
        "INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos.",
        "La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos.",
        "Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales.",
        "Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales.",
        "Por lo tanto, es muy importante buscar este tipo de información valiosa.",
        "La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales.",
        "En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda.",
        "La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8].",
        "Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere.",
        "Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido.",
        "Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas.",
        "En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes.",
        "Hay tres subproblemas importantes en la recuperación de información distribuida.",
        "Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21].",
        "Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21].",
        "Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22].",
        "Existen muchos tipos de soluciones para la recuperación de información distribuida.",
        "Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares.",
        "Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios.",
        "Está compuesto por la descripción del recurso y los componentes de selección de recursos.",
        "Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente.",
        "La recuperación distribuida de documentos es una tarea más sofisticada.",
        "Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos.",
        "Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios.",
        "El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo.",
        "Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión.",
        "Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21].",
        "Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21].",
        "Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de recuperación de documentos distribuidos, tratándolos como objetivos de optimización diferentes.",
        "Primero, se construye una base de datos de muestra centralizada mediante el muestreo aleatorio de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas; también se estiman las estadísticas del tamaño de la base de datos.",
        "Un modelo de transformación logística se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia.",
        "Segundo, después de que se envía una nueva consulta, la consulta se puede utilizar para buscar en la base de datos de muestras centralizada que produce una puntuación para cada documento muestreado.",
        "La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada puede estimarse aplicando el modelo logístico al puntaje de cada documento.",
        "Entonces, las probabilidades de relevancia de todos los documentos (en su mayoría no vistos) entre las bases de datos disponibles pueden ser estimadas utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos.",
        "Para la tarea de selección de recursos para un sistema de recomendación de bases de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alto recall.",
        "Para la selección de recursos para un sistema distribuido de recuperación de documentos, se prefieren las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia.",
        "Este criterio de selección cumple con el objetivo de alta precisión de la aplicación de recuperación de documentos distribuidos.",
        "Además, se aplica el algoritmo de aprendizaje semisupervisado (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada.",
        "El marco de utilidad unificado hace muy pocas suposiciones y funciona en entornos no cooperativos.",
        "Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos correspondientes; y ii) Muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos.",
        "Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades correspondientes de relevancia y utiliza la base de datos de muestra centralizada como puente entre las bases de datos individuales y el modelo logístico.",
        "El esfuerzo humano (juicio de relevancia) necesario para entrenar el modelo logístico centralizado único no aumenta con el número de bases de datos.",
        "Esta es una gran ventaja sobre investigaciones anteriores, las cuales requerían que la cantidad de esfuerzo humano fuera lineal con el número de bases de datos [7,15].",
        "El marco de utilidad unificada no solo es más sólido teóricamente, sino también muy efectivo.",
        "Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de vanguardia en una variedad de configuraciones.",
        "La siguiente sección discute el trabajo relacionado.",
        "La sección 3 describe el nuevo modelo unificado de maximización de utilidad.",
        "La sección 4 explica nuestra metodología experimental.",
        "Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos.",
        "La sección 7 concluye. 2.",
        "Investigación previa Ha habido una considerable investigación sobre todos los subproblemas de la recuperación de información distribuida.",
        "Exploramos los trabajos más relacionados en esta sección.",
        "El primer problema de la recuperación de información distribuida es la representación de recursos.",
        "El protocolo STARTS es una solución para adquirir descripciones de recursos en entornos cooperativos [8].",
        "Sin embargo, en entornos no cooperativos, aunque las bases de datos estén dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no.",
        "Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí.",
        "Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en consultas, que genera y envía consultas de forma aleatoria a motores de búsqueda individuales y recupera algunos documentos para construir las descripciones.",
        "Dado que los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico.",
        "Los experimentos han demostrado que descripciones de recursos bastante precisas pueden ser construidas enviando alrededor de 80 consultas y descargando alrededor de 300 documentos [1].",
        "Muchos algoritmos de selección de recursos como gGlOSS/vGlOSS [8] y CORI [1] han sido propuestos en la última década.",
        "El algoritmo CORI representa cada base de datos por sus términos, las frecuencias de los documentos y un pequeño número de estadísticas del corpus (detalles en [1]).",
        "Como investigaciones previas en diferentes conjuntos de datos han demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo utilizamos como algoritmo base en este trabajo.",
        "El algoritmo de selección de recursos de estimación de distribución de documentos relevantes (ReDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia.",
        "Aunque se ha demostrado que el algoritmo ReDDE es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21].",
        "El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar puntuaciones de documentos específicas de la base de datos en puntuaciones de documentos independientes de la base de datos comparables.",
        "El algoritmo de fusión de resultados de aprendizaje semisupervisado (SSL) [20,22] utiliza los documentos adquiridos mediante muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos y de la consulta.",
        "Estos modelos lineales se utilizan para convertir las puntuaciones de documentos específicas de la base de datos en las puntuaciones de documentos centralizadas aproximadas.",
        "El algoritmo SSL ha demostrado ser efectivo [22].",
        "Sirve como un componente importante de nuestro marco unificado de maximización de utilidad (Sección 3).",
        "Para lograr resultados precisos en la recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos en sistemas de recomendación de bases de datos.",
        "Pero como se señaló anteriormente, un algoritmo de selección de recursos optimizado para un alto recuerdo puede no funcionar bien para la recuperación de documentos, que tiene como objetivo la alta precisión.",
        "Este tipo de inconsistencia ha sido observada en investigaciones previas [4,21].",
        "La investigación en [21] intentó resolver el problema con un método heurístico.",
        "La investigación más similar a lo que proponemos aquí es el marco teórico de la toma de decisiones (DTF) [7,15].",
        "Este marco de trabajo calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y se han propuesto varios métodos [15] para estimar la calidad de recuperación.",
        "Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF.",
        "Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de bases de datos y recuperación distribuida de documentos) en el mismo marco unificado.",
        "Segundo, el DTF construye un modelo para cada base de datos para calcular las probabilidades de relevancia.",
        "Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos.",
        "Por el contrario, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada.",
        "La base de datos de muestra centralizada puede servir como puente para conectar las bases de datos individuales con el modelo logístico centralizado, de esta manera se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos.",
        "Esta estrategia puede ahorrar una gran cantidad de esfuerzo en juicio humano y es una gran ventaja del marco de maximización de utilidad unificada sobre el DTF, especialmente cuando hay un gran número de bases de datos.",
        "MARCO DE MAXIMIZACIÓN DE UTILIDAD UNIFICADA El marco de Maximización de Utilidad Unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría no vistos) disponibles en el entorno de búsqueda distribuida.",
        "En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el modelo de Maximización de Utilidad Unificado.",
        "También describimos cómo el modelo puede ser optimizado para el objetivo de alto recuerdo de un sistema de recomendación de base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido. 3.1 Estimación de Probabilidades de Relevancia Como se señaló anteriormente, el propósito de la selección de recursos es el alto recuerdo y el propósito de la recuperación de documentos es la alta precisión.",
        "Para cumplir con estos objetivos diversos, el problema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos.",
        "Este es un problema difícil porque solo podemos observar una muestra de los contenidos de cada base de datos utilizando muestreo basado en consultas.",
        "Nuestra estrategia es aprovechar al máximo toda la información disponible para calcular las estimaciones de probabilidad. 3.1.1 Aprendizaje de Probabilidades de Relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se construye mediante muestreo basado en consultas y los tamaños de la base de datos se estiman utilizando el método de muestreo y remuestreo [21].",
        "Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (Inquery [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de entrenamiento.",
        "Para cada consulta de entrenamiento, se aplica el algoritmo de selección de recursos CORI [1] para seleccionar un cierto número (por ejemplo, 10) de bases de datos y recuperar 50 identificadores de documentos de cada base de datos.",
        "El algoritmo de fusión de resultados SSL [20,22] se utiliza para combinar los resultados.",
        "Luego, podemos descargar los 50 documentos principales de la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando Inquery y las estadísticas del corpus de la base de datos de muestra centralizada.",
        "Las puntuaciones centralizadas se normalizan aún más (dividiéndolas por la puntuación centralizada máxima para cada consulta), ya que este método ha sido sugerido para mejorar la precisión de la estimación en investigaciones anteriores [15].",
        "El juicio humano se adquiere para esos documentos y se construye un modelo logístico para transformar las puntuaciones de documentos centralizados normalizados en probabilidades de relevancia de la siguiente manera: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) donde )( _ dSc es la puntuación de documento centralizada normalizada y ac y bc son los dos parámetros del modelo logístico.",
        "Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de entrenamiento.",
        "El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de las puntuaciones de documentos centralizadas. 3.1.2 Estimación de las puntuaciones de documentos centralizadas Cuando el usuario envía una nueva consulta, se calculan las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada.",
        "Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar las puntuaciones de los documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados.",
        "Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas del tamaño de la base de datos.",
        "Definimos el factor de escala de la base de datos para la base de datos i como la razón entre el tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: SF_i = ^N_db / _N_db_samp_i donde ^N_db es el tamaño estimado de la base de datos y _N_db_samp_i es el número de documentos de la base de datos i en la base de datos de muestra centralizada.",
        "La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podríamos suponer que hay alrededor de 50 documentos en esa base de datos que tienen puntuaciones de alrededor de 0.5.",
        "De hecho, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntuación del documento centralizado para cada base de datos.",
        "Formalmente, clasificamos todos los documentos muestreados de la base de datos i-ésima por sus puntajes de documento centralizado 34 para obtener la lista de puntajes de documento centralizado muestreado {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} para la base de datos i; asumimos que si pudiéramos calcular los puntajes de documento centralizado para todos los documentos en esta base de datos y obtener la lista completa de puntajes de documento centralizado, el documento superior en la lista muestreada tendría un rango de SFdbi/2, el segundo documento en la lista muestreada tendría un rango de SFdbi3/2, y así sucesivamente.",
        "Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
        "La interpolación lineal por tramos se aplica para estimar la curva de puntuación del documento centralizado, como se ilustra en la Figura 1.",
        "La lista completa de puntuaciones de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documentos centralizados como: ],1[,)(S ^^ c idbij Njd ∈ .",
        "Se puede observar en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntuación del documento centralizado.",
        "Sin embargo, para bases de datos con grandes proporciones de escala de base de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos mejor clasificados (por ejemplo, [1, SFdbi/2]).",
        "Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizados de los documentos mejor clasificados para bases de datos con ratios a gran escala (por ejemplo, mayores de 100).",
        "Específicamente, se construye un modelo logístico para cada una de estas bases de datos.",
        "El modelo logístico se utiliza para estimar la puntuación del documento centralizado superior 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con las puntuaciones centralizadas más altas. 0iα , 1iα y 2iα son los parámetros del modelo logístico.",
        "Para cada consulta de entrenamiento, se descarga el documento mejor recuperado de cada base de datos y se calcula la puntuación del documento centralizado correspondiente.",
        "Junto con las puntuaciones de los dos documentos muestreados principales, estos parámetros pueden ser estimados.",
        "Después de estimar la puntuación centralizada del documento principal, se ajusta una función exponencial para la parte superior ([1, SFdbi/2]) de la curva de puntuación del documento centralizado como: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) Los dos parámetros 0iβ y 1iβ se ajustan para asegurarse de que la función exponencial pase por los dos puntos (1, ^ 1)( ic dS ) y (SFdbi/2, Sc(dsi1)).",
        "La función exponencial se utiliza únicamente para ajustar la parte superior de la curva de puntuación del documento centralizado, mientras que la parte inferior de la curva sigue siendo ajustada con el método de interpolación lineal descrito anteriormente.",
        "El ajuste mediante la función exponencial de los documentos mejor clasificados ha demostrado empíricamente producir resultados más precisos.",
        "A partir de las curvas de puntuación de documentos centralizadas, podemos estimar las listas completas de puntuación de documentos centralizados correspondientes para todas las bases de datos disponibles.",
        "Después de que las puntuaciones estimadas de los documentos centralizados se normalizan, las listas completas de probabilidades de relevancia pueden ser construidas a partir de las listas completas de puntuaciones de documentos centralizados mediante la Ecuación 1.",
        "Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El Modelo Unificado de Maximización de Utilidad En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco.",
        "En la tarea de recomendación de bases de datos, el sistema necesita decidir cómo clasificar las bases de datos.",
        "En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada.",
        "Generalizamos el proceso de selección de recomendaciones de bases de datos, que implícitamente recomienda todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos.",
        "Formalmente, denotamos di como el número de documentos que nos gustaría recuperar de la base de datos i y ,.....},{ 21 ddd = como una acción de selección para todas las bases de datos.",
        "La decisión de selección de la base de datos se toma en base a las listas completas de probabilidades de relevancia para todas las bases de datos.",
        "Las listas completas de probabilidades de relevancia se infieren a partir de toda la información disponible, específicamente sR, que representa las descripciones de recursos adquiridas mediante muestreo basado en consultas y las estimaciones del tamaño de la base de datos adquiridas mediante muestreo-resampleo; cS representa las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada.",
        "Si el método de estimación de puntajes de documentos centralizados y probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈.",
        "El vector aleatorio   denota un conjunto arbitrario de listas completas de probabilidades de relevancia y ),|( cs SRP θ como la probabilidad de generar este conjunto de listas.",
        "Finalmente, a cada acción de selección d y un conjunto de listas completas de la Figura 1.",
        "Construcción de la lista completa de puntuación de documentos centralizada mediante interpolación lineal (el factor de escala de la base de datos es 50). Para 35 probabilidades de relevancia θ, asociamos una función de utilidad ),( dU θ que indica el beneficio de realizar la selección d cuando las verdaderas listas completas de probabilidades de relevancia son θ.",
        "Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7). Un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en los valores de parámetros más probables en lugar de calcular toda la expectativa.",
        "En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recomendación de bases de datos.",
        "El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de N bases de datos de Nsdb) que contengan tantos documentos relevantes como sea posible, lo cual puede definirse formalmente como: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) es la función indicadora, que es 1 cuando se selecciona la i-ésima base de datos y 0 en caso contrario.",
        "Inserta esta ecuación en el modelo básico de la Ecuación 8 y asocia la restricción del número de base de datos seleccionado para obtener lo siguiente: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) La solución de este problema de optimización es muy simple.",
        "Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) Las bases de datos Nsdb con el mayor número esperado de documentos relevantes pueden ser seleccionadas para cumplir con el objetivo de alto recall.",
        "Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recuperación distribuida de documentos.",
        "Se mide mediante la Precisión en la parte superior de la lista final de documentos fusionados.",
        "Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la Precisión de los documentos recuperados de las bases de datos seleccionadas. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Tenga en cuenta que la diferencia clave entre la Ecuación 12 y la Ecuación 9 es que la Ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos, mientras que la Ecuación 12 solo considera una parte mucho más pequeña de la clasificación.",
        "Específicamente, podemos calcular la decisión de selección óptima mediante: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Diferentes tipos de restricciones causadas por las diferentes características de las tareas de recuperación de documentos pueden estar asociadas con el problema de optimización anterior.",
        "La más común es seleccionar un número fijo (Nsdb) de bases de datos y recuperar un número fijo (Nrdoc) de documentos de cada base de datos seleccionada, definido formalmente como: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) Este problema de optimización puede resolverse fácilmente calculando el número de documentos relevantes esperados en la parte superior de la lista completa de probabilidades de relevancia de cada base de datos: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Luego, las bases de datos pueden ser clasificadas por estos valores y seleccionadas.",
        "Llamamos a este algoritmo UUM/HP-FL (Maximización Unificada de Utilidad para Alta Precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada).",
        "Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada.",
        "Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes cantidades de documentos.",
        "Para simplificar, se requiere que las longitudes de la lista de resultados sean múltiplos de un número base 10. (Este valor también puede variar, pero para simplificar se establece en 10 en este documento).",
        "Esta restricción está establecida para simular el comportamiento de los motores de búsqueda comerciales en la web. (Motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 identificadores de documentos por página de resultados).",
        "Este procedimiento ahorra tiempo de cálculo al calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea de 10 en lugar de 1 (más detalles se discuten posteriormente).",
        "Para una mayor simplificación, restringimos la selección a un máximo de 100 documentos de cada base de datos (di<=100). Luego, el problema de optimización de la selección se formaliza de la siguiente manera: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc es el número total de documentos a recuperar.",
        "Desafortunadamente, no hay una solución simple para este problema de optimización como la hay para las Ecuaciones 10 y 14.",
        "Sin embargo, se puede aplicar un algoritmo de programación dinámica de 36 para calcular la solución óptima.",
        "Los pasos básicos de este método de programación dinámica se describen en la Figura 2.",
        "Dado que este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se le llama algoritmo UUM/HP-VL.",
        "Después de que se toman las decisiones de selección, se buscan las bases de datos seleccionadas y se recuperan los identificadores de documentos correspondientes de cada base de datos.",
        "El paso final de la recuperación de documentos es fusionar los resultados devueltos en una única lista clasificada con el algoritmo de aprendizaje semisupervisado.",
        "Se señaló anteriormente que el algoritmo SSL mapea las puntuaciones específicas de la base de datos en las puntuaciones de documentos centralizadas y construye la lista clasificada final en consecuencia, lo cual es consistente con todos nuestros procedimientos de selección donde se seleccionan los documentos con mayores probabilidades de relevancia (y por ende, puntuaciones de documentos centralizadas más altas). 4.",
        "METODOLOGÍA EXPERIMENTAL 4.1 Bancos de pruebas Es deseable evaluar algoritmos de recuperación de información distribuida con bancos de pruebas que simulen de cerca las aplicaciones del mundo real.",
        "Las colecciones web TREC WT2g o WT10g proporcionan una forma de dividir los documentos por diferentes servidores web.",
        "De esta manera, se podrían crear un gran número (O(1000)) de bases de datos con contenidos bastante diversos, lo que podría convertir a este banco de pruebas en un buen candidato para simular entornos operativos como la web oculta de dominio abierto.",
        "Sin embargo, dos debilidades de este banco de pruebas son: i) Cada base de datos contiene solo una pequeña cantidad de documentos (259 documentos en promedio para WT2g) [4]; y ii) El contenido de WT2g o WT10g se extrae arbitrariamente de la web.",
        "No es probable que una base de datos web oculta proporcione páginas personales o páginas web que indiquen que las páginas están en construcción y no contengan información útil en absoluto.",
        "Estos tipos de páginas web están contenidos en los conjuntos de datos WT2g/WT10g.",
        "Por lo tanto, los datos ruidosos de la Web no son similares a los contenidos de alta calidad de las bases de datos ocultas de la Web, que generalmente están organizados por expertos en el dominio.",
        "Otra opción es los datos de noticias/gobierno de TREC [1,15,17,18,21].",
        "Los datos gubernamentales/noticias de TREC se centran en temas relativamente específicos.",
        "Comparado con los datos web de TREC: i) Los documentos de noticias/gobierno son mucho más similares a los contenidos proporcionados por una base de datos orientada a temas que a una página web arbitraria, ii) Una base de datos en este banco de pruebas es más grande que la de los datos web de TREC.",
        "En promedio, una base de datos contiene miles de documentos, lo cual es más realista que una base de datos de datos web de TREC con alrededor de 250 documentos.",
        "Dado que los contenidos y tamaños de las bases de datos en el banco de pruebas de noticias/gobierno de TREC son más similares a los de una base de datos orientada a temas, es un buen candidato para simular los entornos de recuperación de información distribuida de grandes organizaciones (empresas) o sitios web ocultos específicos de dominio, como West, que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3].",
        "Dado que la mayoría de los sistemas actuales de recuperación de información distribuida están desarrollados para entornos de grandes organizaciones (empresas) o para la Web oculta de dominios específicos en lugar de la Web oculta de dominio abierto, en este trabajo se eligió el banco de pruebas de noticias/gobierno de TREC.",
        "El banco de pruebas Trec123-100col-bysource es uno de los más utilizados en las pruebas de noticias y gobierno de TREC [1,15,17,21].",
        "Fue elegido en este trabajo.",
        "Tres bancos de pruebas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes también se utilizaron para proporcionar una simulación más exhaustiva para entornos reales.",
        "Se crearon 100 bases de datos a partir de los CDs de TREC 1, 2 y 3.",
        "Fueron organizados por fuente y fecha de publicación [1].",
        "Los tamaños de las bases de datos no están sesgados.",
        "Los detalles se encuentran en la Tabla 1.",
        "Tres bancos de pruebas construidos en [21] se basaron en el banco de pruebas trec123-100colbysource.",
        "Cada banco de pruebas contiene muchas bases de datos pequeñas y dos bases de datos grandes creadas al fusionar alrededor de 10 a 20 bases de datos pequeñas.",
        "Listas completas de probabilidades de relevancia para todas las bases de datos |DB|.",
        "Solución de selección óptima para la Ecuación 16. i) Crear el arreglo tridimensional: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Cada Sel (x, y, z) está asociado con una decisión de selección xyzd, que representa la mejor decisión de selección en la condición: solo se consideran bases de datos del número 1 al número x para la selección; se recuperarán un total de y*10 documentos; solo se seleccionan z bases de datos de los candidatos de la base de datos x.",
        "Y Sel (x, y, z) es el valor de utilidad correspondiente al elegir la mejor selección. ii) Inicializar Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) solo con la información de relevancia estimada de la 1ª base de datos. iii) Iterar el candidato actual de la base de datos i desde 2 hasta |DB| Para cada entrada Sel (i, y, z): Encontrar k tal que: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k∗ documentos de la base de datos i-ésima, de lo contrario no debemos seleccionar esta base de datos y se debe mantener la solución anterior mejor Sel (i-1, y, z).",
        "Luego establezca el valor de iyzd y Sel (i, y, z) en consecuencia. iv) La mejor solución de selección se da por _ /10| | Toral rdoc sdbDB N Nd y el valor de utilidad correspondiente es Sel (|DB|, NTotal_rdoc/10, Nsdb).",
        "Figura 2.",
        "El procedimiento de optimización de programación dinámica para la Ecuación 16.",
        "Tabla 1: Estadísticas del banco de pruebas.",
        "Número de documentos Tamaño (MB) Tamaño del banco de pruebas (GB) Mínimo Promedio Máximo Mínimo Promedio Máximo Trec123 3.2 752 10782 39713 28 32 42 Tabla 2: Estadísticas del conjunto de consultas.",
        "Nombre del conjunto de temas TREC Campo del tema TREC Longitud promedio (palabras) Trec123 51-150 Título 3.1 37 Trec123-2ldb-60col (representativo): Las bases de datos en el trec123-100col-bysource se ordenaron en orden alfabético.",
        "Dos grandes bases de datos fueron creadas al fusionar 20 bases de datos pequeñas con el método de round-robin.",
        "Por lo tanto, las dos bases de datos grandes tienen más documentos relevantes debido a sus tamaños grandes, aunque las densidades de documentos relevantes son aproximadamente iguales a las de las bases de datos pequeñas.",
        "Las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, APall y WSJall.",
        "Las otras 60 colecciones quedaron sin cambios.",
        "Las bases de datos APall y WSJall tienen una mayor densidad de documentos relevantes para las consultas de TREC que las bases de datos pequeñas.",
        "Por lo tanto, las dos bases de datos grandes tienen muchos más documentos relevantes que las bases de datos pequeñas.",
        "Las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, FRall y DOEall.",
        "Las otras 80 colecciones quedaron sin cambios.",
        "Las bases de datos FRall y DOEall tienen densidades más bajas de documentos relevantes para las consultas de TREC que las bases de datos pequeñas, a pesar de ser mucho más grandes. Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150.",
        "Las consultas 101-150 se utilizaron como consultas de entrenamiento y las consultas 51-100 se utilizaron como consultas de prueba (detalles en la Tabla 2). 4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o en la Web oculta específica de dominio, diferentes bases de datos pueden utilizar diferentes tipos de motores de búsqueda.",
        "Para simular el entorno de múltiples motores de búsqueda, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: INQUERY [2], un modelo de lenguaje estadístico de unigrama con suavizado lineal [12,20] y un algoritmo de recuperación TFIDF con peso ltc [12,20].",
        "Todos estos algoritmos fueron implementados con la herramienta Lemur [12].",
        "Estos tres tipos de motores de búsqueda fueron asignados a las bases de datos entre los cuatro bancos de pruebas de manera round-robin. 5.",
        "RESULTADOS: SELECCIÓN DE RECURSOS DE LA RECOMENDACIÓN DE BASES DE DATOS Todos los cuatro bancos de pruebas descritos en la Sección 4 fueron utilizados en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de bases de datos.",
        "Las descripciones de los recursos fueron creadas utilizando muestreo basado en consultas.",
        "Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos.",
        "Las estadísticas del tamaño de la base de datos fueron estimadas mediante el método de muestra y remuestra [21].",
        "Cincuenta consultas (101-150) se utilizaron como consultas de entrenamiento para construir el modelo logístico relevante y ajustar las funciones exponenciales de las curvas de puntuación de documentos centralizados para bases de datos de gran proporción (detalles en la Sección 3.1).",
        "Otros 50 consultas (51-100) se utilizaron como datos de prueba.",
        "Los algoritmos de selección de recursos de los sistemas de recomendación de bases de datos suelen compararse utilizando la métrica de recuperación nR [1,17,18,21].",
        "Que B denote una clasificación base, que a menudo es la RBR (clasificación basada en relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos.",
        "Y que Bi y Ei denoten el número de documentos relevantes en la base de datos clasificada i-ésima de B o E. Entonces, Rn se define de la siguiente manera: = = = k i i k i i k B E R 1 1 (17) Por lo general, el objetivo es buscar solo algunas bases de datos, por lo que nuestras cifras solo muestran resultados para la selección de hasta 20 bases de datos.",
        "Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, CORI, ReDDE y UUM/HR.",
        "El algoritmo UUM/HR se describe en la Sección 3.3.",
        "Se puede observar en la Figura 3 que los algoritmos ReDDE y UUM/HR son más efectivos (en los conjuntos de pruebas representativos, relevantes y no relevantes) o igual de efectivos (en el conjunto de pruebas Trec123-100Col) que el algoritmo de selección de recursos CORI.",
        "El algoritmo UUM/HR es más efectivo que el algoritmo ReDDE en los conjuntos de pruebas representativos y relevantes y es aproximadamente igual que el algoritmo ReDDE en los conjuntos de pruebas Trec123100Col y no relevantes.",
        "Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo ReDDE.",
        "Se puede observar que al seleccionar solo algunas bases de datos en el Trec123-100Col o en los conjuntos de pruebas no relevantes, el algoritmo ReDEE tiene una pequeña ventaja sobre el algoritmo UUM/HR.",
        "Atribuimos esto a dos causas: i) El algoritmo ReDDE fue ajustado en el banco de pruebas Trec123-100Col; y ii) Aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico para estimar probabilidades de relevancia no es lo suficientemente preciso.",
        "Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este pequeño rompecabezas.",
        "Colecciones seleccionadas.",
        "Colecciones seleccionadas.",
        "Plataforma de pruebas Trec123-100Col.",
        "Plataforma de pruebas representativa.",
        "Colección seleccionada.",
        "Colección seleccionada.",
        "Plataforma de pruebas relevante.",
        "Plataforma de pruebas no relevante.",
        "Figura 3.",
        "Experimentos de selección de recursos en los cuatro bancos de pruebas. 38 6.",
        "RESULTADOS: EFECTIVIDAD DE LA RECUPERACIÓN DE DOCUMENTOS Para la recuperación de documentos, se buscan en las bases de datos seleccionadas y los resultados devueltos se fusionan en una lista final única.",
        "En todos los experimentos discutidos en esta sección, los resultados obtenidos de bases de datos individuales fueron combinados por el algoritmo de fusión de resultados de aprendizaje semisupervisado.",
        "Esta versión del algoritmo SSL [22] tiene permitido descargar un pequeño número de textos de documentos devueltos sobre la marcha para crear datos de entrenamiento adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicos de la base de datos en puntuaciones de documentos centralizadas estimadas.",
        "Se ha demostrado ser muy efectivo en entornos donde solo se obtienen listas de resultados cortas de cada base de datos seleccionada [22].",
        "Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos.",
        "La efectividad de la recuperación de documentos se midió mediante la Precisión en la parte superior de la lista final de documentos.",
        "Los experimentos en esta sección se llevaron a cabo para estudiar la efectividad de recuperación de documentos de cinco algoritmos de selección, a saber, los algoritmos CORI, ReDDE, UUM/HR, UUM/HP-FL y UUM/HP-VL.",
        "Los últimos tres algoritmos fueron propuestos en la Sección 3.",
        "Todos los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada.",
        "El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos a recuperar de cada base de datos seleccionada; el número recuperado estaba limitado a ser de 10 a 100, y un múltiplo de 10.",
        "El Trec123-100Col y los bancos de pruebas representativos fueron seleccionados para la recuperación de documentos, ya que representan dos casos extremos de efectividad en la selección de recursos; en un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante Tabla 5.",
        "Precisión en el banco de pruebas representativo cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR).",
        "Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 documentos 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 documentos 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 documentos 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 documentos 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Tabla 6.",
        "Precisión en el banco de pruebas representativo cuando se seleccionaron 5 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR).",
        "Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 documentos 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 documentos 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 documentos 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 documentos 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Tabla 3.",
        "Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR).",
        "Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 documentos 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 documentos 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 documentos 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 documentos 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Tabla 4.",
        "Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 5 bases de datos. (El primer punto de referencia es CORI; el segundo punto de referencia para los métodos UUM/HP es UUM/HR).",
        "La precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 documentos 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 documentos 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 documentos 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 documentos 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 mucho peor que los otros algoritmos.",
        "Las Tablas 3 y 4 muestran los resultados en el banco de pruebas Trec123-100Col, y las Tablas 5 y 6 muestran los resultados en el banco de pruebas representativo.",
        "En el banco de pruebas Trec123-100Col, la efectividad de recuperación de documentos del algoritmo de selección CORI es aproximadamente la misma o un poco mejor que el algoritmo ReDDE, pero ambos son peores que los otros tres algoritmos (Tablas 3 y 4).",
        "El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos CORI y ReDDE.",
        "Una de las principales diferencias entre el algoritmo UUM/HR y el algoritmo ReDDE fue señalada anteriormente: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntuación de documentos centralizadas, mientras que el algoritmo ReDDE [21] utiliza un método heurístico, asume que las curvas de puntuación de documentos centralizadas son funciones escalonadas y no hace distinción entre la parte superior de las curvas.",
        "Esta diferencia hace que UUM/HR sea mejor que el algoritmo ReDDE para distinguir documentos con altas probabilidades de relevancia de aquéllos con bajas probabilidades de relevancia.",
        "Por lo tanto, el UUM/HR refleja mejor el objetivo de recuperación de alta precisión que el algoritmo ReDDE y, por lo tanto, es más efectivo para la recuperación de documentos.",
        "El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, como lo hacen los algoritmos UUM/HP-FL y UUM/HP-VL.",
        "Se puede observar que en este banco de pruebas, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos.",
        "Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de recuperación de documentos en las Ecuaciones 14 y 16.",
        "En el banco de pruebas representativo, CORI es mucho menos efectivo que otros algoritmos para la recuperación distribuida de documentos (Tablas 5 y 6).",
        "Los resultados de recuperación de documentos del algoritmo ReDDE son mejores que los del algoritmo CORI pero aún peores que los resultados del algoritmo UUM/HR.",
        "En este banco de pruebas, los tres algoritmos de UUM son aproximadamente igual de efectivos.",
        "Un análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho mayor que los experimentos en el banco de pruebas Trec123-100Col, ya que todos tienden a seleccionar las dos bases de datos grandes.",
        "Esto explica por qué son igualmente efectivos para la recuperación de documentos.",
        "En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos y reportar solo listas clasificadas de resultados.",
        "Dado que el modelo unificado de maximización de utilidad solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos.",
        "El único ajuste es que el algoritmo SSL fusiona listas clasificadas sin puntuaciones de documentos asignando a los documentos puntuaciones de pseudo-documentos normalizadas por sus rangos (En una lista clasificada de 50 documentos, el primero tiene una puntuación de 1, el segundo tiene una puntuación de 0.98, etc.), lo cual ha sido estudiado en [22].",
        "Los resultados del experimento en el banco de pruebas trec123-100Col-bysource con 3 bases de datos seleccionadas se muestran en la Tabla 7.",
        "La configuración del experimento fue la misma que antes, excepto que las puntuaciones de los documentos fueron eliminadas intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de identificadores de documentos.",
        "Se puede observar en los resultados que el UUM/HP-FL y el UUM/HP-VL funcionan bien con bases de datos que no devuelven puntuaciones de documentos y siguen siendo más efectivos que otras alternativas.",
        "Otros experimentos con bases de datos que no devuelven puntuaciones de documentos no se informan, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL.",
        "Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión de manera explícita en la recuperación de documentos.",
        "Los nuevos algoritmos basados en este principio logran resultados mejores o al menos tan buenos como los algoritmos previos de vanguardia en varios entornos.",
        "CONCLUSIÓN La recuperación distribuida de información resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e Internet.",
        "La mayoría de investigaciones previas utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de bases de datos para la aplicación de recuperación de documentos distribuidos.",
        "Sostenemos que el objetivo de alta recuperación de recursos en la recomendación de bases de datos y el objetivo de alta precisión en la recuperación de documentos están relacionados pero no son idénticos.",
        "Este tipo de inconsistencia también ha sido observada en trabajos anteriores, pero las soluciones previas utilizaron métodos heurísticos o asumieron la cooperación de bases de datos individuales (por ejemplo, que todas las bases de datos utilizaran el mismo tipo de motores de búsqueda), lo cual frecuentemente no es cierto en un entorno no cooperativo.",
        "En este trabajo proponemos un modelo unificado de maximización de utilidad para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado.",
        "En este marco, las decisiones de selección se obtienen optimizando diferentes funciones objetivo.",
        "Hasta donde sabemos, este es el primer trabajo que intenta visualizar y modelar teóricamente la tarea de recuperación de información distribuida de manera integrada.",
        "El nuevo marco continúa una tendencia reciente de investigación que estudia el uso de muestreo basado en consultas y una base de datos de muestras centralizada.",
        "Se entrenó un único modelo logístico en la Tabla 7 centralizada.",
        "Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). (Los motores de búsqueda no devuelven puntajes de documentos) Precisión en la Clasificación de Documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 documentos 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 documentos 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 documentos 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 documentos 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 base de datos de muestra para estimar las probabilidades de relevancia de documentos por sus puntajes de recuperación centralizados, mientras que la base de datos de muestra centralizada sirve como puente para conectar las bases de datos individuales con el modelo logístico centralizado.",
        "Por lo tanto, las probabilidades de relevancia para todos los documentos en las bases de datos pueden ser estimadas con una cantidad muy pequeña de juicio de relevancia humano, lo cual es mucho más eficiente que los métodos anteriores que construyen un modelo separado para cada base de datos.",
        "Este marco no solo es más sólido teóricamente, sino también muy efectivo.",
        "Un algoritmo para la selección de recursos (UUM/HR) y dos algoritmos para la recuperación de documentos (UUM/HP-FL y UUM/HP-VL) se derivan de este marco.",
        "Se han realizado estudios empíricos en bancos de pruebas para simular las soluciones de búsqueda distribuida de grandes organizaciones (empresas) o la Web oculta específica de un dominio.",
        "Además, los algoritmos de selección de recursos UUM/HP-FL y UUM/HP-VL se amplían con una variante del algoritmo de fusión de resultados SSL para abordar la tarea de recuperación de documentos distribuidos cuando las bases de datos seleccionadas no devuelven puntuaciones de documentos.",
        "Los experimentos han demostrado que estos algoritmos logran resultados que son al menos tan buenos como el estado del arte previo, y a veces considerablemente mejores.",
        "Un análisis detallado indica que la ventaja de estos algoritmos proviene de optimizar explícitamente los objetivos de las tareas específicas.",
        "El marco unificado de maximización de utilidad está abierto a diferentes extensiones.",
        "Cuando el costo está asociado con la búsqueda en las bases de datos en línea, el marco de utilidad puede ajustarse para estimar automáticamente el mejor número de bases de datos a buscar, de modo que se puedan recuperar una gran cantidad de documentos relevantes con costos relativamente bajos.",
        "Otra extensión del marco es considerar la efectividad de la recuperación de información de las bases de datos en línea, lo cual es un tema importante en los entornos operativos.",
        "Todas estas son las direcciones de la investigación futura.",
        "AGRADECIMIENTO Esta investigación fue apoyada por las subvenciones de la NSF EIA-9983253 y IIS-0118767.",
        "Cualquier opinión, hallazgo, conclusión o recomendación expresada en este documento son del autor y no necesariamente reflejan las del patrocinador.",
        "REFERENCIAS [1] J. Callan. (2000).",
        "Recuperación de información distribuida.",
        "En W.B.",
        "Croft, editor, Avances en Recuperación de Información.",
        "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. \n\nEditorial Kluwer Academic. (pp. 127-150). [2] J. Callan, W.B.",
        "Croft, y J. Broglio. (1995).",
        "Experimentos TREC y TIPSTER con INQUERY.",
        "Procesamiento y Gestión de la Información, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson y M. Meziou. (2002).",
        "Selección de base de datos utilizando recursos de colección lógica adquiridos y físicos reales en un entorno operativo masivo específico de dominio.",
        "Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos.",
        "En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [4] N. Craswell. (2000).",
        "Métodos para la recuperación distribuida de información.",
        "I'm sorry, but the sentence \"Ph.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate to Spanish?",
        "Tesis doctoral, Universidad Nacional Australiana. [5] N. Craswell, D. Hawking y P. Thistlewaite. (1999).",
        "Combinando resultados de motores de búsqueda aislados.",
        "En Actas de la 10ª Conferencia de Bases de Datos Australasiana. [6] D. DSouza, J. Thom y J. Zobel. (2000).",
        "Una comparación de técnicas para seleccionar colecciones de texto.",
        "En Actas de la 11ª Conferencia de Bases de Datos Australasiana. [7] N. Fuhr. (1999).",
        "Un enfoque de Teoría de la Decisión para la selección de bases de datos en IR en red.",
        "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina y A. Paepcke. (1997).",
        "Propuesta de Stanford para la metabusqueda en internet.",
        "En Actas de la 20ª Conferencia Internacional ACM-SIGMOD sobre Gestión de Datos. [9] L. Gravano, P. Ipeirotis y M. Sahami. (2003).",
        "QProber: Un sistema para la clasificación automática de bases de datos de la web oculta.",
        "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis y L. Gravano. (2002).",
        "Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos.",
        "En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] El kit de herramientas lemur. http://www.cs.cmu.edu/~lemur [13] J. Lu y J. Callan. (2003).",
        "Recuperación de información basada en contenido en redes peer-to-peer.",
        "En Actas de la 12ª Conferencia Internacional sobre Información y Gestión del Conocimiento. [14] W. Meng, C.T.",
        "Yu y K.L.",
        "Liu. (2002) Construcción de motores de búsqueda eficientes y efectivos.",
        "ACM Comput.",
        "Surv. 34(1). [15] H. Nottelmann y N. Fuhr. (2003).",
        "Evaluando diferentes métodos para estimar la calidad de recuperación para la selección de recursos.",
        "En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [16] H., Nottelmann y N., Fuhr. (2003).",
        "La arquitectura MIND para bibliotecas digitales federadas de multimedia heterogénea.",
        "Taller ACM SIGIR 2003 sobre Recuperación de Información Distribuida. [17] A.L.",
        "Powell, J.C. French, J. Callan, M. Connell y C.L.",
        "Viles. (2000). \n\nViles. (2000).",
        "El impacto de la selección de bases de datos en la búsqueda distribuida.",
        "En Actas de la 23ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [18] A.L.",
        "Powell y J.C. French. (2003).",
        "Comparando el rendimiento de los algoritmos de selección de bases de datos.",
        "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). \n\nACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
        "Busca en la web invisible.",
        "Guardian Unlimited. [20] L. Si y J. Callan. (2002).",
        "Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda.",
        "En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [21] L. Si y J. Callan. (2003).",
        "Método de estimación de distribución de documentos relevantes para la selección de recursos.",
        "En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [22] L. Si y J. Callan. (2003).",
        "Un método de aprendizaje semi-supervisado para fusionar los resultados de un motor de búsqueda.",
        "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41\n\nACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
    ],
    "error_count": 8,
    "keys": {
        "resource selection of distributed text information retrieval": {
            "translated_key": "selección de recursos de recuperación de información textual distribuida",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for <br>resource selection of distributed text information retrieval</br>.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for <br>resource selection of distributed text information retrieval</br>."
            ],
            "translated_annotated_samples": [
                "Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la <br>selección de recursos de recuperación de información textual distribuida</br>."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la <br>selección de recursos de recuperación de información textual distribuida</br>. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La recuperación distribuida de documentos es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de recuperación de documentos distribuidos, tratándolos como objetivos de optimización diferentes. Primero, se construye una base de datos de muestra centralizada mediante el muestreo aleatorio de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas; también se estiman las estadísticas del tamaño de la base de datos. Un modelo de transformación logística se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia. Segundo, después de que se envía una nueva consulta, la consulta se puede utilizar para buscar en la base de datos de muestras centralizada que produce una puntuación para cada documento muestreado. La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada puede estimarse aplicando el modelo logístico al puntaje de cada documento. Entonces, las probabilidades de relevancia de todos los documentos (en su mayoría no vistos) entre las bases de datos disponibles pueden ser estimadas utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos. Para la tarea de selección de recursos para un sistema de recomendación de bases de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alto recall. Para la selección de recursos para un sistema distribuido de recuperación de documentos, se prefieren las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia. Este criterio de selección cumple con el objetivo de alta precisión de la aplicación de recuperación de documentos distribuidos. Además, se aplica el algoritmo de aprendizaje semisupervisado (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada. El marco de utilidad unificado hace muy pocas suposiciones y funciona en entornos no cooperativos. Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos correspondientes; y ii) Muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos. Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades correspondientes de relevancia y utiliza la base de datos de muestra centralizada como puente entre las bases de datos individuales y el modelo logístico. El esfuerzo humano (juicio de relevancia) necesario para entrenar el modelo logístico centralizado único no aumenta con el número de bases de datos. Esta es una gran ventaja sobre investigaciones anteriores, las cuales requerían que la cantidad de esfuerzo humano fuera lineal con el número de bases de datos [7,15]. El marco de utilidad unificada no solo es más sólido teóricamente, sino también muy efectivo. Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de vanguardia en una variedad de configuraciones. La siguiente sección discute el trabajo relacionado. La sección 3 describe el nuevo modelo unificado de maximización de utilidad. La sección 4 explica nuestra metodología experimental. Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos. La sección 7 concluye. 2. Investigación previa Ha habido una considerable investigación sobre todos los subproblemas de la recuperación de información distribuida. Exploramos los trabajos más relacionados en esta sección. El primer problema de la recuperación de información distribuida es la representación de recursos. El protocolo STARTS es una solución para adquirir descripciones de recursos en entornos cooperativos [8]. Sin embargo, en entornos no cooperativos, aunque las bases de datos estén dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no. Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí. Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en consultas, que genera y envía consultas de forma aleatoria a motores de búsqueda individuales y recupera algunos documentos para construir las descripciones. Dado que los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico. Los experimentos han demostrado que descripciones de recursos bastante precisas pueden ser construidas enviando alrededor de 80 consultas y descargando alrededor de 300 documentos [1]. Muchos algoritmos de selección de recursos como gGlOSS/vGlOSS [8] y CORI [1] han sido propuestos en la última década. El algoritmo CORI representa cada base de datos por sus términos, las frecuencias de los documentos y un pequeño número de estadísticas del corpus (detalles en [1]). Como investigaciones previas en diferentes conjuntos de datos han demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo utilizamos como algoritmo base en este trabajo. El algoritmo de selección de recursos de estimación de distribución de documentos relevantes (ReDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia. Aunque se ha demostrado que el algoritmo ReDDE es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21]. El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar puntuaciones de documentos específicas de la base de datos en puntuaciones de documentos independientes de la base de datos comparables. El algoritmo de fusión de resultados de aprendizaje semisupervisado (SSL) [20,22] utiliza los documentos adquiridos mediante muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos y de la consulta. Estos modelos lineales se utilizan para convertir las puntuaciones de documentos específicas de la base de datos en las puntuaciones de documentos centralizadas aproximadas. El algoritmo SSL ha demostrado ser efectivo [22]. Sirve como un componente importante de nuestro marco unificado de maximización de utilidad (Sección 3). Para lograr resultados precisos en la recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos en sistemas de recomendación de bases de datos. Pero como se señaló anteriormente, un algoritmo de selección de recursos optimizado para un alto recuerdo puede no funcionar bien para la recuperación de documentos, que tiene como objetivo la alta precisión. Este tipo de inconsistencia ha sido observada en investigaciones previas [4,21]. La investigación en [21] intentó resolver el problema con un método heurístico. La investigación más similar a lo que proponemos aquí es el marco teórico de la toma de decisiones (DTF) [7,15]. Este marco de trabajo calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y se han propuesto varios métodos [15] para estimar la calidad de recuperación. Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF. Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de bases de datos y recuperación distribuida de documentos) en el mismo marco unificado. Segundo, el DTF construye un modelo para cada base de datos para calcular las probabilidades de relevancia. Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos. Por el contrario, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada. La base de datos de muestra centralizada puede servir como puente para conectar las bases de datos individuales con el modelo logístico centralizado, de esta manera se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos. Esta estrategia puede ahorrar una gran cantidad de esfuerzo en juicio humano y es una gran ventaja del marco de maximización de utilidad unificada sobre el DTF, especialmente cuando hay un gran número de bases de datos. MARCO DE MAXIMIZACIÓN DE UTILIDAD UNIFICADA El marco de Maximización de Utilidad Unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría no vistos) disponibles en el entorno de búsqueda distribuida. En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el modelo de Maximización de Utilidad Unificado. También describimos cómo el modelo puede ser optimizado para el objetivo de alto recuerdo de un sistema de recomendación de base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido. 3.1 Estimación de Probabilidades de Relevancia Como se señaló anteriormente, el propósito de la selección de recursos es el alto recuerdo y el propósito de la recuperación de documentos es la alta precisión. Para cumplir con estos objetivos diversos, el problema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos. Este es un problema difícil porque solo podemos observar una muestra de los contenidos de cada base de datos utilizando muestreo basado en consultas. Nuestra estrategia es aprovechar al máximo toda la información disponible para calcular las estimaciones de probabilidad. 3.1.1 Aprendizaje de Probabilidades de Relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se construye mediante muestreo basado en consultas y los tamaños de la base de datos se estiman utilizando el método de muestreo y remuestreo [21]. Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (Inquery [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de entrenamiento. Para cada consulta de entrenamiento, se aplica el algoritmo de selección de recursos CORI [1] para seleccionar un cierto número (por ejemplo, 10) de bases de datos y recuperar 50 identificadores de documentos de cada base de datos. El algoritmo de fusión de resultados SSL [20,22] se utiliza para combinar los resultados. Luego, podemos descargar los 50 documentos principales de la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando Inquery y las estadísticas del corpus de la base de datos de muestra centralizada. Las puntuaciones centralizadas se normalizan aún más (dividiéndolas por la puntuación centralizada máxima para cada consulta), ya que este método ha sido sugerido para mejorar la precisión de la estimación en investigaciones anteriores [15]. El juicio humano se adquiere para esos documentos y se construye un modelo logístico para transformar las puntuaciones de documentos centralizados normalizados en probabilidades de relevancia de la siguiente manera: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) donde )( _ dSc es la puntuación de documento centralizada normalizada y ac y bc son los dos parámetros del modelo logístico. Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de entrenamiento. El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de las puntuaciones de documentos centralizadas. 3.1.2 Estimación de las puntuaciones de documentos centralizadas Cuando el usuario envía una nueva consulta, se calculan las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar las puntuaciones de los documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados. Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas del tamaño de la base de datos. Definimos el factor de escala de la base de datos para la base de datos i como la razón entre el tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: SF_i = ^N_db / _N_db_samp_i donde ^N_db es el tamaño estimado de la base de datos y _N_db_samp_i es el número de documentos de la base de datos i en la base de datos de muestra centralizada. La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podríamos suponer que hay alrededor de 50 documentos en esa base de datos que tienen puntuaciones de alrededor de 0.5. De hecho, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntuación del documento centralizado para cada base de datos. Formalmente, clasificamos todos los documentos muestreados de la base de datos i-ésima por sus puntajes de documento centralizado 34 para obtener la lista de puntajes de documento centralizado muestreado {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} para la base de datos i; asumimos que si pudiéramos calcular los puntajes de documento centralizado para todos los documentos en esta base de datos y obtener la lista completa de puntajes de documento centralizado, el documento superior en la lista muestreada tendría un rango de SFdbi/2, el segundo documento en la lista muestreada tendría un rango de SFdbi3/2, y así sucesivamente. Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. La interpolación lineal por tramos se aplica para estimar la curva de puntuación del documento centralizado, como se ilustra en la Figura 1. La lista completa de puntuaciones de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documentos centralizados como: ],1[,)(S ^^ c idbij Njd ∈ . Se puede observar en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntuación del documento centralizado. Sin embargo, para bases de datos con grandes proporciones de escala de base de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos mejor clasificados (por ejemplo, [1, SFdbi/2]). Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizados de los documentos mejor clasificados para bases de datos con ratios a gran escala (por ejemplo, mayores de 100). Específicamente, se construye un modelo logístico para cada una de estas bases de datos. El modelo logístico se utiliza para estimar la puntuación del documento centralizado superior 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con las puntuaciones centralizadas más altas. 0iα , 1iα y 2iα son los parámetros del modelo logístico. Para cada consulta de entrenamiento, se descarga el documento mejor recuperado de cada base de datos y se calcula la puntuación del documento centralizado correspondiente. Junto con las puntuaciones de los dos documentos muestreados principales, estos parámetros pueden ser estimados. Después de estimar la puntuación centralizada del documento principal, se ajusta una función exponencial para la parte superior ([1, SFdbi/2]) de la curva de puntuación del documento centralizado como: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) Los dos parámetros 0iβ y 1iβ se ajustan para asegurarse de que la función exponencial pase por los dos puntos (1, ^ 1)( ic dS ) y (SFdbi/2, Sc(dsi1)). La función exponencial se utiliza únicamente para ajustar la parte superior de la curva de puntuación del documento centralizado, mientras que la parte inferior de la curva sigue siendo ajustada con el método de interpolación lineal descrito anteriormente. El ajuste mediante la función exponencial de los documentos mejor clasificados ha demostrado empíricamente producir resultados más precisos. A partir de las curvas de puntuación de documentos centralizadas, podemos estimar las listas completas de puntuación de documentos centralizados correspondientes para todas las bases de datos disponibles. Después de que las puntuaciones estimadas de los documentos centralizados se normalizan, las listas completas de probabilidades de relevancia pueden ser construidas a partir de las listas completas de puntuaciones de documentos centralizados mediante la Ecuación 1. Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El Modelo Unificado de Maximización de Utilidad En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco. En la tarea de recomendación de bases de datos, el sistema necesita decidir cómo clasificar las bases de datos. En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada. Generalizamos el proceso de selección de recomendaciones de bases de datos, que implícitamente recomienda todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos. Formalmente, denotamos di como el número de documentos que nos gustaría recuperar de la base de datos i y ,.....},{ 21 ddd = como una acción de selección para todas las bases de datos. La decisión de selección de la base de datos se toma en base a las listas completas de probabilidades de relevancia para todas las bases de datos. Las listas completas de probabilidades de relevancia se infieren a partir de toda la información disponible, específicamente sR, que representa las descripciones de recursos adquiridas mediante muestreo basado en consultas y las estimaciones del tamaño de la base de datos adquiridas mediante muestreo-resampleo; cS representa las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Si el método de estimación de puntajes de documentos centralizados y probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈. El vector aleatorio   denota un conjunto arbitrario de listas completas de probabilidades de relevancia y ),|( cs SRP θ como la probabilidad de generar este conjunto de listas. Finalmente, a cada acción de selección d y un conjunto de listas completas de la Figura 1. Construcción de la lista completa de puntuación de documentos centralizada mediante interpolación lineal (el factor de escala de la base de datos es 50). Para 35 probabilidades de relevancia θ, asociamos una función de utilidad ),( dU θ que indica el beneficio de realizar la selección d cuando las verdaderas listas completas de probabilidades de relevancia son θ. Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7). Un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en los valores de parámetros más probables en lugar de calcular toda la expectativa. En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recomendación de bases de datos. El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de N bases de datos de Nsdb) que contengan tantos documentos relevantes como sea posible, lo cual puede definirse formalmente como: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) es la función indicadora, que es 1 cuando se selecciona la i-ésima base de datos y 0 en caso contrario. Inserta esta ecuación en el modelo básico de la Ecuación 8 y asocia la restricción del número de base de datos seleccionado para obtener lo siguiente: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) La solución de este problema de optimización es muy simple. Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) Las bases de datos Nsdb con el mayor número esperado de documentos relevantes pueden ser seleccionadas para cumplir con el objetivo de alto recall. Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recuperación distribuida de documentos. Se mide mediante la Precisión en la parte superior de la lista final de documentos fusionados. Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la Precisión de los documentos recuperados de las bases de datos seleccionadas. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Tenga en cuenta que la diferencia clave entre la Ecuación 12 y la Ecuación 9 es que la Ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos, mientras que la Ecuación 12 solo considera una parte mucho más pequeña de la clasificación. Específicamente, podemos calcular la decisión de selección óptima mediante: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Diferentes tipos de restricciones causadas por las diferentes características de las tareas de recuperación de documentos pueden estar asociadas con el problema de optimización anterior. La más común es seleccionar un número fijo (Nsdb) de bases de datos y recuperar un número fijo (Nrdoc) de documentos de cada base de datos seleccionada, definido formalmente como: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) Este problema de optimización puede resolverse fácilmente calculando el número de documentos relevantes esperados en la parte superior de la lista completa de probabilidades de relevancia de cada base de datos: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Luego, las bases de datos pueden ser clasificadas por estos valores y seleccionadas. Llamamos a este algoritmo UUM/HP-FL (Maximización Unificada de Utilidad para Alta Precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada). Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada. Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes cantidades de documentos. Para simplificar, se requiere que las longitudes de la lista de resultados sean múltiplos de un número base 10. (Este valor también puede variar, pero para simplificar se establece en 10 en este documento). Esta restricción está establecida para simular el comportamiento de los motores de búsqueda comerciales en la web. (Motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 identificadores de documentos por página de resultados). Este procedimiento ahorra tiempo de cálculo al calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea de 10 en lugar de 1 (más detalles se discuten posteriormente). Para una mayor simplificación, restringimos la selección a un máximo de 100 documentos de cada base de datos (di<=100). Luego, el problema de optimización de la selección se formaliza de la siguiente manera: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc es el número total de documentos a recuperar. Desafortunadamente, no hay una solución simple para este problema de optimización como la hay para las Ecuaciones 10 y 14. Sin embargo, se puede aplicar un algoritmo de programación dinámica de 36 para calcular la solución óptima. Los pasos básicos de este método de programación dinámica se describen en la Figura 2. Dado que este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se le llama algoritmo UUM/HP-VL. Después de que se toman las decisiones de selección, se buscan las bases de datos seleccionadas y se recuperan los identificadores de documentos correspondientes de cada base de datos. El paso final de la recuperación de documentos es fusionar los resultados devueltos en una única lista clasificada con el algoritmo de aprendizaje semisupervisado. Se señaló anteriormente que el algoritmo SSL mapea las puntuaciones específicas de la base de datos en las puntuaciones de documentos centralizadas y construye la lista clasificada final en consecuencia, lo cual es consistente con todos nuestros procedimientos de selección donde se seleccionan los documentos con mayores probabilidades de relevancia (y por ende, puntuaciones de documentos centralizadas más altas). 4. METODOLOGÍA EXPERIMENTAL 4.1 Bancos de pruebas Es deseable evaluar algoritmos de recuperación de información distribuida con bancos de pruebas que simulen de cerca las aplicaciones del mundo real. Las colecciones web TREC WT2g o WT10g proporcionan una forma de dividir los documentos por diferentes servidores web. De esta manera, se podrían crear un gran número (O(1000)) de bases de datos con contenidos bastante diversos, lo que podría convertir a este banco de pruebas en un buen candidato para simular entornos operativos como la web oculta de dominio abierto. Sin embargo, dos debilidades de este banco de pruebas son: i) Cada base de datos contiene solo una pequeña cantidad de documentos (259 documentos en promedio para WT2g) [4]; y ii) El contenido de WT2g o WT10g se extrae arbitrariamente de la web. No es probable que una base de datos web oculta proporcione páginas personales o páginas web que indiquen que las páginas están en construcción y no contengan información útil en absoluto. Estos tipos de páginas web están contenidos en los conjuntos de datos WT2g/WT10g. Por lo tanto, los datos ruidosos de la Web no son similares a los contenidos de alta calidad de las bases de datos ocultas de la Web, que generalmente están organizados por expertos en el dominio. Otra opción es los datos de noticias/gobierno de TREC [1,15,17,18,21]. Los datos gubernamentales/noticias de TREC se centran en temas relativamente específicos. Comparado con los datos web de TREC: i) Los documentos de noticias/gobierno son mucho más similares a los contenidos proporcionados por una base de datos orientada a temas que a una página web arbitraria, ii) Una base de datos en este banco de pruebas es más grande que la de los datos web de TREC. En promedio, una base de datos contiene miles de documentos, lo cual es más realista que una base de datos de datos web de TREC con alrededor de 250 documentos. Dado que los contenidos y tamaños de las bases de datos en el banco de pruebas de noticias/gobierno de TREC son más similares a los de una base de datos orientada a temas, es un buen candidato para simular los entornos de recuperación de información distribuida de grandes organizaciones (empresas) o sitios web ocultos específicos de dominio, como West, que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3]. Dado que la mayoría de los sistemas actuales de recuperación de información distribuida están desarrollados para entornos de grandes organizaciones (empresas) o para la Web oculta de dominios específicos en lugar de la Web oculta de dominio abierto, en este trabajo se eligió el banco de pruebas de noticias/gobierno de TREC. El banco de pruebas Trec123-100col-bysource es uno de los más utilizados en las pruebas de noticias y gobierno de TREC [1,15,17,21]. Fue elegido en este trabajo. Tres bancos de pruebas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes también se utilizaron para proporcionar una simulación más exhaustiva para entornos reales. Se crearon 100 bases de datos a partir de los CDs de TREC 1, 2 y 3. Fueron organizados por fuente y fecha de publicación [1]. Los tamaños de las bases de datos no están sesgados. Los detalles se encuentran en la Tabla 1. Tres bancos de pruebas construidos en [21] se basaron en el banco de pruebas trec123-100colbysource. Cada banco de pruebas contiene muchas bases de datos pequeñas y dos bases de datos grandes creadas al fusionar alrededor de 10 a 20 bases de datos pequeñas. Listas completas de probabilidades de relevancia para todas las bases de datos |DB|. Solución de selección óptima para la Ecuación 16. i) Crear el arreglo tridimensional: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Cada Sel (x, y, z) está asociado con una decisión de selección xyzd, que representa la mejor decisión de selección en la condición: solo se consideran bases de datos del número 1 al número x para la selección; se recuperarán un total de y*10 documentos; solo se seleccionan z bases de datos de los candidatos de la base de datos x. Y Sel (x, y, z) es el valor de utilidad correspondiente al elegir la mejor selección. ii) Inicializar Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) solo con la información de relevancia estimada de la 1ª base de datos. iii) Iterar el candidato actual de la base de datos i desde 2 hasta |DB| Para cada entrada Sel (i, y, z): Encontrar k tal que: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k∗ documentos de la base de datos i-ésima, de lo contrario no debemos seleccionar esta base de datos y se debe mantener la solución anterior mejor Sel (i-1, y, z). Luego establezca el valor de iyzd y Sel (i, y, z) en consecuencia. iv) La mejor solución de selección se da por _ /10| | Toral rdoc sdbDB N Nd y el valor de utilidad correspondiente es Sel (|DB|, NTotal_rdoc/10, Nsdb). Figura 2. El procedimiento de optimización de programación dinámica para la Ecuación 16. Tabla 1: Estadísticas del banco de pruebas. Número de documentos Tamaño (MB) Tamaño del banco de pruebas (GB) Mínimo Promedio Máximo Mínimo Promedio Máximo Trec123 3.2 752 10782 39713 28 32 42 Tabla 2: Estadísticas del conjunto de consultas. Nombre del conjunto de temas TREC Campo del tema TREC Longitud promedio (palabras) Trec123 51-150 Título 3.1 37 Trec123-2ldb-60col (representativo): Las bases de datos en el trec123-100col-bysource se ordenaron en orden alfabético. Dos grandes bases de datos fueron creadas al fusionar 20 bases de datos pequeñas con el método de round-robin. Por lo tanto, las dos bases de datos grandes tienen más documentos relevantes debido a sus tamaños grandes, aunque las densidades de documentos relevantes son aproximadamente iguales a las de las bases de datos pequeñas. Las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, APall y WSJall. Las otras 60 colecciones quedaron sin cambios. Las bases de datos APall y WSJall tienen una mayor densidad de documentos relevantes para las consultas de TREC que las bases de datos pequeñas. Por lo tanto, las dos bases de datos grandes tienen muchos más documentos relevantes que las bases de datos pequeñas. Las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, FRall y DOEall. Las otras 80 colecciones quedaron sin cambios. Las bases de datos FRall y DOEall tienen densidades más bajas de documentos relevantes para las consultas de TREC que las bases de datos pequeñas, a pesar de ser mucho más grandes. Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150. Las consultas 101-150 se utilizaron como consultas de entrenamiento y las consultas 51-100 se utilizaron como consultas de prueba (detalles en la Tabla 2). 4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o en la Web oculta específica de dominio, diferentes bases de datos pueden utilizar diferentes tipos de motores de búsqueda. Para simular el entorno de múltiples motores de búsqueda, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: INQUERY [2], un modelo de lenguaje estadístico de unigrama con suavizado lineal [12,20] y un algoritmo de recuperación TFIDF con peso ltc [12,20]. Todos estos algoritmos fueron implementados con la herramienta Lemur [12]. Estos tres tipos de motores de búsqueda fueron asignados a las bases de datos entre los cuatro bancos de pruebas de manera round-robin. 5. RESULTADOS: SELECCIÓN DE RECURSOS DE LA RECOMENDACIÓN DE BASES DE DATOS Todos los cuatro bancos de pruebas descritos en la Sección 4 fueron utilizados en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de bases de datos. Las descripciones de los recursos fueron creadas utilizando muestreo basado en consultas. Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos. Las estadísticas del tamaño de la base de datos fueron estimadas mediante el método de muestra y remuestra [21]. Cincuenta consultas (101-150) se utilizaron como consultas de entrenamiento para construir el modelo logístico relevante y ajustar las funciones exponenciales de las curvas de puntuación de documentos centralizados para bases de datos de gran proporción (detalles en la Sección 3.1). Otros 50 consultas (51-100) se utilizaron como datos de prueba. Los algoritmos de selección de recursos de los sistemas de recomendación de bases de datos suelen compararse utilizando la métrica de recuperación nR [1,17,18,21]. Que B denote una clasificación base, que a menudo es la RBR (clasificación basada en relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos. Y que Bi y Ei denoten el número de documentos relevantes en la base de datos clasificada i-ésima de B o E. Entonces, Rn se define de la siguiente manera: = = = k i i k i i k B E R 1 1 (17) Por lo general, el objetivo es buscar solo algunas bases de datos, por lo que nuestras cifras solo muestran resultados para la selección de hasta 20 bases de datos. Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, CORI, ReDDE y UUM/HR. El algoritmo UUM/HR se describe en la Sección 3.3. Se puede observar en la Figura 3 que los algoritmos ReDDE y UUM/HR son más efectivos (en los conjuntos de pruebas representativos, relevantes y no relevantes) o igual de efectivos (en el conjunto de pruebas Trec123-100Col) que el algoritmo de selección de recursos CORI. El algoritmo UUM/HR es más efectivo que el algoritmo ReDDE en los conjuntos de pruebas representativos y relevantes y es aproximadamente igual que el algoritmo ReDDE en los conjuntos de pruebas Trec123100Col y no relevantes. Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo ReDDE. Se puede observar que al seleccionar solo algunas bases de datos en el Trec123-100Col o en los conjuntos de pruebas no relevantes, el algoritmo ReDEE tiene una pequeña ventaja sobre el algoritmo UUM/HR. Atribuimos esto a dos causas: i) El algoritmo ReDDE fue ajustado en el banco de pruebas Trec123-100Col; y ii) Aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico para estimar probabilidades de relevancia no es lo suficientemente preciso. Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este pequeño rompecabezas. Colecciones seleccionadas. Colecciones seleccionadas. Plataforma de pruebas Trec123-100Col. Plataforma de pruebas representativa. Colección seleccionada. Colección seleccionada. Plataforma de pruebas relevante. Plataforma de pruebas no relevante. Figura 3. Experimentos de selección de recursos en los cuatro bancos de pruebas. 38 6. RESULTADOS: EFECTIVIDAD DE LA RECUPERACIÓN DE DOCUMENTOS Para la recuperación de documentos, se buscan en las bases de datos seleccionadas y los resultados devueltos se fusionan en una lista final única. En todos los experimentos discutidos en esta sección, los resultados obtenidos de bases de datos individuales fueron combinados por el algoritmo de fusión de resultados de aprendizaje semisupervisado. Esta versión del algoritmo SSL [22] tiene permitido descargar un pequeño número de textos de documentos devueltos sobre la marcha para crear datos de entrenamiento adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicos de la base de datos en puntuaciones de documentos centralizadas estimadas. Se ha demostrado ser muy efectivo en entornos donde solo se obtienen listas de resultados cortas de cada base de datos seleccionada [22]. Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos. La efectividad de la recuperación de documentos se midió mediante la Precisión en la parte superior de la lista final de documentos. Los experimentos en esta sección se llevaron a cabo para estudiar la efectividad de recuperación de documentos de cinco algoritmos de selección, a saber, los algoritmos CORI, ReDDE, UUM/HR, UUM/HP-FL y UUM/HP-VL. Los últimos tres algoritmos fueron propuestos en la Sección 3. Todos los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada. El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos a recuperar de cada base de datos seleccionada; el número recuperado estaba limitado a ser de 10 a 100, y un múltiplo de 10. El Trec123-100Col y los bancos de pruebas representativos fueron seleccionados para la recuperación de documentos, ya que representan dos casos extremos de efectividad en la selección de recursos; en un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante Tabla 5. Precisión en el banco de pruebas representativo cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 documentos 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 documentos 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 documentos 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 documentos 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Tabla 6. Precisión en el banco de pruebas representativo cuando se seleccionaron 5 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 documentos 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 documentos 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 documentos 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 documentos 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Tabla 3. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 documentos 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 documentos 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 documentos 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 documentos 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Tabla 4. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 5 bases de datos. (El primer punto de referencia es CORI; el segundo punto de referencia para los métodos UUM/HP es UUM/HR). La precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 documentos 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 documentos 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 documentos 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 documentos 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 mucho peor que los otros algoritmos. Las Tablas 3 y 4 muestran los resultados en el banco de pruebas Trec123-100Col, y las Tablas 5 y 6 muestran los resultados en el banco de pruebas representativo. En el banco de pruebas Trec123-100Col, la efectividad de recuperación de documentos del algoritmo de selección CORI es aproximadamente la misma o un poco mejor que el algoritmo ReDDE, pero ambos son peores que los otros tres algoritmos (Tablas 3 y 4). El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos CORI y ReDDE. Una de las principales diferencias entre el algoritmo UUM/HR y el algoritmo ReDDE fue señalada anteriormente: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntuación de documentos centralizadas, mientras que el algoritmo ReDDE [21] utiliza un método heurístico, asume que las curvas de puntuación de documentos centralizadas son funciones escalonadas y no hace distinción entre la parte superior de las curvas. Esta diferencia hace que UUM/HR sea mejor que el algoritmo ReDDE para distinguir documentos con altas probabilidades de relevancia de aquéllos con bajas probabilidades de relevancia. Por lo tanto, el UUM/HR refleja mejor el objetivo de recuperación de alta precisión que el algoritmo ReDDE y, por lo tanto, es más efectivo para la recuperación de documentos. El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, como lo hacen los algoritmos UUM/HP-FL y UUM/HP-VL. Se puede observar que en este banco de pruebas, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos. Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de recuperación de documentos en las Ecuaciones 14 y 16. En el banco de pruebas representativo, CORI es mucho menos efectivo que otros algoritmos para la recuperación distribuida de documentos (Tablas 5 y 6). Los resultados de recuperación de documentos del algoritmo ReDDE son mejores que los del algoritmo CORI pero aún peores que los resultados del algoritmo UUM/HR. En este banco de pruebas, los tres algoritmos de UUM son aproximadamente igual de efectivos. Un análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho mayor que los experimentos en el banco de pruebas Trec123-100Col, ya que todos tienden a seleccionar las dos bases de datos grandes. Esto explica por qué son igualmente efectivos para la recuperación de documentos. En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos y reportar solo listas clasificadas de resultados. Dado que el modelo unificado de maximización de utilidad solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos. El único ajuste es que el algoritmo SSL fusiona listas clasificadas sin puntuaciones de documentos asignando a los documentos puntuaciones de pseudo-documentos normalizadas por sus rangos (En una lista clasificada de 50 documentos, el primero tiene una puntuación de 1, el segundo tiene una puntuación de 0.98, etc.), lo cual ha sido estudiado en [22]. Los resultados del experimento en el banco de pruebas trec123-100Col-bysource con 3 bases de datos seleccionadas se muestran en la Tabla 7. La configuración del experimento fue la misma que antes, excepto que las puntuaciones de los documentos fueron eliminadas intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de identificadores de documentos. Se puede observar en los resultados que el UUM/HP-FL y el UUM/HP-VL funcionan bien con bases de datos que no devuelven puntuaciones de documentos y siguen siendo más efectivos que otras alternativas. Otros experimentos con bases de datos que no devuelven puntuaciones de documentos no se informan, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL. Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión de manera explícita en la recuperación de documentos. Los nuevos algoritmos basados en este principio logran resultados mejores o al menos tan buenos como los algoritmos previos de vanguardia en varios entornos. CONCLUSIÓN La recuperación distribuida de información resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e Internet. La mayoría de investigaciones previas utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de bases de datos para la aplicación de recuperación de documentos distribuidos. Sostenemos que el objetivo de alta recuperación de recursos en la recomendación de bases de datos y el objetivo de alta precisión en la recuperación de documentos están relacionados pero no son idénticos. Este tipo de inconsistencia también ha sido observada en trabajos anteriores, pero las soluciones previas utilizaron métodos heurísticos o asumieron la cooperación de bases de datos individuales (por ejemplo, que todas las bases de datos utilizaran el mismo tipo de motores de búsqueda), lo cual frecuentemente no es cierto en un entorno no cooperativo. En este trabajo proponemos un modelo unificado de maximización de utilidad para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado. En este marco, las decisiones de selección se obtienen optimizando diferentes funciones objetivo. Hasta donde sabemos, este es el primer trabajo que intenta visualizar y modelar teóricamente la tarea de recuperación de información distribuida de manera integrada. El nuevo marco continúa una tendencia reciente de investigación que estudia el uso de muestreo basado en consultas y una base de datos de muestras centralizada. Se entrenó un único modelo logístico en la Tabla 7 centralizada. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). (Los motores de búsqueda no devuelven puntajes de documentos) Precisión en la Clasificación de Documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 documentos 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 documentos 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 documentos 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 documentos 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 base de datos de muestra para estimar las probabilidades de relevancia de documentos por sus puntajes de recuperación centralizados, mientras que la base de datos de muestra centralizada sirve como puente para conectar las bases de datos individuales con el modelo logístico centralizado. Por lo tanto, las probabilidades de relevancia para todos los documentos en las bases de datos pueden ser estimadas con una cantidad muy pequeña de juicio de relevancia humano, lo cual es mucho más eficiente que los métodos anteriores que construyen un modelo separado para cada base de datos. Este marco no solo es más sólido teóricamente, sino también muy efectivo. Un algoritmo para la selección de recursos (UUM/HR) y dos algoritmos para la recuperación de documentos (UUM/HP-FL y UUM/HP-VL) se derivan de este marco. Se han realizado estudios empíricos en bancos de pruebas para simular las soluciones de búsqueda distribuida de grandes organizaciones (empresas) o la Web oculta específica de un dominio. Además, los algoritmos de selección de recursos UUM/HP-FL y UUM/HP-VL se amplían con una variante del algoritmo de fusión de resultados SSL para abordar la tarea de recuperación de documentos distribuidos cuando las bases de datos seleccionadas no devuelven puntuaciones de documentos. Los experimentos han demostrado que estos algoritmos logran resultados que son al menos tan buenos como el estado del arte previo, y a veces considerablemente mejores. Un análisis detallado indica que la ventaja de estos algoritmos proviene de optimizar explícitamente los objetivos de las tareas específicas. El marco unificado de maximización de utilidad está abierto a diferentes extensiones. Cuando el costo está asociado con la búsqueda en las bases de datos en línea, el marco de utilidad puede ajustarse para estimar automáticamente el mejor número de bases de datos a buscar, de modo que se puedan recuperar una gran cantidad de documentos relevantes con costos relativamente bajos. Otra extensión del marco es considerar la efectividad de la recuperación de información de las bases de datos en línea, lo cual es un tema importante en los entornos operativos. Todas estas son las direcciones de la investigación futura. AGRADECIMIENTO Esta investigación fue apoyada por las subvenciones de la NSF EIA-9983253 y IIS-0118767. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este documento son del autor y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] J. Callan. (2000). Recuperación de información distribuida. En W.B. Croft, editor, Avances en Recuperación de Información. Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. \n\nEditorial Kluwer Academic. (pp. 127-150). [2] J. Callan, W.B. Croft, y J. Broglio. (1995). Experimentos TREC y TIPSTER con INQUERY. Procesamiento y Gestión de la Información, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson y M. Meziou. (2002). Selección de base de datos utilizando recursos de colección lógica adquiridos y físicos reales en un entorno operativo masivo específico de dominio. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [4] N. Craswell. (2000). Métodos para la recuperación distribuida de información. I'm sorry, but the sentence \"Ph.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate to Spanish? Tesis doctoral, Universidad Nacional Australiana. [5] N. Craswell, D. Hawking y P. Thistlewaite. (1999). Combinando resultados de motores de búsqueda aislados. En Actas de la 10ª Conferencia de Bases de Datos Australasiana. [6] D. DSouza, J. Thom y J. Zobel. (2000). Una comparación de técnicas para seleccionar colecciones de texto. En Actas de la 11ª Conferencia de Bases de Datos Australasiana. [7] N. Fuhr. (1999). Un enfoque de Teoría de la Decisión para la selección de bases de datos en IR en red. ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina y A. Paepcke. (1997). Propuesta de Stanford para la metabusqueda en internet. En Actas de la 20ª Conferencia Internacional ACM-SIGMOD sobre Gestión de Datos. [9] L. Gravano, P. Ipeirotis y M. Sahami. (2003). QProber: Un sistema para la clasificación automática de bases de datos de la web oculta. ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis y L. Gravano. (2002). Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] El kit de herramientas lemur. http://www.cs.cmu.edu/~lemur [13] J. Lu y J. Callan. (2003). Recuperación de información basada en contenido en redes peer-to-peer. En Actas de la 12ª Conferencia Internacional sobre Información y Gestión del Conocimiento. [14] W. Meng, C.T. Yu y K.L. Liu. (2002) Construcción de motores de búsqueda eficientes y efectivos. ACM Comput. Surv. 34(1). [15] H. Nottelmann y N. Fuhr. (2003). Evaluando diferentes métodos para estimar la calidad de recuperación para la selección de recursos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [16] H., Nottelmann y N., Fuhr. (2003). La arquitectura MIND para bibliotecas digitales federadas de multimedia heterogénea. Taller ACM SIGIR 2003 sobre Recuperación de Información Distribuida. [17] A.L. Powell, J.C. French, J. Callan, M. Connell y C.L. Viles. (2000). \n\nViles. (2000). El impacto de la selección de bases de datos en la búsqueda distribuida. En Actas de la 23ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [18] A.L. Powell y J.C. French. (2003). Comparando el rendimiento de los algoritmos de selección de bases de datos. ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). \n\nACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). Busca en la web invisible. Guardian Unlimited. [20] L. Si y J. Callan. (2002). Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [21] L. Si y J. Callan. (2003). Método de estimación de distribución de documentos relevantes para la selección de recursos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [22] L. Si y J. Callan. (2003). Un método de aprendizaje semi-supervisado para fusionar los resultados de un motor de búsqueda. ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41\n\nACM Transactions on Information Systems, 21(4). (pp. 457-491). 41 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "distributed text information retrieval resource selection": {
            "translated_key": "selección de recursos de recuperación de información textual distribuida",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "federated search": {
            "translated_key": "búsqueda federada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as <br>federated search</br> [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in <br>federated search</br> tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in <br>federated search</br> tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "Distributed information retrieval, also known as <br>federated search</br> [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in <br>federated search</br> tasks such as database recommendation.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in <br>federated search</br> tasks such as distributed document retrieval."
            ],
            "translated_annotated_samples": [
                "La recuperación de información distribuida, también conocida como <br>búsqueda federada</br>, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos.",
                "En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de <br>búsqueda federada</br> como la recomendación de bases de datos.",
                "Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de <br>búsqueda federada</br> como la recuperación distribuida de documentos."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como <br>búsqueda federada</br>, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La recuperación distribuida de documentos es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de recuperación de documentos distribuidos, tratándolos como objetivos de optimización diferentes. Primero, se construye una base de datos de muestra centralizada mediante el muestreo aleatorio de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas; también se estiman las estadísticas del tamaño de la base de datos. Un modelo de transformación logística se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia. Segundo, después de que se envía una nueva consulta, la consulta se puede utilizar para buscar en la base de datos de muestras centralizada que produce una puntuación para cada documento muestreado. La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada puede estimarse aplicando el modelo logístico al puntaje de cada documento. Entonces, las probabilidades de relevancia de todos los documentos (en su mayoría no vistos) entre las bases de datos disponibles pueden ser estimadas utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos. Para la tarea de selección de recursos para un sistema de recomendación de bases de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alto recall. Para la selección de recursos para un sistema distribuido de recuperación de documentos, se prefieren las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia. Este criterio de selección cumple con el objetivo de alta precisión de la aplicación de recuperación de documentos distribuidos. Además, se aplica el algoritmo de aprendizaje semisupervisado (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada. El marco de utilidad unificado hace muy pocas suposiciones y funciona en entornos no cooperativos. Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos correspondientes; y ii) Muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos. Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades correspondientes de relevancia y utiliza la base de datos de muestra centralizada como puente entre las bases de datos individuales y el modelo logístico. El esfuerzo humano (juicio de relevancia) necesario para entrenar el modelo logístico centralizado único no aumenta con el número de bases de datos. Esta es una gran ventaja sobre investigaciones anteriores, las cuales requerían que la cantidad de esfuerzo humano fuera lineal con el número de bases de datos [7,15]. El marco de utilidad unificada no solo es más sólido teóricamente, sino también muy efectivo. Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de vanguardia en una variedad de configuraciones. La siguiente sección discute el trabajo relacionado. La sección 3 describe el nuevo modelo unificado de maximización de utilidad. La sección 4 explica nuestra metodología experimental. Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos. La sección 7 concluye. 2. Investigación previa Ha habido una considerable investigación sobre todos los subproblemas de la recuperación de información distribuida. Exploramos los trabajos más relacionados en esta sección. El primer problema de la recuperación de información distribuida es la representación de recursos. El protocolo STARTS es una solución para adquirir descripciones de recursos en entornos cooperativos [8]. Sin embargo, en entornos no cooperativos, aunque las bases de datos estén dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no. Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí. Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en consultas, que genera y envía consultas de forma aleatoria a motores de búsqueda individuales y recupera algunos documentos para construir las descripciones. Dado que los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico. Los experimentos han demostrado que descripciones de recursos bastante precisas pueden ser construidas enviando alrededor de 80 consultas y descargando alrededor de 300 documentos [1]. Muchos algoritmos de selección de recursos como gGlOSS/vGlOSS [8] y CORI [1] han sido propuestos en la última década. El algoritmo CORI representa cada base de datos por sus términos, las frecuencias de los documentos y un pequeño número de estadísticas del corpus (detalles en [1]). Como investigaciones previas en diferentes conjuntos de datos han demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo utilizamos como algoritmo base en este trabajo. El algoritmo de selección de recursos de estimación de distribución de documentos relevantes (ReDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia. Aunque se ha demostrado que el algoritmo ReDDE es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21]. El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar puntuaciones de documentos específicas de la base de datos en puntuaciones de documentos independientes de la base de datos comparables. El algoritmo de fusión de resultados de aprendizaje semisupervisado (SSL) [20,22] utiliza los documentos adquiridos mediante muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos y de la consulta. Estos modelos lineales se utilizan para convertir las puntuaciones de documentos específicas de la base de datos en las puntuaciones de documentos centralizadas aproximadas. El algoritmo SSL ha demostrado ser efectivo [22]. Sirve como un componente importante de nuestro marco unificado de maximización de utilidad (Sección 3). Para lograr resultados precisos en la recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos en sistemas de recomendación de bases de datos. Pero como se señaló anteriormente, un algoritmo de selección de recursos optimizado para un alto recuerdo puede no funcionar bien para la recuperación de documentos, que tiene como objetivo la alta precisión. Este tipo de inconsistencia ha sido observada en investigaciones previas [4,21]. La investigación en [21] intentó resolver el problema con un método heurístico. La investigación más similar a lo que proponemos aquí es el marco teórico de la toma de decisiones (DTF) [7,15]. Este marco de trabajo calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y se han propuesto varios métodos [15] para estimar la calidad de recuperación. Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF. Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de bases de datos y recuperación distribuida de documentos) en el mismo marco unificado. Segundo, el DTF construye un modelo para cada base de datos para calcular las probabilidades de relevancia. Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos. Por el contrario, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada. La base de datos de muestra centralizada puede servir como puente para conectar las bases de datos individuales con el modelo logístico centralizado, de esta manera se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos. Esta estrategia puede ahorrar una gran cantidad de esfuerzo en juicio humano y es una gran ventaja del marco de maximización de utilidad unificada sobre el DTF, especialmente cuando hay un gran número de bases de datos. MARCO DE MAXIMIZACIÓN DE UTILIDAD UNIFICADA El marco de Maximización de Utilidad Unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría no vistos) disponibles en el entorno de búsqueda distribuida. En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el modelo de Maximización de Utilidad Unificado. También describimos cómo el modelo puede ser optimizado para el objetivo de alto recuerdo de un sistema de recomendación de base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido. 3.1 Estimación de Probabilidades de Relevancia Como se señaló anteriormente, el propósito de la selección de recursos es el alto recuerdo y el propósito de la recuperación de documentos es la alta precisión. Para cumplir con estos objetivos diversos, el problema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos. Este es un problema difícil porque solo podemos observar una muestra de los contenidos de cada base de datos utilizando muestreo basado en consultas. Nuestra estrategia es aprovechar al máximo toda la información disponible para calcular las estimaciones de probabilidad. 3.1.1 Aprendizaje de Probabilidades de Relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se construye mediante muestreo basado en consultas y los tamaños de la base de datos se estiman utilizando el método de muestreo y remuestreo [21]. Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (Inquery [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de entrenamiento. Para cada consulta de entrenamiento, se aplica el algoritmo de selección de recursos CORI [1] para seleccionar un cierto número (por ejemplo, 10) de bases de datos y recuperar 50 identificadores de documentos de cada base de datos. El algoritmo de fusión de resultados SSL [20,22] se utiliza para combinar los resultados. Luego, podemos descargar los 50 documentos principales de la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando Inquery y las estadísticas del corpus de la base de datos de muestra centralizada. Las puntuaciones centralizadas se normalizan aún más (dividiéndolas por la puntuación centralizada máxima para cada consulta), ya que este método ha sido sugerido para mejorar la precisión de la estimación en investigaciones anteriores [15]. El juicio humano se adquiere para esos documentos y se construye un modelo logístico para transformar las puntuaciones de documentos centralizados normalizados en probabilidades de relevancia de la siguiente manera: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) donde )( _ dSc es la puntuación de documento centralizada normalizada y ac y bc son los dos parámetros del modelo logístico. Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de entrenamiento. El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de las puntuaciones de documentos centralizadas. 3.1.2 Estimación de las puntuaciones de documentos centralizadas Cuando el usuario envía una nueva consulta, se calculan las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar las puntuaciones de los documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados. Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas del tamaño de la base de datos. Definimos el factor de escala de la base de datos para la base de datos i como la razón entre el tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: SF_i = ^N_db / _N_db_samp_i donde ^N_db es el tamaño estimado de la base de datos y _N_db_samp_i es el número de documentos de la base de datos i en la base de datos de muestra centralizada. La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podríamos suponer que hay alrededor de 50 documentos en esa base de datos que tienen puntuaciones de alrededor de 0.5. De hecho, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntuación del documento centralizado para cada base de datos. Formalmente, clasificamos todos los documentos muestreados de la base de datos i-ésima por sus puntajes de documento centralizado 34 para obtener la lista de puntajes de documento centralizado muestreado {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} para la base de datos i; asumimos que si pudiéramos calcular los puntajes de documento centralizado para todos los documentos en esta base de datos y obtener la lista completa de puntajes de documento centralizado, el documento superior en la lista muestreada tendría un rango de SFdbi/2, el segundo documento en la lista muestreada tendría un rango de SFdbi3/2, y así sucesivamente. Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. La interpolación lineal por tramos se aplica para estimar la curva de puntuación del documento centralizado, como se ilustra en la Figura 1. La lista completa de puntuaciones de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documentos centralizados como: ],1[,)(S ^^ c idbij Njd ∈ . Se puede observar en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntuación del documento centralizado. Sin embargo, para bases de datos con grandes proporciones de escala de base de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos mejor clasificados (por ejemplo, [1, SFdbi/2]). Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizados de los documentos mejor clasificados para bases de datos con ratios a gran escala (por ejemplo, mayores de 100). Específicamente, se construye un modelo logístico para cada una de estas bases de datos. El modelo logístico se utiliza para estimar la puntuación del documento centralizado superior 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con las puntuaciones centralizadas más altas. 0iα , 1iα y 2iα son los parámetros del modelo logístico. Para cada consulta de entrenamiento, se descarga el documento mejor recuperado de cada base de datos y se calcula la puntuación del documento centralizado correspondiente. Junto con las puntuaciones de los dos documentos muestreados principales, estos parámetros pueden ser estimados. Después de estimar la puntuación centralizada del documento principal, se ajusta una función exponencial para la parte superior ([1, SFdbi/2]) de la curva de puntuación del documento centralizado como: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) Los dos parámetros 0iβ y 1iβ se ajustan para asegurarse de que la función exponencial pase por los dos puntos (1, ^ 1)( ic dS ) y (SFdbi/2, Sc(dsi1)). La función exponencial se utiliza únicamente para ajustar la parte superior de la curva de puntuación del documento centralizado, mientras que la parte inferior de la curva sigue siendo ajustada con el método de interpolación lineal descrito anteriormente. El ajuste mediante la función exponencial de los documentos mejor clasificados ha demostrado empíricamente producir resultados más precisos. A partir de las curvas de puntuación de documentos centralizadas, podemos estimar las listas completas de puntuación de documentos centralizados correspondientes para todas las bases de datos disponibles. Después de que las puntuaciones estimadas de los documentos centralizados se normalizan, las listas completas de probabilidades de relevancia pueden ser construidas a partir de las listas completas de puntuaciones de documentos centralizados mediante la Ecuación 1. Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El Modelo Unificado de Maximización de Utilidad En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco. En la tarea de recomendación de bases de datos, el sistema necesita decidir cómo clasificar las bases de datos. En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada. Generalizamos el proceso de selección de recomendaciones de bases de datos, que implícitamente recomienda todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos. Formalmente, denotamos di como el número de documentos que nos gustaría recuperar de la base de datos i y ,.....},{ 21 ddd = como una acción de selección para todas las bases de datos. La decisión de selección de la base de datos se toma en base a las listas completas de probabilidades de relevancia para todas las bases de datos. Las listas completas de probabilidades de relevancia se infieren a partir de toda la información disponible, específicamente sR, que representa las descripciones de recursos adquiridas mediante muestreo basado en consultas y las estimaciones del tamaño de la base de datos adquiridas mediante muestreo-resampleo; cS representa las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Si el método de estimación de puntajes de documentos centralizados y probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈. El vector aleatorio   denota un conjunto arbitrario de listas completas de probabilidades de relevancia y ),|( cs SRP θ como la probabilidad de generar este conjunto de listas. Finalmente, a cada acción de selección d y un conjunto de listas completas de la Figura 1. Construcción de la lista completa de puntuación de documentos centralizada mediante interpolación lineal (el factor de escala de la base de datos es 50). Para 35 probabilidades de relevancia θ, asociamos una función de utilidad ),( dU θ que indica el beneficio de realizar la selección d cuando las verdaderas listas completas de probabilidades de relevancia son θ. Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7). Un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en los valores de parámetros más probables en lugar de calcular toda la expectativa. En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de <br>búsqueda federada</br> como la recomendación de bases de datos. El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de N bases de datos de Nsdb) que contengan tantos documentos relevantes como sea posible, lo cual puede definirse formalmente como: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) es la función indicadora, que es 1 cuando se selecciona la i-ésima base de datos y 0 en caso contrario. Inserta esta ecuación en el modelo básico de la Ecuación 8 y asocia la restricción del número de base de datos seleccionado para obtener lo siguiente: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) La solución de este problema de optimización es muy simple. Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) Las bases de datos Nsdb con el mayor número esperado de documentos relevantes pueden ser seleccionadas para cumplir con el objetivo de alto recall. Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de <br>búsqueda federada</br> como la recuperación distribuida de documentos. Se mide mediante la Precisión en la parte superior de la lista final de documentos fusionados. Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la Precisión de los documentos recuperados de las bases de datos seleccionadas. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Tenga en cuenta que la diferencia clave entre la Ecuación 12 y la Ecuación 9 es que la Ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos, mientras que la Ecuación 12 solo considera una parte mucho más pequeña de la clasificación. Específicamente, podemos calcular la decisión de selección óptima mediante: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Diferentes tipos de restricciones causadas por las diferentes características de las tareas de recuperación de documentos pueden estar asociadas con el problema de optimización anterior. La más común es seleccionar un número fijo (Nsdb) de bases de datos y recuperar un número fijo (Nrdoc) de documentos de cada base de datos seleccionada, definido formalmente como: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) Este problema de optimización puede resolverse fácilmente calculando el número de documentos relevantes esperados en la parte superior de la lista completa de probabilidades de relevancia de cada base de datos: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Luego, las bases de datos pueden ser clasificadas por estos valores y seleccionadas. Llamamos a este algoritmo UUM/HP-FL (Maximización Unificada de Utilidad para Alta Precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada). Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada. Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes cantidades de documentos. Para simplificar, se requiere que las longitudes de la lista de resultados sean múltiplos de un número base 10. (Este valor también puede variar, pero para simplificar se establece en 10 en este documento). Esta restricción está establecida para simular el comportamiento de los motores de búsqueda comerciales en la web. (Motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 identificadores de documentos por página de resultados). Este procedimiento ahorra tiempo de cálculo al calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea de 10 en lugar de 1 (más detalles se discuten posteriormente). Para una mayor simplificación, restringimos la selección a un máximo de 100 documentos de cada base de datos (di<=100). Luego, el problema de optimización de la selección se formaliza de la siguiente manera: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc es el número total de documentos a recuperar. Desafortunadamente, no hay una solución simple para este problema de optimización como la hay para las Ecuaciones 10 y 14. Sin embargo, se puede aplicar un algoritmo de programación dinámica de 36 para calcular la solución óptima. Los pasos básicos de este método de programación dinámica se describen en la Figura 2. Dado que este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se le llama algoritmo UUM/HP-VL. Después de que se toman las decisiones de selección, se buscan las bases de datos seleccionadas y se recuperan los identificadores de documentos correspondientes de cada base de datos. El paso final de la recuperación de documentos es fusionar los resultados devueltos en una única lista clasificada con el algoritmo de aprendizaje semisupervisado. Se señaló anteriormente que el algoritmo SSL mapea las puntuaciones específicas de la base de datos en las puntuaciones de documentos centralizadas y construye la lista clasificada final en consecuencia, lo cual es consistente con todos nuestros procedimientos de selección donde se seleccionan los documentos con mayores probabilidades de relevancia (y por ende, puntuaciones de documentos centralizadas más altas). 4. METODOLOGÍA EXPERIMENTAL 4.1 Bancos de pruebas Es deseable evaluar algoritmos de recuperación de información distribuida con bancos de pruebas que simulen de cerca las aplicaciones del mundo real. Las colecciones web TREC WT2g o WT10g proporcionan una forma de dividir los documentos por diferentes servidores web. De esta manera, se podrían crear un gran número (O(1000)) de bases de datos con contenidos bastante diversos, lo que podría convertir a este banco de pruebas en un buen candidato para simular entornos operativos como la web oculta de dominio abierto. Sin embargo, dos debilidades de este banco de pruebas son: i) Cada base de datos contiene solo una pequeña cantidad de documentos (259 documentos en promedio para WT2g) [4]; y ii) El contenido de WT2g o WT10g se extrae arbitrariamente de la web. No es probable que una base de datos web oculta proporcione páginas personales o páginas web que indiquen que las páginas están en construcción y no contengan información útil en absoluto. Estos tipos de páginas web están contenidos en los conjuntos de datos WT2g/WT10g. Por lo tanto, los datos ruidosos de la Web no son similares a los contenidos de alta calidad de las bases de datos ocultas de la Web, que generalmente están organizados por expertos en el dominio. Otra opción es los datos de noticias/gobierno de TREC [1,15,17,18,21]. Los datos gubernamentales/noticias de TREC se centran en temas relativamente específicos. Comparado con los datos web de TREC: i) Los documentos de noticias/gobierno son mucho más similares a los contenidos proporcionados por una base de datos orientada a temas que a una página web arbitraria, ii) Una base de datos en este banco de pruebas es más grande que la de los datos web de TREC. En promedio, una base de datos contiene miles de documentos, lo cual es más realista que una base de datos de datos web de TREC con alrededor de 250 documentos. Dado que los contenidos y tamaños de las bases de datos en el banco de pruebas de noticias/gobierno de TREC son más similares a los de una base de datos orientada a temas, es un buen candidato para simular los entornos de recuperación de información distribuida de grandes organizaciones (empresas) o sitios web ocultos específicos de dominio, como West, que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3]. Dado que la mayoría de los sistemas actuales de recuperación de información distribuida están desarrollados para entornos de grandes organizaciones (empresas) o para la Web oculta de dominios específicos en lugar de la Web oculta de dominio abierto, en este trabajo se eligió el banco de pruebas de noticias/gobierno de TREC. El banco de pruebas Trec123-100col-bysource es uno de los más utilizados en las pruebas de noticias y gobierno de TREC [1,15,17,21]. Fue elegido en este trabajo. Tres bancos de pruebas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes también se utilizaron para proporcionar una simulación más exhaustiva para entornos reales. Se crearon 100 bases de datos a partir de los CDs de TREC 1, 2 y 3. Fueron organizados por fuente y fecha de publicación [1]. Los tamaños de las bases de datos no están sesgados. Los detalles se encuentran en la Tabla 1. Tres bancos de pruebas construidos en [21] se basaron en el banco de pruebas trec123-100colbysource. Cada banco de pruebas contiene muchas bases de datos pequeñas y dos bases de datos grandes creadas al fusionar alrededor de 10 a 20 bases de datos pequeñas. Listas completas de probabilidades de relevancia para todas las bases de datos |DB|. Solución de selección óptima para la Ecuación 16. i) Crear el arreglo tridimensional: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Cada Sel (x, y, z) está asociado con una decisión de selección xyzd, que representa la mejor decisión de selección en la condición: solo se consideran bases de datos del número 1 al número x para la selección; se recuperarán un total de y*10 documentos; solo se seleccionan z bases de datos de los candidatos de la base de datos x. Y Sel (x, y, z) es el valor de utilidad correspondiente al elegir la mejor selección. ii) Inicializar Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) solo con la información de relevancia estimada de la 1ª base de datos. iii) Iterar el candidato actual de la base de datos i desde 2 hasta |DB| Para cada entrada Sel (i, y, z): Encontrar k tal que: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k∗ documentos de la base de datos i-ésima, de lo contrario no debemos seleccionar esta base de datos y se debe mantener la solución anterior mejor Sel (i-1, y, z). Luego establezca el valor de iyzd y Sel (i, y, z) en consecuencia. iv) La mejor solución de selección se da por _ /10| | Toral rdoc sdbDB N Nd y el valor de utilidad correspondiente es Sel (|DB|, NTotal_rdoc/10, Nsdb). Figura 2. El procedimiento de optimización de programación dinámica para la Ecuación 16. Tabla 1: Estadísticas del banco de pruebas. Número de documentos Tamaño (MB) Tamaño del banco de pruebas (GB) Mínimo Promedio Máximo Mínimo Promedio Máximo Trec123 3.2 752 10782 39713 28 32 42 Tabla 2: Estadísticas del conjunto de consultas. Nombre del conjunto de temas TREC Campo del tema TREC Longitud promedio (palabras) Trec123 51-150 Título 3.1 37 Trec123-2ldb-60col (representativo): Las bases de datos en el trec123-100col-bysource se ordenaron en orden alfabético. Dos grandes bases de datos fueron creadas al fusionar 20 bases de datos pequeñas con el método de round-robin. Por lo tanto, las dos bases de datos grandes tienen más documentos relevantes debido a sus tamaños grandes, aunque las densidades de documentos relevantes son aproximadamente iguales a las de las bases de datos pequeñas. Las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, APall y WSJall. Las otras 60 colecciones quedaron sin cambios. Las bases de datos APall y WSJall tienen una mayor densidad de documentos relevantes para las consultas de TREC que las bases de datos pequeñas. Por lo tanto, las dos bases de datos grandes tienen muchos más documentos relevantes que las bases de datos pequeñas. Las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, FRall y DOEall. Las otras 80 colecciones quedaron sin cambios. Las bases de datos FRall y DOEall tienen densidades más bajas de documentos relevantes para las consultas de TREC que las bases de datos pequeñas, a pesar de ser mucho más grandes. Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150. Las consultas 101-150 se utilizaron como consultas de entrenamiento y las consultas 51-100 se utilizaron como consultas de prueba (detalles en la Tabla 2). 4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o en la Web oculta específica de dominio, diferentes bases de datos pueden utilizar diferentes tipos de motores de búsqueda. Para simular el entorno de múltiples motores de búsqueda, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: INQUERY [2], un modelo de lenguaje estadístico de unigrama con suavizado lineal [12,20] y un algoritmo de recuperación TFIDF con peso ltc [12,20]. Todos estos algoritmos fueron implementados con la herramienta Lemur [12]. Estos tres tipos de motores de búsqueda fueron asignados a las bases de datos entre los cuatro bancos de pruebas de manera round-robin. 5. RESULTADOS: SELECCIÓN DE RECURSOS DE LA RECOMENDACIÓN DE BASES DE DATOS Todos los cuatro bancos de pruebas descritos en la Sección 4 fueron utilizados en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de bases de datos. Las descripciones de los recursos fueron creadas utilizando muestreo basado en consultas. Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos. Las estadísticas del tamaño de la base de datos fueron estimadas mediante el método de muestra y remuestra [21]. Cincuenta consultas (101-150) se utilizaron como consultas de entrenamiento para construir el modelo logístico relevante y ajustar las funciones exponenciales de las curvas de puntuación de documentos centralizados para bases de datos de gran proporción (detalles en la Sección 3.1). Otros 50 consultas (51-100) se utilizaron como datos de prueba. Los algoritmos de selección de recursos de los sistemas de recomendación de bases de datos suelen compararse utilizando la métrica de recuperación nR [1,17,18,21]. Que B denote una clasificación base, que a menudo es la RBR (clasificación basada en relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos. Y que Bi y Ei denoten el número de documentos relevantes en la base de datos clasificada i-ésima de B o E. Entonces, Rn se define de la siguiente manera: = = = k i i k i i k B E R 1 1 (17) Por lo general, el objetivo es buscar solo algunas bases de datos, por lo que nuestras cifras solo muestran resultados para la selección de hasta 20 bases de datos. Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, CORI, ReDDE y UUM/HR. El algoritmo UUM/HR se describe en la Sección 3.3. Se puede observar en la Figura 3 que los algoritmos ReDDE y UUM/HR son más efectivos (en los conjuntos de pruebas representativos, relevantes y no relevantes) o igual de efectivos (en el conjunto de pruebas Trec123-100Col) que el algoritmo de selección de recursos CORI. El algoritmo UUM/HR es más efectivo que el algoritmo ReDDE en los conjuntos de pruebas representativos y relevantes y es aproximadamente igual que el algoritmo ReDDE en los conjuntos de pruebas Trec123100Col y no relevantes. Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo ReDDE. Se puede observar que al seleccionar solo algunas bases de datos en el Trec123-100Col o en los conjuntos de pruebas no relevantes, el algoritmo ReDEE tiene una pequeña ventaja sobre el algoritmo UUM/HR. Atribuimos esto a dos causas: i) El algoritmo ReDDE fue ajustado en el banco de pruebas Trec123-100Col; y ii) Aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico para estimar probabilidades de relevancia no es lo suficientemente preciso. Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este pequeño rompecabezas. Colecciones seleccionadas. Colecciones seleccionadas. Plataforma de pruebas Trec123-100Col. Plataforma de pruebas representativa. Colección seleccionada. Colección seleccionada. Plataforma de pruebas relevante. Plataforma de pruebas no relevante. Figura 3. Experimentos de selección de recursos en los cuatro bancos de pruebas. 38 6. RESULTADOS: EFECTIVIDAD DE LA RECUPERACIÓN DE DOCUMENTOS Para la recuperación de documentos, se buscan en las bases de datos seleccionadas y los resultados devueltos se fusionan en una lista final única. En todos los experimentos discutidos en esta sección, los resultados obtenidos de bases de datos individuales fueron combinados por el algoritmo de fusión de resultados de aprendizaje semisupervisado. Esta versión del algoritmo SSL [22] tiene permitido descargar un pequeño número de textos de documentos devueltos sobre la marcha para crear datos de entrenamiento adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicos de la base de datos en puntuaciones de documentos centralizadas estimadas. Se ha demostrado ser muy efectivo en entornos donde solo se obtienen listas de resultados cortas de cada base de datos seleccionada [22]. Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos. La efectividad de la recuperación de documentos se midió mediante la Precisión en la parte superior de la lista final de documentos. Los experimentos en esta sección se llevaron a cabo para estudiar la efectividad de recuperación de documentos de cinco algoritmos de selección, a saber, los algoritmos CORI, ReDDE, UUM/HR, UUM/HP-FL y UUM/HP-VL. Los últimos tres algoritmos fueron propuestos en la Sección 3. Todos los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada. El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos a recuperar de cada base de datos seleccionada; el número recuperado estaba limitado a ser de 10 a 100, y un múltiplo de 10. El Trec123-100Col y los bancos de pruebas representativos fueron seleccionados para la recuperación de documentos, ya que representan dos casos extremos de efectividad en la selección de recursos; en un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante Tabla 5. Precisión en el banco de pruebas representativo cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 documentos 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 documentos 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 documentos 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 documentos 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Tabla 6. Precisión en el banco de pruebas representativo cuando se seleccionaron 5 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 documentos 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 documentos 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 documentos 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 documentos 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Tabla 3. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 documentos 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 documentos 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 documentos 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 documentos 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Tabla 4. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 5 bases de datos. (El primer punto de referencia es CORI; el segundo punto de referencia para los métodos UUM/HP es UUM/HR). La precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 documentos 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 documentos 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 documentos 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 documentos 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 mucho peor que los otros algoritmos. Las Tablas 3 y 4 muestran los resultados en el banco de pruebas Trec123-100Col, y las Tablas 5 y 6 muestran los resultados en el banco de pruebas representativo. En el banco de pruebas Trec123-100Col, la efectividad de recuperación de documentos del algoritmo de selección CORI es aproximadamente la misma o un poco mejor que el algoritmo ReDDE, pero ambos son peores que los otros tres algoritmos (Tablas 3 y 4). El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos CORI y ReDDE. Una de las principales diferencias entre el algoritmo UUM/HR y el algoritmo ReDDE fue señalada anteriormente: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntuación de documentos centralizadas, mientras que el algoritmo ReDDE [21] utiliza un método heurístico, asume que las curvas de puntuación de documentos centralizadas son funciones escalonadas y no hace distinción entre la parte superior de las curvas. Esta diferencia hace que UUM/HR sea mejor que el algoritmo ReDDE para distinguir documentos con altas probabilidades de relevancia de aquéllos con bajas probabilidades de relevancia. Por lo tanto, el UUM/HR refleja mejor el objetivo de recuperación de alta precisión que el algoritmo ReDDE y, por lo tanto, es más efectivo para la recuperación de documentos. El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, como lo hacen los algoritmos UUM/HP-FL y UUM/HP-VL. Se puede observar que en este banco de pruebas, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos. Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de recuperación de documentos en las Ecuaciones 14 y 16. En el banco de pruebas representativo, CORI es mucho menos efectivo que otros algoritmos para la recuperación distribuida de documentos (Tablas 5 y 6). Los resultados de recuperación de documentos del algoritmo ReDDE son mejores que los del algoritmo CORI pero aún peores que los resultados del algoritmo UUM/HR. En este banco de pruebas, los tres algoritmos de UUM son aproximadamente igual de efectivos. Un análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho mayor que los experimentos en el banco de pruebas Trec123-100Col, ya que todos tienden a seleccionar las dos bases de datos grandes. Esto explica por qué son igualmente efectivos para la recuperación de documentos. En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos y reportar solo listas clasificadas de resultados. Dado que el modelo unificado de maximización de utilidad solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos. El único ajuste es que el algoritmo SSL fusiona listas clasificadas sin puntuaciones de documentos asignando a los documentos puntuaciones de pseudo-documentos normalizadas por sus rangos (En una lista clasificada de 50 documentos, el primero tiene una puntuación de 1, el segundo tiene una puntuación de 0.98, etc.), lo cual ha sido estudiado en [22]. Los resultados del experimento en el banco de pruebas trec123-100Col-bysource con 3 bases de datos seleccionadas se muestran en la Tabla 7. La configuración del experimento fue la misma que antes, excepto que las puntuaciones de los documentos fueron eliminadas intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de identificadores de documentos. Se puede observar en los resultados que el UUM/HP-FL y el UUM/HP-VL funcionan bien con bases de datos que no devuelven puntuaciones de documentos y siguen siendo más efectivos que otras alternativas. Otros experimentos con bases de datos que no devuelven puntuaciones de documentos no se informan, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL. Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión de manera explícita en la recuperación de documentos. Los nuevos algoritmos basados en este principio logran resultados mejores o al menos tan buenos como los algoritmos previos de vanguardia en varios entornos. CONCLUSIÓN La recuperación distribuida de información resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e Internet. La mayoría de investigaciones previas utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de bases de datos para la aplicación de recuperación de documentos distribuidos. Sostenemos que el objetivo de alta recuperación de recursos en la recomendación de bases de datos y el objetivo de alta precisión en la recuperación de documentos están relacionados pero no son idénticos. Este tipo de inconsistencia también ha sido observada en trabajos anteriores, pero las soluciones previas utilizaron métodos heurísticos o asumieron la cooperación de bases de datos individuales (por ejemplo, que todas las bases de datos utilizaran el mismo tipo de motores de búsqueda), lo cual frecuentemente no es cierto en un entorno no cooperativo. En este trabajo proponemos un modelo unificado de maximización de utilidad para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado. En este marco, las decisiones de selección se obtienen optimizando diferentes funciones objetivo. Hasta donde sabemos, este es el primer trabajo que intenta visualizar y modelar teóricamente la tarea de recuperación de información distribuida de manera integrada. El nuevo marco continúa una tendencia reciente de investigación que estudia el uso de muestreo basado en consultas y una base de datos de muestras centralizada. Se entrenó un único modelo logístico en la Tabla 7 centralizada. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). (Los motores de búsqueda no devuelven puntajes de documentos) Precisión en la Clasificación de Documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 documentos 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 documentos 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 documentos 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 documentos 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 base de datos de muestra para estimar las probabilidades de relevancia de documentos por sus puntajes de recuperación centralizados, mientras que la base de datos de muestra centralizada sirve como puente para conectar las bases de datos individuales con el modelo logístico centralizado. Por lo tanto, las probabilidades de relevancia para todos los documentos en las bases de datos pueden ser estimadas con una cantidad muy pequeña de juicio de relevancia humano, lo cual es mucho más eficiente que los métodos anteriores que construyen un modelo separado para cada base de datos. Este marco no solo es más sólido teóricamente, sino también muy efectivo. Un algoritmo para la selección de recursos (UUM/HR) y dos algoritmos para la recuperación de documentos (UUM/HP-FL y UUM/HP-VL) se derivan de este marco. Se han realizado estudios empíricos en bancos de pruebas para simular las soluciones de búsqueda distribuida de grandes organizaciones (empresas) o la Web oculta específica de un dominio. Además, los algoritmos de selección de recursos UUM/HP-FL y UUM/HP-VL se amplían con una variante del algoritmo de fusión de resultados SSL para abordar la tarea de recuperación de documentos distribuidos cuando las bases de datos seleccionadas no devuelven puntuaciones de documentos. Los experimentos han demostrado que estos algoritmos logran resultados que son al menos tan buenos como el estado del arte previo, y a veces considerablemente mejores. Un análisis detallado indica que la ventaja de estos algoritmos proviene de optimizar explícitamente los objetivos de las tareas específicas. El marco unificado de maximización de utilidad está abierto a diferentes extensiones. Cuando el costo está asociado con la búsqueda en las bases de datos en línea, el marco de utilidad puede ajustarse para estimar automáticamente el mejor número de bases de datos a buscar, de modo que se puedan recuperar una gran cantidad de documentos relevantes con costos relativamente bajos. Otra extensión del marco es considerar la efectividad de la recuperación de información de las bases de datos en línea, lo cual es un tema importante en los entornos operativos. Todas estas son las direcciones de la investigación futura. AGRADECIMIENTO Esta investigación fue apoyada por las subvenciones de la NSF EIA-9983253 y IIS-0118767. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este documento son del autor y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] J. Callan. (2000). Recuperación de información distribuida. En W.B. Croft, editor, Avances en Recuperación de Información. Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. \n\nEditorial Kluwer Academic. (pp. 127-150). [2] J. Callan, W.B. Croft, y J. Broglio. (1995). Experimentos TREC y TIPSTER con INQUERY. Procesamiento y Gestión de la Información, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson y M. Meziou. (2002). Selección de base de datos utilizando recursos de colección lógica adquiridos y físicos reales en un entorno operativo masivo específico de dominio. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [4] N. Craswell. (2000). Métodos para la recuperación distribuida de información. I'm sorry, but the sentence \"Ph.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate to Spanish? Tesis doctoral, Universidad Nacional Australiana. [5] N. Craswell, D. Hawking y P. Thistlewaite. (1999). Combinando resultados de motores de búsqueda aislados. En Actas de la 10ª Conferencia de Bases de Datos Australasiana. [6] D. DSouza, J. Thom y J. Zobel. (2000). Una comparación de técnicas para seleccionar colecciones de texto. En Actas de la 11ª Conferencia de Bases de Datos Australasiana. [7] N. Fuhr. (1999). Un enfoque de Teoría de la Decisión para la selección de bases de datos en IR en red. ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina y A. Paepcke. (1997). Propuesta de Stanford para la metabusqueda en internet. En Actas de la 20ª Conferencia Internacional ACM-SIGMOD sobre Gestión de Datos. [9] L. Gravano, P. Ipeirotis y M. Sahami. (2003). QProber: Un sistema para la clasificación automática de bases de datos de la web oculta. ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis y L. Gravano. (2002). Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] El kit de herramientas lemur. http://www.cs.cmu.edu/~lemur [13] J. Lu y J. Callan. (2003). Recuperación de información basada en contenido en redes peer-to-peer. En Actas de la 12ª Conferencia Internacional sobre Información y Gestión del Conocimiento. [14] W. Meng, C.T. Yu y K.L. Liu. (2002) Construcción de motores de búsqueda eficientes y efectivos. ACM Comput. Surv. 34(1). [15] H. Nottelmann y N. Fuhr. (2003). Evaluando diferentes métodos para estimar la calidad de recuperación para la selección de recursos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [16] H., Nottelmann y N., Fuhr. (2003). La arquitectura MIND para bibliotecas digitales federadas de multimedia heterogénea. Taller ACM SIGIR 2003 sobre Recuperación de Información Distribuida. [17] A.L. Powell, J.C. French, J. Callan, M. Connell y C.L. Viles. (2000). \n\nViles. (2000). El impacto de la selección de bases de datos en la búsqueda distribuida. En Actas de la 23ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [18] A.L. Powell y J.C. French. (2003). Comparando el rendimiento de los algoritmos de selección de bases de datos. ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). \n\nACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). Busca en la web invisible. Guardian Unlimited. [20] L. Si y J. Callan. (2002). Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [21] L. Si y J. Callan. (2003). Método de estimación de distribución de documentos relevantes para la selección de recursos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [22] L. Si y J. Callan. (2003). Un método de aprendizaje semi-supervisado para fusionar los resultados de un motor de búsqueda. ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41\n\nACM Transactions on Information Systems, 21(4). (pp. 457-491). 41 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "hidden web content": {
            "translated_key": "contenido web oculto",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "resource representation": {
            "translated_key": "representación de recursos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (<br>resource representation</br>) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is <br>resource representation</br>.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "First, information about the contents of each individual database must be acquired (<br>resource representation</br>) [1,8,21].",
                "The first problem of distributed information retrieval is <br>resource representation</br>."
            ],
            "translated_annotated_samples": [
                "Primero, se debe adquirir información sobre el contenido de cada base de datos individual (<br>representación de recursos</br>) [1,8,21].",
                "El primer problema de la recuperación de información distribuida es la <br>representación de recursos</br>."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (<br>representación de recursos</br>) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La recuperación distribuida de documentos es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de recuperación de documentos distribuidos, tratándolos como objetivos de optimización diferentes. Primero, se construye una base de datos de muestra centralizada mediante el muestreo aleatorio de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas; también se estiman las estadísticas del tamaño de la base de datos. Un modelo de transformación logística se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia. Segundo, después de que se envía una nueva consulta, la consulta se puede utilizar para buscar en la base de datos de muestras centralizada que produce una puntuación para cada documento muestreado. La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada puede estimarse aplicando el modelo logístico al puntaje de cada documento. Entonces, las probabilidades de relevancia de todos los documentos (en su mayoría no vistos) entre las bases de datos disponibles pueden ser estimadas utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos. Para la tarea de selección de recursos para un sistema de recomendación de bases de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alto recall. Para la selección de recursos para un sistema distribuido de recuperación de documentos, se prefieren las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia. Este criterio de selección cumple con el objetivo de alta precisión de la aplicación de recuperación de documentos distribuidos. Además, se aplica el algoritmo de aprendizaje semisupervisado (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada. El marco de utilidad unificado hace muy pocas suposiciones y funciona en entornos no cooperativos. Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos correspondientes; y ii) Muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos. Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades correspondientes de relevancia y utiliza la base de datos de muestra centralizada como puente entre las bases de datos individuales y el modelo logístico. El esfuerzo humano (juicio de relevancia) necesario para entrenar el modelo logístico centralizado único no aumenta con el número de bases de datos. Esta es una gran ventaja sobre investigaciones anteriores, las cuales requerían que la cantidad de esfuerzo humano fuera lineal con el número de bases de datos [7,15]. El marco de utilidad unificada no solo es más sólido teóricamente, sino también muy efectivo. Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de vanguardia en una variedad de configuraciones. La siguiente sección discute el trabajo relacionado. La sección 3 describe el nuevo modelo unificado de maximización de utilidad. La sección 4 explica nuestra metodología experimental. Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos. La sección 7 concluye. 2. Investigación previa Ha habido una considerable investigación sobre todos los subproblemas de la recuperación de información distribuida. Exploramos los trabajos más relacionados en esta sección. El primer problema de la recuperación de información distribuida es la <br>representación de recursos</br>. El protocolo STARTS es una solución para adquirir descripciones de recursos en entornos cooperativos [8]. Sin embargo, en entornos no cooperativos, aunque las bases de datos estén dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no. Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí. Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en consultas, que genera y envía consultas de forma aleatoria a motores de búsqueda individuales y recupera algunos documentos para construir las descripciones. Dado que los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico. Los experimentos han demostrado que descripciones de recursos bastante precisas pueden ser construidas enviando alrededor de 80 consultas y descargando alrededor de 300 documentos [1]. Muchos algoritmos de selección de recursos como gGlOSS/vGlOSS [8] y CORI [1] han sido propuestos en la última década. El algoritmo CORI representa cada base de datos por sus términos, las frecuencias de los documentos y un pequeño número de estadísticas del corpus (detalles en [1]). Como investigaciones previas en diferentes conjuntos de datos han demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo utilizamos como algoritmo base en este trabajo. El algoritmo de selección de recursos de estimación de distribución de documentos relevantes (ReDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia. Aunque se ha demostrado que el algoritmo ReDDE es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21]. El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar puntuaciones de documentos específicas de la base de datos en puntuaciones de documentos independientes de la base de datos comparables. El algoritmo de fusión de resultados de aprendizaje semisupervisado (SSL) [20,22] utiliza los documentos adquiridos mediante muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos y de la consulta. Estos modelos lineales se utilizan para convertir las puntuaciones de documentos específicas de la base de datos en las puntuaciones de documentos centralizadas aproximadas. El algoritmo SSL ha demostrado ser efectivo [22]. Sirve como un componente importante de nuestro marco unificado de maximización de utilidad (Sección 3). Para lograr resultados precisos en la recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos en sistemas de recomendación de bases de datos. Pero como se señaló anteriormente, un algoritmo de selección de recursos optimizado para un alto recuerdo puede no funcionar bien para la recuperación de documentos, que tiene como objetivo la alta precisión. Este tipo de inconsistencia ha sido observada en investigaciones previas [4,21]. La investigación en [21] intentó resolver el problema con un método heurístico. La investigación más similar a lo que proponemos aquí es el marco teórico de la toma de decisiones (DTF) [7,15]. Este marco de trabajo calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y se han propuesto varios métodos [15] para estimar la calidad de recuperación. Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF. Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de bases de datos y recuperación distribuida de documentos) en el mismo marco unificado. Segundo, el DTF construye un modelo para cada base de datos para calcular las probabilidades de relevancia. Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos. Por el contrario, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada. La base de datos de muestra centralizada puede servir como puente para conectar las bases de datos individuales con el modelo logístico centralizado, de esta manera se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos. Esta estrategia puede ahorrar una gran cantidad de esfuerzo en juicio humano y es una gran ventaja del marco de maximización de utilidad unificada sobre el DTF, especialmente cuando hay un gran número de bases de datos. MARCO DE MAXIMIZACIÓN DE UTILIDAD UNIFICADA El marco de Maximización de Utilidad Unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría no vistos) disponibles en el entorno de búsqueda distribuida. En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el modelo de Maximización de Utilidad Unificado. También describimos cómo el modelo puede ser optimizado para el objetivo de alto recuerdo de un sistema de recomendación de base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido. 3.1 Estimación de Probabilidades de Relevancia Como se señaló anteriormente, el propósito de la selección de recursos es el alto recuerdo y el propósito de la recuperación de documentos es la alta precisión. Para cumplir con estos objetivos diversos, el problema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos. Este es un problema difícil porque solo podemos observar una muestra de los contenidos de cada base de datos utilizando muestreo basado en consultas. Nuestra estrategia es aprovechar al máximo toda la información disponible para calcular las estimaciones de probabilidad. 3.1.1 Aprendizaje de Probabilidades de Relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se construye mediante muestreo basado en consultas y los tamaños de la base de datos se estiman utilizando el método de muestreo y remuestreo [21]. Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (Inquery [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de entrenamiento. Para cada consulta de entrenamiento, se aplica el algoritmo de selección de recursos CORI [1] para seleccionar un cierto número (por ejemplo, 10) de bases de datos y recuperar 50 identificadores de documentos de cada base de datos. El algoritmo de fusión de resultados SSL [20,22] se utiliza para combinar los resultados. Luego, podemos descargar los 50 documentos principales de la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando Inquery y las estadísticas del corpus de la base de datos de muestra centralizada. Las puntuaciones centralizadas se normalizan aún más (dividiéndolas por la puntuación centralizada máxima para cada consulta), ya que este método ha sido sugerido para mejorar la precisión de la estimación en investigaciones anteriores [15]. El juicio humano se adquiere para esos documentos y se construye un modelo logístico para transformar las puntuaciones de documentos centralizados normalizados en probabilidades de relevancia de la siguiente manera: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) donde )( _ dSc es la puntuación de documento centralizada normalizada y ac y bc son los dos parámetros del modelo logístico. Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de entrenamiento. El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de las puntuaciones de documentos centralizadas. 3.1.2 Estimación de las puntuaciones de documentos centralizadas Cuando el usuario envía una nueva consulta, se calculan las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar las puntuaciones de los documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados. Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas del tamaño de la base de datos. Definimos el factor de escala de la base de datos para la base de datos i como la razón entre el tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: SF_i = ^N_db / _N_db_samp_i donde ^N_db es el tamaño estimado de la base de datos y _N_db_samp_i es el número de documentos de la base de datos i en la base de datos de muestra centralizada. La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podríamos suponer que hay alrededor de 50 documentos en esa base de datos que tienen puntuaciones de alrededor de 0.5. De hecho, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntuación del documento centralizado para cada base de datos. Formalmente, clasificamos todos los documentos muestreados de la base de datos i-ésima por sus puntajes de documento centralizado 34 para obtener la lista de puntajes de documento centralizado muestreado {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} para la base de datos i; asumimos que si pudiéramos calcular los puntajes de documento centralizado para todos los documentos en esta base de datos y obtener la lista completa de puntajes de documento centralizado, el documento superior en la lista muestreada tendría un rango de SFdbi/2, el segundo documento en la lista muestreada tendría un rango de SFdbi3/2, y así sucesivamente. Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. La interpolación lineal por tramos se aplica para estimar la curva de puntuación del documento centralizado, como se ilustra en la Figura 1. La lista completa de puntuaciones de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documentos centralizados como: ],1[,)(S ^^ c idbij Njd ∈ . Se puede observar en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntuación del documento centralizado. Sin embargo, para bases de datos con grandes proporciones de escala de base de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos mejor clasificados (por ejemplo, [1, SFdbi/2]). Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizados de los documentos mejor clasificados para bases de datos con ratios a gran escala (por ejemplo, mayores de 100). Específicamente, se construye un modelo logístico para cada una de estas bases de datos. El modelo logístico se utiliza para estimar la puntuación del documento centralizado superior 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con las puntuaciones centralizadas más altas. 0iα , 1iα y 2iα son los parámetros del modelo logístico. Para cada consulta de entrenamiento, se descarga el documento mejor recuperado de cada base de datos y se calcula la puntuación del documento centralizado correspondiente. Junto con las puntuaciones de los dos documentos muestreados principales, estos parámetros pueden ser estimados. Después de estimar la puntuación centralizada del documento principal, se ajusta una función exponencial para la parte superior ([1, SFdbi/2]) de la curva de puntuación del documento centralizado como: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) Los dos parámetros 0iβ y 1iβ se ajustan para asegurarse de que la función exponencial pase por los dos puntos (1, ^ 1)( ic dS ) y (SFdbi/2, Sc(dsi1)). La función exponencial se utiliza únicamente para ajustar la parte superior de la curva de puntuación del documento centralizado, mientras que la parte inferior de la curva sigue siendo ajustada con el método de interpolación lineal descrito anteriormente. El ajuste mediante la función exponencial de los documentos mejor clasificados ha demostrado empíricamente producir resultados más precisos. A partir de las curvas de puntuación de documentos centralizadas, podemos estimar las listas completas de puntuación de documentos centralizados correspondientes para todas las bases de datos disponibles. Después de que las puntuaciones estimadas de los documentos centralizados se normalizan, las listas completas de probabilidades de relevancia pueden ser construidas a partir de las listas completas de puntuaciones de documentos centralizados mediante la Ecuación 1. Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El Modelo Unificado de Maximización de Utilidad En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco. En la tarea de recomendación de bases de datos, el sistema necesita decidir cómo clasificar las bases de datos. En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada. Generalizamos el proceso de selección de recomendaciones de bases de datos, que implícitamente recomienda todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos. Formalmente, denotamos di como el número de documentos que nos gustaría recuperar de la base de datos i y ,.....},{ 21 ddd = como una acción de selección para todas las bases de datos. La decisión de selección de la base de datos se toma en base a las listas completas de probabilidades de relevancia para todas las bases de datos. Las listas completas de probabilidades de relevancia se infieren a partir de toda la información disponible, específicamente sR, que representa las descripciones de recursos adquiridas mediante muestreo basado en consultas y las estimaciones del tamaño de la base de datos adquiridas mediante muestreo-resampleo; cS representa las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Si el método de estimación de puntajes de documentos centralizados y probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈. El vector aleatorio   denota un conjunto arbitrario de listas completas de probabilidades de relevancia y ),|( cs SRP θ como la probabilidad de generar este conjunto de listas. Finalmente, a cada acción de selección d y un conjunto de listas completas de la Figura 1. Construcción de la lista completa de puntuación de documentos centralizada mediante interpolación lineal (el factor de escala de la base de datos es 50). Para 35 probabilidades de relevancia θ, asociamos una función de utilidad ),( dU θ que indica el beneficio de realizar la selección d cuando las verdaderas listas completas de probabilidades de relevancia son θ. Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7). Un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en los valores de parámetros más probables en lugar de calcular toda la expectativa. En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recomendación de bases de datos. El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de N bases de datos de Nsdb) que contengan tantos documentos relevantes como sea posible, lo cual puede definirse formalmente como: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) es la función indicadora, que es 1 cuando se selecciona la i-ésima base de datos y 0 en caso contrario. Inserta esta ecuación en el modelo básico de la Ecuación 8 y asocia la restricción del número de base de datos seleccionado para obtener lo siguiente: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) La solución de este problema de optimización es muy simple. Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) Las bases de datos Nsdb con el mayor número esperado de documentos relevantes pueden ser seleccionadas para cumplir con el objetivo de alto recall. Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recuperación distribuida de documentos. Se mide mediante la Precisión en la parte superior de la lista final de documentos fusionados. Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la Precisión de los documentos recuperados de las bases de datos seleccionadas. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Tenga en cuenta que la diferencia clave entre la Ecuación 12 y la Ecuación 9 es que la Ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos, mientras que la Ecuación 12 solo considera una parte mucho más pequeña de la clasificación. Específicamente, podemos calcular la decisión de selección óptima mediante: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Diferentes tipos de restricciones causadas por las diferentes características de las tareas de recuperación de documentos pueden estar asociadas con el problema de optimización anterior. La más común es seleccionar un número fijo (Nsdb) de bases de datos y recuperar un número fijo (Nrdoc) de documentos de cada base de datos seleccionada, definido formalmente como: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) Este problema de optimización puede resolverse fácilmente calculando el número de documentos relevantes esperados en la parte superior de la lista completa de probabilidades de relevancia de cada base de datos: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Luego, las bases de datos pueden ser clasificadas por estos valores y seleccionadas. Llamamos a este algoritmo UUM/HP-FL (Maximización Unificada de Utilidad para Alta Precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada). Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada. Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes cantidades de documentos. Para simplificar, se requiere que las longitudes de la lista de resultados sean múltiplos de un número base 10. (Este valor también puede variar, pero para simplificar se establece en 10 en este documento). Esta restricción está establecida para simular el comportamiento de los motores de búsqueda comerciales en la web. (Motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 identificadores de documentos por página de resultados). Este procedimiento ahorra tiempo de cálculo al calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea de 10 en lugar de 1 (más detalles se discuten posteriormente). Para una mayor simplificación, restringimos la selección a un máximo de 100 documentos de cada base de datos (di<=100). Luego, el problema de optimización de la selección se formaliza de la siguiente manera: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc es el número total de documentos a recuperar. Desafortunadamente, no hay una solución simple para este problema de optimización como la hay para las Ecuaciones 10 y 14. Sin embargo, se puede aplicar un algoritmo de programación dinámica de 36 para calcular la solución óptima. Los pasos básicos de este método de programación dinámica se describen en la Figura 2. Dado que este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se le llama algoritmo UUM/HP-VL. Después de que se toman las decisiones de selección, se buscan las bases de datos seleccionadas y se recuperan los identificadores de documentos correspondientes de cada base de datos. El paso final de la recuperación de documentos es fusionar los resultados devueltos en una única lista clasificada con el algoritmo de aprendizaje semisupervisado. Se señaló anteriormente que el algoritmo SSL mapea las puntuaciones específicas de la base de datos en las puntuaciones de documentos centralizadas y construye la lista clasificada final en consecuencia, lo cual es consistente con todos nuestros procedimientos de selección donde se seleccionan los documentos con mayores probabilidades de relevancia (y por ende, puntuaciones de documentos centralizadas más altas). 4. METODOLOGÍA EXPERIMENTAL 4.1 Bancos de pruebas Es deseable evaluar algoritmos de recuperación de información distribuida con bancos de pruebas que simulen de cerca las aplicaciones del mundo real. Las colecciones web TREC WT2g o WT10g proporcionan una forma de dividir los documentos por diferentes servidores web. De esta manera, se podrían crear un gran número (O(1000)) de bases de datos con contenidos bastante diversos, lo que podría convertir a este banco de pruebas en un buen candidato para simular entornos operativos como la web oculta de dominio abierto. Sin embargo, dos debilidades de este banco de pruebas son: i) Cada base de datos contiene solo una pequeña cantidad de documentos (259 documentos en promedio para WT2g) [4]; y ii) El contenido de WT2g o WT10g se extrae arbitrariamente de la web. No es probable que una base de datos web oculta proporcione páginas personales o páginas web que indiquen que las páginas están en construcción y no contengan información útil en absoluto. Estos tipos de páginas web están contenidos en los conjuntos de datos WT2g/WT10g. Por lo tanto, los datos ruidosos de la Web no son similares a los contenidos de alta calidad de las bases de datos ocultas de la Web, que generalmente están organizados por expertos en el dominio. Otra opción es los datos de noticias/gobierno de TREC [1,15,17,18,21]. Los datos gubernamentales/noticias de TREC se centran en temas relativamente específicos. Comparado con los datos web de TREC: i) Los documentos de noticias/gobierno son mucho más similares a los contenidos proporcionados por una base de datos orientada a temas que a una página web arbitraria, ii) Una base de datos en este banco de pruebas es más grande que la de los datos web de TREC. En promedio, una base de datos contiene miles de documentos, lo cual es más realista que una base de datos de datos web de TREC con alrededor de 250 documentos. Dado que los contenidos y tamaños de las bases de datos en el banco de pruebas de noticias/gobierno de TREC son más similares a los de una base de datos orientada a temas, es un buen candidato para simular los entornos de recuperación de información distribuida de grandes organizaciones (empresas) o sitios web ocultos específicos de dominio, como West, que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3]. Dado que la mayoría de los sistemas actuales de recuperación de información distribuida están desarrollados para entornos de grandes organizaciones (empresas) o para la Web oculta de dominios específicos en lugar de la Web oculta de dominio abierto, en este trabajo se eligió el banco de pruebas de noticias/gobierno de TREC. El banco de pruebas Trec123-100col-bysource es uno de los más utilizados en las pruebas de noticias y gobierno de TREC [1,15,17,21]. Fue elegido en este trabajo. Tres bancos de pruebas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes también se utilizaron para proporcionar una simulación más exhaustiva para entornos reales. Se crearon 100 bases de datos a partir de los CDs de TREC 1, 2 y 3. Fueron organizados por fuente y fecha de publicación [1]. Los tamaños de las bases de datos no están sesgados. Los detalles se encuentran en la Tabla 1. Tres bancos de pruebas construidos en [21] se basaron en el banco de pruebas trec123-100colbysource. Cada banco de pruebas contiene muchas bases de datos pequeñas y dos bases de datos grandes creadas al fusionar alrededor de 10 a 20 bases de datos pequeñas. Listas completas de probabilidades de relevancia para todas las bases de datos |DB|. Solución de selección óptima para la Ecuación 16. i) Crear el arreglo tridimensional: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Cada Sel (x, y, z) está asociado con una decisión de selección xyzd, que representa la mejor decisión de selección en la condición: solo se consideran bases de datos del número 1 al número x para la selección; se recuperarán un total de y*10 documentos; solo se seleccionan z bases de datos de los candidatos de la base de datos x. Y Sel (x, y, z) es el valor de utilidad correspondiente al elegir la mejor selección. ii) Inicializar Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) solo con la información de relevancia estimada de la 1ª base de datos. iii) Iterar el candidato actual de la base de datos i desde 2 hasta |DB| Para cada entrada Sel (i, y, z): Encontrar k tal que: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k∗ documentos de la base de datos i-ésima, de lo contrario no debemos seleccionar esta base de datos y se debe mantener la solución anterior mejor Sel (i-1, y, z). Luego establezca el valor de iyzd y Sel (i, y, z) en consecuencia. iv) La mejor solución de selección se da por _ /10| | Toral rdoc sdbDB N Nd y el valor de utilidad correspondiente es Sel (|DB|, NTotal_rdoc/10, Nsdb). Figura 2. El procedimiento de optimización de programación dinámica para la Ecuación 16. Tabla 1: Estadísticas del banco de pruebas. Número de documentos Tamaño (MB) Tamaño del banco de pruebas (GB) Mínimo Promedio Máximo Mínimo Promedio Máximo Trec123 3.2 752 10782 39713 28 32 42 Tabla 2: Estadísticas del conjunto de consultas. Nombre del conjunto de temas TREC Campo del tema TREC Longitud promedio (palabras) Trec123 51-150 Título 3.1 37 Trec123-2ldb-60col (representativo): Las bases de datos en el trec123-100col-bysource se ordenaron en orden alfabético. Dos grandes bases de datos fueron creadas al fusionar 20 bases de datos pequeñas con el método de round-robin. Por lo tanto, las dos bases de datos grandes tienen más documentos relevantes debido a sus tamaños grandes, aunque las densidades de documentos relevantes son aproximadamente iguales a las de las bases de datos pequeñas. Las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, APall y WSJall. Las otras 60 colecciones quedaron sin cambios. Las bases de datos APall y WSJall tienen una mayor densidad de documentos relevantes para las consultas de TREC que las bases de datos pequeñas. Por lo tanto, las dos bases de datos grandes tienen muchos más documentos relevantes que las bases de datos pequeñas. Las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, FRall y DOEall. Las otras 80 colecciones quedaron sin cambios. Las bases de datos FRall y DOEall tienen densidades más bajas de documentos relevantes para las consultas de TREC que las bases de datos pequeñas, a pesar de ser mucho más grandes. Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150. Las consultas 101-150 se utilizaron como consultas de entrenamiento y las consultas 51-100 se utilizaron como consultas de prueba (detalles en la Tabla 2). 4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o en la Web oculta específica de dominio, diferentes bases de datos pueden utilizar diferentes tipos de motores de búsqueda. Para simular el entorno de múltiples motores de búsqueda, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: INQUERY [2], un modelo de lenguaje estadístico de unigrama con suavizado lineal [12,20] y un algoritmo de recuperación TFIDF con peso ltc [12,20]. Todos estos algoritmos fueron implementados con la herramienta Lemur [12]. Estos tres tipos de motores de búsqueda fueron asignados a las bases de datos entre los cuatro bancos de pruebas de manera round-robin. 5. RESULTADOS: SELECCIÓN DE RECURSOS DE LA RECOMENDACIÓN DE BASES DE DATOS Todos los cuatro bancos de pruebas descritos en la Sección 4 fueron utilizados en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de bases de datos. Las descripciones de los recursos fueron creadas utilizando muestreo basado en consultas. Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos. Las estadísticas del tamaño de la base de datos fueron estimadas mediante el método de muestra y remuestra [21]. Cincuenta consultas (101-150) se utilizaron como consultas de entrenamiento para construir el modelo logístico relevante y ajustar las funciones exponenciales de las curvas de puntuación de documentos centralizados para bases de datos de gran proporción (detalles en la Sección 3.1). Otros 50 consultas (51-100) se utilizaron como datos de prueba. Los algoritmos de selección de recursos de los sistemas de recomendación de bases de datos suelen compararse utilizando la métrica de recuperación nR [1,17,18,21]. Que B denote una clasificación base, que a menudo es la RBR (clasificación basada en relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos. Y que Bi y Ei denoten el número de documentos relevantes en la base de datos clasificada i-ésima de B o E. Entonces, Rn se define de la siguiente manera: = = = k i i k i i k B E R 1 1 (17) Por lo general, el objetivo es buscar solo algunas bases de datos, por lo que nuestras cifras solo muestran resultados para la selección de hasta 20 bases de datos. Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, CORI, ReDDE y UUM/HR. El algoritmo UUM/HR se describe en la Sección 3.3. Se puede observar en la Figura 3 que los algoritmos ReDDE y UUM/HR son más efectivos (en los conjuntos de pruebas representativos, relevantes y no relevantes) o igual de efectivos (en el conjunto de pruebas Trec123-100Col) que el algoritmo de selección de recursos CORI. El algoritmo UUM/HR es más efectivo que el algoritmo ReDDE en los conjuntos de pruebas representativos y relevantes y es aproximadamente igual que el algoritmo ReDDE en los conjuntos de pruebas Trec123100Col y no relevantes. Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo ReDDE. Se puede observar que al seleccionar solo algunas bases de datos en el Trec123-100Col o en los conjuntos de pruebas no relevantes, el algoritmo ReDEE tiene una pequeña ventaja sobre el algoritmo UUM/HR. Atribuimos esto a dos causas: i) El algoritmo ReDDE fue ajustado en el banco de pruebas Trec123-100Col; y ii) Aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico para estimar probabilidades de relevancia no es lo suficientemente preciso. Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este pequeño rompecabezas. Colecciones seleccionadas. Colecciones seleccionadas. Plataforma de pruebas Trec123-100Col. Plataforma de pruebas representativa. Colección seleccionada. Colección seleccionada. Plataforma de pruebas relevante. Plataforma de pruebas no relevante. Figura 3. Experimentos de selección de recursos en los cuatro bancos de pruebas. 38 6. RESULTADOS: EFECTIVIDAD DE LA RECUPERACIÓN DE DOCUMENTOS Para la recuperación de documentos, se buscan en las bases de datos seleccionadas y los resultados devueltos se fusionan en una lista final única. En todos los experimentos discutidos en esta sección, los resultados obtenidos de bases de datos individuales fueron combinados por el algoritmo de fusión de resultados de aprendizaje semisupervisado. Esta versión del algoritmo SSL [22] tiene permitido descargar un pequeño número de textos de documentos devueltos sobre la marcha para crear datos de entrenamiento adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicos de la base de datos en puntuaciones de documentos centralizadas estimadas. Se ha demostrado ser muy efectivo en entornos donde solo se obtienen listas de resultados cortas de cada base de datos seleccionada [22]. Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos. La efectividad de la recuperación de documentos se midió mediante la Precisión en la parte superior de la lista final de documentos. Los experimentos en esta sección se llevaron a cabo para estudiar la efectividad de recuperación de documentos de cinco algoritmos de selección, a saber, los algoritmos CORI, ReDDE, UUM/HR, UUM/HP-FL y UUM/HP-VL. Los últimos tres algoritmos fueron propuestos en la Sección 3. Todos los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada. El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos a recuperar de cada base de datos seleccionada; el número recuperado estaba limitado a ser de 10 a 100, y un múltiplo de 10. El Trec123-100Col y los bancos de pruebas representativos fueron seleccionados para la recuperación de documentos, ya que representan dos casos extremos de efectividad en la selección de recursos; en un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante Tabla 5. Precisión en el banco de pruebas representativo cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 documentos 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 documentos 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 documentos 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 documentos 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Tabla 6. Precisión en el banco de pruebas representativo cuando se seleccionaron 5 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 documentos 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 documentos 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 documentos 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 documentos 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Tabla 3. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 documentos 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 documentos 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 documentos 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 documentos 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Tabla 4. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 5 bases de datos. (El primer punto de referencia es CORI; el segundo punto de referencia para los métodos UUM/HP es UUM/HR). La precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 documentos 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 documentos 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 documentos 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 documentos 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 mucho peor que los otros algoritmos. Las Tablas 3 y 4 muestran los resultados en el banco de pruebas Trec123-100Col, y las Tablas 5 y 6 muestran los resultados en el banco de pruebas representativo. En el banco de pruebas Trec123-100Col, la efectividad de recuperación de documentos del algoritmo de selección CORI es aproximadamente la misma o un poco mejor que el algoritmo ReDDE, pero ambos son peores que los otros tres algoritmos (Tablas 3 y 4). El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos CORI y ReDDE. Una de las principales diferencias entre el algoritmo UUM/HR y el algoritmo ReDDE fue señalada anteriormente: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntuación de documentos centralizadas, mientras que el algoritmo ReDDE [21] utiliza un método heurístico, asume que las curvas de puntuación de documentos centralizadas son funciones escalonadas y no hace distinción entre la parte superior de las curvas. Esta diferencia hace que UUM/HR sea mejor que el algoritmo ReDDE para distinguir documentos con altas probabilidades de relevancia de aquéllos con bajas probabilidades de relevancia. Por lo tanto, el UUM/HR refleja mejor el objetivo de recuperación de alta precisión que el algoritmo ReDDE y, por lo tanto, es más efectivo para la recuperación de documentos. El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, como lo hacen los algoritmos UUM/HP-FL y UUM/HP-VL. Se puede observar que en este banco de pruebas, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos. Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de recuperación de documentos en las Ecuaciones 14 y 16. En el banco de pruebas representativo, CORI es mucho menos efectivo que otros algoritmos para la recuperación distribuida de documentos (Tablas 5 y 6). Los resultados de recuperación de documentos del algoritmo ReDDE son mejores que los del algoritmo CORI pero aún peores que los resultados del algoritmo UUM/HR. En este banco de pruebas, los tres algoritmos de UUM son aproximadamente igual de efectivos. Un análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho mayor que los experimentos en el banco de pruebas Trec123-100Col, ya que todos tienden a seleccionar las dos bases de datos grandes. Esto explica por qué son igualmente efectivos para la recuperación de documentos. En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos y reportar solo listas clasificadas de resultados. Dado que el modelo unificado de maximización de utilidad solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos. El único ajuste es que el algoritmo SSL fusiona listas clasificadas sin puntuaciones de documentos asignando a los documentos puntuaciones de pseudo-documentos normalizadas por sus rangos (En una lista clasificada de 50 documentos, el primero tiene una puntuación de 1, el segundo tiene una puntuación de 0.98, etc.), lo cual ha sido estudiado en [22]. Los resultados del experimento en el banco de pruebas trec123-100Col-bysource con 3 bases de datos seleccionadas se muestran en la Tabla 7. La configuración del experimento fue la misma que antes, excepto que las puntuaciones de los documentos fueron eliminadas intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de identificadores de documentos. Se puede observar en los resultados que el UUM/HP-FL y el UUM/HP-VL funcionan bien con bases de datos que no devuelven puntuaciones de documentos y siguen siendo más efectivos que otras alternativas. Otros experimentos con bases de datos que no devuelven puntuaciones de documentos no se informan, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL. Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión de manera explícita en la recuperación de documentos. Los nuevos algoritmos basados en este principio logran resultados mejores o al menos tan buenos como los algoritmos previos de vanguardia en varios entornos. CONCLUSIÓN La recuperación distribuida de información resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e Internet. La mayoría de investigaciones previas utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de bases de datos para la aplicación de recuperación de documentos distribuidos. Sostenemos que el objetivo de alta recuperación de recursos en la recomendación de bases de datos y el objetivo de alta precisión en la recuperación de documentos están relacionados pero no son idénticos. Este tipo de inconsistencia también ha sido observada en trabajos anteriores, pero las soluciones previas utilizaron métodos heurísticos o asumieron la cooperación de bases de datos individuales (por ejemplo, que todas las bases de datos utilizaran el mismo tipo de motores de búsqueda), lo cual frecuentemente no es cierto en un entorno no cooperativo. En este trabajo proponemos un modelo unificado de maximización de utilidad para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado. En este marco, las decisiones de selección se obtienen optimizando diferentes funciones objetivo. Hasta donde sabemos, este es el primer trabajo que intenta visualizar y modelar teóricamente la tarea de recuperación de información distribuida de manera integrada. El nuevo marco continúa una tendencia reciente de investigación que estudia el uso de muestreo basado en consultas y una base de datos de muestras centralizada. Se entrenó un único modelo logístico en la Tabla 7 centralizada. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). (Los motores de búsqueda no devuelven puntajes de documentos) Precisión en la Clasificación de Documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 documentos 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 documentos 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 documentos 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 documentos 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 base de datos de muestra para estimar las probabilidades de relevancia de documentos por sus puntajes de recuperación centralizados, mientras que la base de datos de muestra centralizada sirve como puente para conectar las bases de datos individuales con el modelo logístico centralizado. Por lo tanto, las probabilidades de relevancia para todos los documentos en las bases de datos pueden ser estimadas con una cantidad muy pequeña de juicio de relevancia humano, lo cual es mucho más eficiente que los métodos anteriores que construyen un modelo separado para cada base de datos. Este marco no solo es más sólido teóricamente, sino también muy efectivo. Un algoritmo para la selección de recursos (UUM/HR) y dos algoritmos para la recuperación de documentos (UUM/HP-FL y UUM/HP-VL) se derivan de este marco. Se han realizado estudios empíricos en bancos de pruebas para simular las soluciones de búsqueda distribuida de grandes organizaciones (empresas) o la Web oculta específica de un dominio. Además, los algoritmos de selección de recursos UUM/HP-FL y UUM/HP-VL se amplían con una variante del algoritmo de fusión de resultados SSL para abordar la tarea de recuperación de documentos distribuidos cuando las bases de datos seleccionadas no devuelven puntuaciones de documentos. Los experimentos han demostrado que estos algoritmos logran resultados que son al menos tan buenos como el estado del arte previo, y a veces considerablemente mejores. Un análisis detallado indica que la ventaja de estos algoritmos proviene de optimizar explícitamente los objetivos de las tareas específicas. El marco unificado de maximización de utilidad está abierto a diferentes extensiones. Cuando el costo está asociado con la búsqueda en las bases de datos en línea, el marco de utilidad puede ajustarse para estimar automáticamente el mejor número de bases de datos a buscar, de modo que se puedan recuperar una gran cantidad de documentos relevantes con costos relativamente bajos. Otra extensión del marco es considerar la efectividad de la recuperación de información de las bases de datos en línea, lo cual es un tema importante en los entornos operativos. Todas estas son las direcciones de la investigación futura. AGRADECIMIENTO Esta investigación fue apoyada por las subvenciones de la NSF EIA-9983253 y IIS-0118767. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este documento son del autor y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] J. Callan. (2000). Recuperación de información distribuida. En W.B. Croft, editor, Avances en Recuperación de Información. Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. \n\nEditorial Kluwer Academic. (pp. 127-150). [2] J. Callan, W.B. Croft, y J. Broglio. (1995). Experimentos TREC y TIPSTER con INQUERY. Procesamiento y Gestión de la Información, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson y M. Meziou. (2002). Selección de base de datos utilizando recursos de colección lógica adquiridos y físicos reales en un entorno operativo masivo específico de dominio. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [4] N. Craswell. (2000). Métodos para la recuperación distribuida de información. I'm sorry, but the sentence \"Ph.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate to Spanish? Tesis doctoral, Universidad Nacional Australiana. [5] N. Craswell, D. Hawking y P. Thistlewaite. (1999). Combinando resultados de motores de búsqueda aislados. En Actas de la 10ª Conferencia de Bases de Datos Australasiana. [6] D. DSouza, J. Thom y J. Zobel. (2000). Una comparación de técnicas para seleccionar colecciones de texto. En Actas de la 11ª Conferencia de Bases de Datos Australasiana. [7] N. Fuhr. (1999). Un enfoque de Teoría de la Decisión para la selección de bases de datos en IR en red. ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina y A. Paepcke. (1997). Propuesta de Stanford para la metabusqueda en internet. En Actas de la 20ª Conferencia Internacional ACM-SIGMOD sobre Gestión de Datos. [9] L. Gravano, P. Ipeirotis y M. Sahami. (2003). QProber: Un sistema para la clasificación automática de bases de datos de la web oculta. ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis y L. Gravano. (2002). Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] El kit de herramientas lemur. http://www.cs.cmu.edu/~lemur [13] J. Lu y J. Callan. (2003). Recuperación de información basada en contenido en redes peer-to-peer. En Actas de la 12ª Conferencia Internacional sobre Información y Gestión del Conocimiento. [14] W. Meng, C.T. Yu y K.L. Liu. (2002) Construcción de motores de búsqueda eficientes y efectivos. ACM Comput. Surv. 34(1). [15] H. Nottelmann y N. Fuhr. (2003). Evaluando diferentes métodos para estimar la calidad de recuperación para la selección de recursos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [16] H., Nottelmann y N., Fuhr. (2003). La arquitectura MIND para bibliotecas digitales federadas de multimedia heterogénea. Taller ACM SIGIR 2003 sobre Recuperación de Información Distribuida. [17] A.L. Powell, J.C. French, J. Callan, M. Connell y C.L. Viles. (2000). \n\nViles. (2000). El impacto de la selección de bases de datos en la búsqueda distribuida. En Actas de la 23ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [18] A.L. Powell y J.C. French. (2003). Comparando el rendimiento de los algoritmos de selección de bases de datos. ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). \n\nACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). Busca en la web invisible. Guardian Unlimited. [20] L. Si y J. Callan. (2002). Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [21] L. Si y J. Callan. (2003). Método de estimación de distribución de documentos relevantes para la selección de recursos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [22] L. Si y J. Callan. (2003). Un método de aprendizaje semi-supervisado para fusionar los resultados de un motor de búsqueda. ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41\n\nACM Transactions on Information Systems, 21(4). (pp. 457-491). 41 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "resource selection": {
            "translated_key": "selección de recursos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for <br>resource selection</br> Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for <br>resource selection</br> of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, <br>resource selection</br> can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (<br>resource selection</br>) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the <br>resource selection</br> components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective <br>resource selection</br> algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the <br>resource selection</br> problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of <br>resource selection</br> for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For <br>resource selection</br> for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the <br>resource selection</br> problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for <br>resource selection</br> and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many <br>resource selection</br> algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) <br>resource selection</br> algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use <br>resource selection</br> algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good <br>resource selection</br> algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of <br>resource selection</br> is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI <br>resource selection</br> algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the <br>resource selection</br> problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 <br>resource selection</br> for High-Recall High-recall is the goal of the <br>resource selection</br> algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 <br>resource selection</br> for High-Precision High-Precision is the goal of <br>resource selection</br> algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: <br>resource selection</br> OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the <br>resource selection</br> effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "<br>resource selection</br> algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a <br>resource selection</br> algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three <br>resource selection</br> algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI <br>resource selection</br> algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "<br>resource selection</br> experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of <br>resource selection</br> effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective <br>resource selection</br> algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall <br>resource selection</br> goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the <br>resource selection</br> of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for <br>resource selection</br> (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL <br>resource selection</br> algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for <br>resource selection</br>.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for <br>resource selection</br>.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "Unified Utility Maximization Framework for <br>resource selection</br> Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for <br>resource selection</br> of distributed text information retrieval.",
                "With the estimated relevance information, <br>resource selection</br> can be made by explicitly optimizing the goals of different applications.",
                "Second, given a query, a set of resources must be selected to do the search (<br>resource selection</br>) [5,7,21].",
                "It is composed of the resource description and the <br>resource selection</br> components."
            ],
            "translated_annotated_samples": [
                "Marco unificado de maximización de utilidad para la <br>selección de recursos</br> en el Instituto de Tecnología del Lenguaje Luo Si.",
                "Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la <br>selección de recursos</br> de recuperación de información textual distribuida.",
                "Con la información de relevancia estimada, la <br>selección de recursos</br> puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones.",
                "Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (<br>selección de recursos</br>) [5,7,21].",
                "Está compuesto por la descripción del recurso y los componentes de <br>selección de recursos</br>."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la <br>selección de recursos</br> en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la <br>selección de recursos</br> de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la <br>selección de recursos</br> puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (<br>selección de recursos</br>) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de <br>selección de recursos</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "retrieval and result merging": {
            "translated_key": "recuperación y fusión de resultados",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "database recommendation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for <br>database recommendation</br>, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A <br>database recommendation</br> system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the <br>database recommendation</br> system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a <br>database recommendation</br> system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of <br>database recommendation</br> system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both <br>database recommendation</br> and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a <br>database recommendation</br> system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of <br>database recommendation</br> system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (<br>database recommendation</br> and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a <br>database recommendation</br> system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (<br>database recommendation</br>) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of <br>database recommendation</br>, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the <br>database recommendation</br> selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the <br>database recommendation</br> system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as <br>database recommendation</br>.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF <br>database recommendation</br> All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the <br>database recommendation</br> system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of <br>database recommendation</br> systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of <br>database recommendation</br> system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of <br>database recommendation</br> and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of <br>database recommendation</br> and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "Specifically, when used for <br>database recommendation</br>, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "A <br>database recommendation</br> system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It selects relevant information sources for users queries as the <br>database recommendation</br> system does.",
                "The goal of a <br>database recommendation</br> system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "However, most previous solutions simply use effective resource selection algorithm of <br>database recommendation</br> system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21]."
            ],
            "translated_annotated_samples": [
                "Específicamente, cuando se utiliza para la <br>recomendación de bases de datos</br>, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos).",
                "Un sistema de <br>recomendación de bases de datos</br> va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios.",
                "Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de <br>recomendación de la base de datos</br>.",
                "El objetivo de un sistema de <br>recomendación de bases de datos</br> es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo.",
                "Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del <br>sistema de recomendación de bases de datos</br> para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la <br>recomendación de bases de datos</br>, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de <br>recomendación de bases de datos</br> va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La recuperación distribuida de documentos es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de <br>recomendación de la base de datos</br>. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de <br>recomendación de bases de datos</br> es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del <br>sistema de recomendación de bases de datos</br> para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. ",
            "candidates": [],
            "error": [
                [
                    "recomendación de bases de datos",
                    "recomendación de bases de datos",
                    "recomendación de la base de datos",
                    "recomendación de bases de datos",
                    "sistema de recomendación de bases de datos"
                ]
            ]
        },
        "distributed document retrieval": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for <br>distributed document retrieval</br>, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "<br>distributed document retrieval</br> is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of <br>distributed document retrieval</br> is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for <br>distributed document retrieval</br> system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and <br>distributed document retrieval</br> together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a <br>distributed document retrieval</br> system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of <br>distributed document retrieval</br> application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and <br>distributed document retrieval</br>) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a <br>distributed document retrieval</br> system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (<br>distributed document retrieval</br>) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as <br>distributed document retrieval</br>.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for <br>distributed document retrieval</br> (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for <br>distributed document retrieval</br> application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the <br>distributed document retrieval</br> task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for <br>distributed document retrieval</br>, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "<br>distributed document retrieval</br> is a more sophisticated task.",
                "On the other side, the effectiveness of <br>distributed document retrieval</br> is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for <br>distributed document retrieval</br> system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and <br>distributed document retrieval</br> together by treating them as different optimization goals."
            ],
            "translated_annotated_samples": [
                "Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la <br>recuperación distribuida de documentos</br>, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos).",
                "La <br>recuperación distribuida de documentos</br> es una tarea más sofisticada.",
                "Por otro lado, la efectividad de la <br>recuperación distribuida de documentos</br> suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión.",
                "Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de <br>recuperación de documentos distribuido</br> o resuelven la inconsistencia con métodos heurísticos [1,4,21].",
                "Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de <br>recuperación de documentos distribuidos</br>, tratándolos como objetivos de optimización diferentes."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la <br>recuperación distribuida de documentos</br>, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La <br>recuperación distribuida de documentos</br> es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la <br>recuperación distribuida de documentos</br> suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de <br>recuperación de documentos distribuido</br> o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de <br>recuperación de documentos distribuidos</br>, tratándolos como objetivos de optimización diferentes. ",
            "candidates": [],
            "error": [
                [
                    "recuperación distribuida de documentos",
                    "recuperación distribuida de documentos",
                    "recuperación distribuida de documentos",
                    "recuperación de documentos distribuido",
                    "recuperación de documentos distribuidos"
                ]
            ]
        },
        "logistic transformation model": {
            "translated_key": "modelo de transformación logística",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A <br>logistic transformation model</br> is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "A <br>logistic transformation model</br> is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance."
            ],
            "translated_annotated_samples": [
                "Un <br>modelo de transformación logística</br> se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La recuperación distribuida de documentos es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de recuperación de documentos distribuidos, tratándolos como objetivos de optimización diferentes. Primero, se construye una base de datos de muestra centralizada mediante el muestreo aleatorio de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas; también se estiman las estadísticas del tamaño de la base de datos. Un <br>modelo de transformación logística</br> se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia. Segundo, después de que se envía una nueva consulta, la consulta se puede utilizar para buscar en la base de datos de muestras centralizada que produce una puntuación para cada documento muestreado. La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada puede estimarse aplicando el modelo logístico al puntaje de cada documento. Entonces, las probabilidades de relevancia de todos los documentos (en su mayoría no vistos) entre las bases de datos disponibles pueden ser estimadas utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos. Para la tarea de selección de recursos para un sistema de recomendación de bases de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alto recall. Para la selección de recursos para un sistema distribuido de recuperación de documentos, se prefieren las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia. Este criterio de selección cumple con el objetivo de alta precisión de la aplicación de recuperación de documentos distribuidos. Además, se aplica el algoritmo de aprendizaje semisupervisado (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada. El marco de utilidad unificado hace muy pocas suposiciones y funciona en entornos no cooperativos. Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos correspondientes; y ii) Muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos. Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades correspondientes de relevancia y utiliza la base de datos de muestra centralizada como puente entre las bases de datos individuales y el modelo logístico. El esfuerzo humano (juicio de relevancia) necesario para entrenar el modelo logístico centralizado único no aumenta con el número de bases de datos. Esta es una gran ventaja sobre investigaciones anteriores, las cuales requerían que la cantidad de esfuerzo humano fuera lineal con el número de bases de datos [7,15]. El marco de utilidad unificada no solo es más sólido teóricamente, sino también muy efectivo. Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de vanguardia en una variedad de configuraciones. La siguiente sección discute el trabajo relacionado. La sección 3 describe el nuevo modelo unificado de maximización de utilidad. La sección 4 explica nuestra metodología experimental. Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos. La sección 7 concluye. 2. Investigación previa Ha habido una considerable investigación sobre todos los subproblemas de la recuperación de información distribuida. Exploramos los trabajos más relacionados en esta sección. El primer problema de la recuperación de información distribuida es la representación de recursos. El protocolo STARTS es una solución para adquirir descripciones de recursos en entornos cooperativos [8]. Sin embargo, en entornos no cooperativos, aunque las bases de datos estén dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no. Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí. Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en consultas, que genera y envía consultas de forma aleatoria a motores de búsqueda individuales y recupera algunos documentos para construir las descripciones. Dado que los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico. Los experimentos han demostrado que descripciones de recursos bastante precisas pueden ser construidas enviando alrededor de 80 consultas y descargando alrededor de 300 documentos [1]. Muchos algoritmos de selección de recursos como gGlOSS/vGlOSS [8] y CORI [1] han sido propuestos en la última década. El algoritmo CORI representa cada base de datos por sus términos, las frecuencias de los documentos y un pequeño número de estadísticas del corpus (detalles en [1]). Como investigaciones previas en diferentes conjuntos de datos han demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo utilizamos como algoritmo base en este trabajo. El algoritmo de selección de recursos de estimación de distribución de documentos relevantes (ReDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia. Aunque se ha demostrado que el algoritmo ReDDE es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21]. El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar puntuaciones de documentos específicas de la base de datos en puntuaciones de documentos independientes de la base de datos comparables. El algoritmo de fusión de resultados de aprendizaje semisupervisado (SSL) [20,22] utiliza los documentos adquiridos mediante muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos y de la consulta. Estos modelos lineales se utilizan para convertir las puntuaciones de documentos específicas de la base de datos en las puntuaciones de documentos centralizadas aproximadas. El algoritmo SSL ha demostrado ser efectivo [22]. Sirve como un componente importante de nuestro marco unificado de maximización de utilidad (Sección 3). Para lograr resultados precisos en la recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos en sistemas de recomendación de bases de datos. Pero como se señaló anteriormente, un algoritmo de selección de recursos optimizado para un alto recuerdo puede no funcionar bien para la recuperación de documentos, que tiene como objetivo la alta precisión. Este tipo de inconsistencia ha sido observada en investigaciones previas [4,21]. La investigación en [21] intentó resolver el problema con un método heurístico. La investigación más similar a lo que proponemos aquí es el marco teórico de la toma de decisiones (DTF) [7,15]. Este marco de trabajo calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y se han propuesto varios métodos [15] para estimar la calidad de recuperación. Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF. Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de bases de datos y recuperación distribuida de documentos) en el mismo marco unificado. Segundo, el DTF construye un modelo para cada base de datos para calcular las probabilidades de relevancia. Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos. Por el contrario, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada. La base de datos de muestra centralizada puede servir como puente para conectar las bases de datos individuales con el modelo logístico centralizado, de esta manera se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos. Esta estrategia puede ahorrar una gran cantidad de esfuerzo en juicio humano y es una gran ventaja del marco de maximización de utilidad unificada sobre el DTF, especialmente cuando hay un gran número de bases de datos. MARCO DE MAXIMIZACIÓN DE UTILIDAD UNIFICADA El marco de Maximización de Utilidad Unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría no vistos) disponibles en el entorno de búsqueda distribuida. En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el modelo de Maximización de Utilidad Unificado. También describimos cómo el modelo puede ser optimizado para el objetivo de alto recuerdo de un sistema de recomendación de base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido. 3.1 Estimación de Probabilidades de Relevancia Como se señaló anteriormente, el propósito de la selección de recursos es el alto recuerdo y el propósito de la recuperación de documentos es la alta precisión. Para cumplir con estos objetivos diversos, el problema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos. Este es un problema difícil porque solo podemos observar una muestra de los contenidos de cada base de datos utilizando muestreo basado en consultas. Nuestra estrategia es aprovechar al máximo toda la información disponible para calcular las estimaciones de probabilidad. 3.1.1 Aprendizaje de Probabilidades de Relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se construye mediante muestreo basado en consultas y los tamaños de la base de datos se estiman utilizando el método de muestreo y remuestreo [21]. Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (Inquery [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de entrenamiento. Para cada consulta de entrenamiento, se aplica el algoritmo de selección de recursos CORI [1] para seleccionar un cierto número (por ejemplo, 10) de bases de datos y recuperar 50 identificadores de documentos de cada base de datos. El algoritmo de fusión de resultados SSL [20,22] se utiliza para combinar los resultados. Luego, podemos descargar los 50 documentos principales de la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando Inquery y las estadísticas del corpus de la base de datos de muestra centralizada. Las puntuaciones centralizadas se normalizan aún más (dividiéndolas por la puntuación centralizada máxima para cada consulta), ya que este método ha sido sugerido para mejorar la precisión de la estimación en investigaciones anteriores [15]. El juicio humano se adquiere para esos documentos y se construye un modelo logístico para transformar las puntuaciones de documentos centralizados normalizados en probabilidades de relevancia de la siguiente manera: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) donde )( _ dSc es la puntuación de documento centralizada normalizada y ac y bc son los dos parámetros del modelo logístico. Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de entrenamiento. El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de las puntuaciones de documentos centralizadas. 3.1.2 Estimación de las puntuaciones de documentos centralizadas Cuando el usuario envía una nueva consulta, se calculan las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar las puntuaciones de los documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados. Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas del tamaño de la base de datos. Definimos el factor de escala de la base de datos para la base de datos i como la razón entre el tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: SF_i = ^N_db / _N_db_samp_i donde ^N_db es el tamaño estimado de la base de datos y _N_db_samp_i es el número de documentos de la base de datos i en la base de datos de muestra centralizada. La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podríamos suponer que hay alrededor de 50 documentos en esa base de datos que tienen puntuaciones de alrededor de 0.5. De hecho, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntuación del documento centralizado para cada base de datos. Formalmente, clasificamos todos los documentos muestreados de la base de datos i-ésima por sus puntajes de documento centralizado 34 para obtener la lista de puntajes de documento centralizado muestreado {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} para la base de datos i; asumimos que si pudiéramos calcular los puntajes de documento centralizado para todos los documentos en esta base de datos y obtener la lista completa de puntajes de documento centralizado, el documento superior en la lista muestreada tendría un rango de SFdbi/2, el segundo documento en la lista muestreada tendría un rango de SFdbi3/2, y así sucesivamente. Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. La interpolación lineal por tramos se aplica para estimar la curva de puntuación del documento centralizado, como se ilustra en la Figura 1. La lista completa de puntuaciones de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documentos centralizados como: ],1[,)(S ^^ c idbij Njd ∈ . Se puede observar en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntuación del documento centralizado. Sin embargo, para bases de datos con grandes proporciones de escala de base de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos mejor clasificados (por ejemplo, [1, SFdbi/2]). Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizados de los documentos mejor clasificados para bases de datos con ratios a gran escala (por ejemplo, mayores de 100). Específicamente, se construye un modelo logístico para cada una de estas bases de datos. El modelo logístico se utiliza para estimar la puntuación del documento centralizado superior 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con las puntuaciones centralizadas más altas. 0iα , 1iα y 2iα son los parámetros del modelo logístico. Para cada consulta de entrenamiento, se descarga el documento mejor recuperado de cada base de datos y se calcula la puntuación del documento centralizado correspondiente. Junto con las puntuaciones de los dos documentos muestreados principales, estos parámetros pueden ser estimados. Después de estimar la puntuación centralizada del documento principal, se ajusta una función exponencial para la parte superior ([1, SFdbi/2]) de la curva de puntuación del documento centralizado como: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) Los dos parámetros 0iβ y 1iβ se ajustan para asegurarse de que la función exponencial pase por los dos puntos (1, ^ 1)( ic dS ) y (SFdbi/2, Sc(dsi1)). La función exponencial se utiliza únicamente para ajustar la parte superior de la curva de puntuación del documento centralizado, mientras que la parte inferior de la curva sigue siendo ajustada con el método de interpolación lineal descrito anteriormente. El ajuste mediante la función exponencial de los documentos mejor clasificados ha demostrado empíricamente producir resultados más precisos. A partir de las curvas de puntuación de documentos centralizadas, podemos estimar las listas completas de puntuación de documentos centralizados correspondientes para todas las bases de datos disponibles. Después de que las puntuaciones estimadas de los documentos centralizados se normalizan, las listas completas de probabilidades de relevancia pueden ser construidas a partir de las listas completas de puntuaciones de documentos centralizados mediante la Ecuación 1. Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El Modelo Unificado de Maximización de Utilidad En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco. En la tarea de recomendación de bases de datos, el sistema necesita decidir cómo clasificar las bases de datos. En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada. Generalizamos el proceso de selección de recomendaciones de bases de datos, que implícitamente recomienda todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos. Formalmente, denotamos di como el número de documentos que nos gustaría recuperar de la base de datos i y ,.....},{ 21 ddd = como una acción de selección para todas las bases de datos. La decisión de selección de la base de datos se toma en base a las listas completas de probabilidades de relevancia para todas las bases de datos. Las listas completas de probabilidades de relevancia se infieren a partir de toda la información disponible, específicamente sR, que representa las descripciones de recursos adquiridas mediante muestreo basado en consultas y las estimaciones del tamaño de la base de datos adquiridas mediante muestreo-resampleo; cS representa las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Si el método de estimación de puntajes de documentos centralizados y probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈. El vector aleatorio   denota un conjunto arbitrario de listas completas de probabilidades de relevancia y ),|( cs SRP θ como la probabilidad de generar este conjunto de listas. Finalmente, a cada acción de selección d y un conjunto de listas completas de la Figura 1. Construcción de la lista completa de puntuación de documentos centralizada mediante interpolación lineal (el factor de escala de la base de datos es 50). Para 35 probabilidades de relevancia θ, asociamos una función de utilidad ),( dU θ que indica el beneficio de realizar la selección d cuando las verdaderas listas completas de probabilidades de relevancia son θ. Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7). Un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en los valores de parámetros más probables en lugar de calcular toda la expectativa. En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recomendación de bases de datos. El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de N bases de datos de Nsdb) que contengan tantos documentos relevantes como sea posible, lo cual puede definirse formalmente como: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) es la función indicadora, que es 1 cuando se selecciona la i-ésima base de datos y 0 en caso contrario. Inserta esta ecuación en el modelo básico de la Ecuación 8 y asocia la restricción del número de base de datos seleccionado para obtener lo siguiente: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) La solución de este problema de optimización es muy simple. Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) Las bases de datos Nsdb con el mayor número esperado de documentos relevantes pueden ser seleccionadas para cumplir con el objetivo de alto recall. Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recuperación distribuida de documentos. Se mide mediante la Precisión en la parte superior de la lista final de documentos fusionados. Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la Precisión de los documentos recuperados de las bases de datos seleccionadas. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Tenga en cuenta que la diferencia clave entre la Ecuación 12 y la Ecuación 9 es que la Ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos, mientras que la Ecuación 12 solo considera una parte mucho más pequeña de la clasificación. Específicamente, podemos calcular la decisión de selección óptima mediante: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Diferentes tipos de restricciones causadas por las diferentes características de las tareas de recuperación de documentos pueden estar asociadas con el problema de optimización anterior. La más común es seleccionar un número fijo (Nsdb) de bases de datos y recuperar un número fijo (Nrdoc) de documentos de cada base de datos seleccionada, definido formalmente como: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) Este problema de optimización puede resolverse fácilmente calculando el número de documentos relevantes esperados en la parte superior de la lista completa de probabilidades de relevancia de cada base de datos: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Luego, las bases de datos pueden ser clasificadas por estos valores y seleccionadas. Llamamos a este algoritmo UUM/HP-FL (Maximización Unificada de Utilidad para Alta Precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada). Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada. Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes cantidades de documentos. Para simplificar, se requiere que las longitudes de la lista de resultados sean múltiplos de un número base 10. (Este valor también puede variar, pero para simplificar se establece en 10 en este documento). Esta restricción está establecida para simular el comportamiento de los motores de búsqueda comerciales en la web. (Motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 identificadores de documentos por página de resultados). Este procedimiento ahorra tiempo de cálculo al calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea de 10 en lugar de 1 (más detalles se discuten posteriormente). Para una mayor simplificación, restringimos la selección a un máximo de 100 documentos de cada base de datos (di<=100). Luego, el problema de optimización de la selección se formaliza de la siguiente manera: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc es el número total de documentos a recuperar. Desafortunadamente, no hay una solución simple para este problema de optimización como la hay para las Ecuaciones 10 y 14. Sin embargo, se puede aplicar un algoritmo de programación dinámica de 36 para calcular la solución óptima. Los pasos básicos de este método de programación dinámica se describen en la Figura 2. Dado que este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se le llama algoritmo UUM/HP-VL. Después de que se toman las decisiones de selección, se buscan las bases de datos seleccionadas y se recuperan los identificadores de documentos correspondientes de cada base de datos. El paso final de la recuperación de documentos es fusionar los resultados devueltos en una única lista clasificada con el algoritmo de aprendizaje semisupervisado. Se señaló anteriormente que el algoritmo SSL mapea las puntuaciones específicas de la base de datos en las puntuaciones de documentos centralizadas y construye la lista clasificada final en consecuencia, lo cual es consistente con todos nuestros procedimientos de selección donde se seleccionan los documentos con mayores probabilidades de relevancia (y por ende, puntuaciones de documentos centralizadas más altas). 4. METODOLOGÍA EXPERIMENTAL 4.1 Bancos de pruebas Es deseable evaluar algoritmos de recuperación de información distribuida con bancos de pruebas que simulen de cerca las aplicaciones del mundo real. Las colecciones web TREC WT2g o WT10g proporcionan una forma de dividir los documentos por diferentes servidores web. De esta manera, se podrían crear un gran número (O(1000)) de bases de datos con contenidos bastante diversos, lo que podría convertir a este banco de pruebas en un buen candidato para simular entornos operativos como la web oculta de dominio abierto. Sin embargo, dos debilidades de este banco de pruebas son: i) Cada base de datos contiene solo una pequeña cantidad de documentos (259 documentos en promedio para WT2g) [4]; y ii) El contenido de WT2g o WT10g se extrae arbitrariamente de la web. No es probable que una base de datos web oculta proporcione páginas personales o páginas web que indiquen que las páginas están en construcción y no contengan información útil en absoluto. Estos tipos de páginas web están contenidos en los conjuntos de datos WT2g/WT10g. Por lo tanto, los datos ruidosos de la Web no son similares a los contenidos de alta calidad de las bases de datos ocultas de la Web, que generalmente están organizados por expertos en el dominio. Otra opción es los datos de noticias/gobierno de TREC [1,15,17,18,21]. Los datos gubernamentales/noticias de TREC se centran en temas relativamente específicos. Comparado con los datos web de TREC: i) Los documentos de noticias/gobierno son mucho más similares a los contenidos proporcionados por una base de datos orientada a temas que a una página web arbitraria, ii) Una base de datos en este banco de pruebas es más grande que la de los datos web de TREC. En promedio, una base de datos contiene miles de documentos, lo cual es más realista que una base de datos de datos web de TREC con alrededor de 250 documentos. Dado que los contenidos y tamaños de las bases de datos en el banco de pruebas de noticias/gobierno de TREC son más similares a los de una base de datos orientada a temas, es un buen candidato para simular los entornos de recuperación de información distribuida de grandes organizaciones (empresas) o sitios web ocultos específicos de dominio, como West, que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3]. Dado que la mayoría de los sistemas actuales de recuperación de información distribuida están desarrollados para entornos de grandes organizaciones (empresas) o para la Web oculta de dominios específicos en lugar de la Web oculta de dominio abierto, en este trabajo se eligió el banco de pruebas de noticias/gobierno de TREC. El banco de pruebas Trec123-100col-bysource es uno de los más utilizados en las pruebas de noticias y gobierno de TREC [1,15,17,21]. Fue elegido en este trabajo. Tres bancos de pruebas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes también se utilizaron para proporcionar una simulación más exhaustiva para entornos reales. Se crearon 100 bases de datos a partir de los CDs de TREC 1, 2 y 3. Fueron organizados por fuente y fecha de publicación [1]. Los tamaños de las bases de datos no están sesgados. Los detalles se encuentran en la Tabla 1. Tres bancos de pruebas construidos en [21] se basaron en el banco de pruebas trec123-100colbysource. Cada banco de pruebas contiene muchas bases de datos pequeñas y dos bases de datos grandes creadas al fusionar alrededor de 10 a 20 bases de datos pequeñas. Listas completas de probabilidades de relevancia para todas las bases de datos |DB|. Solución de selección óptima para la Ecuación 16. i) Crear el arreglo tridimensional: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Cada Sel (x, y, z) está asociado con una decisión de selección xyzd, que representa la mejor decisión de selección en la condición: solo se consideran bases de datos del número 1 al número x para la selección; se recuperarán un total de y*10 documentos; solo se seleccionan z bases de datos de los candidatos de la base de datos x. Y Sel (x, y, z) es el valor de utilidad correspondiente al elegir la mejor selección. ii) Inicializar Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) solo con la información de relevancia estimada de la 1ª base de datos. iii) Iterar el candidato actual de la base de datos i desde 2 hasta |DB| Para cada entrada Sel (i, y, z): Encontrar k tal que: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k∗ documentos de la base de datos i-ésima, de lo contrario no debemos seleccionar esta base de datos y se debe mantener la solución anterior mejor Sel (i-1, y, z). Luego establezca el valor de iyzd y Sel (i, y, z) en consecuencia. iv) La mejor solución de selección se da por _ /10| | Toral rdoc sdbDB N Nd y el valor de utilidad correspondiente es Sel (|DB|, NTotal_rdoc/10, Nsdb). Figura 2. El procedimiento de optimización de programación dinámica para la Ecuación 16. Tabla 1: Estadísticas del banco de pruebas. Número de documentos Tamaño (MB) Tamaño del banco de pruebas (GB) Mínimo Promedio Máximo Mínimo Promedio Máximo Trec123 3.2 752 10782 39713 28 32 42 Tabla 2: Estadísticas del conjunto de consultas. Nombre del conjunto de temas TREC Campo del tema TREC Longitud promedio (palabras) Trec123 51-150 Título 3.1 37 Trec123-2ldb-60col (representativo): Las bases de datos en el trec123-100col-bysource se ordenaron en orden alfabético. Dos grandes bases de datos fueron creadas al fusionar 20 bases de datos pequeñas con el método de round-robin. Por lo tanto, las dos bases de datos grandes tienen más documentos relevantes debido a sus tamaños grandes, aunque las densidades de documentos relevantes son aproximadamente iguales a las de las bases de datos pequeñas. Las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, APall y WSJall. Las otras 60 colecciones quedaron sin cambios. Las bases de datos APall y WSJall tienen una mayor densidad de documentos relevantes para las consultas de TREC que las bases de datos pequeñas. Por lo tanto, las dos bases de datos grandes tienen muchos más documentos relevantes que las bases de datos pequeñas. Las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, FRall y DOEall. Las otras 80 colecciones quedaron sin cambios. Las bases de datos FRall y DOEall tienen densidades más bajas de documentos relevantes para las consultas de TREC que las bases de datos pequeñas, a pesar de ser mucho más grandes. Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150. Las consultas 101-150 se utilizaron como consultas de entrenamiento y las consultas 51-100 se utilizaron como consultas de prueba (detalles en la Tabla 2). 4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o en la Web oculta específica de dominio, diferentes bases de datos pueden utilizar diferentes tipos de motores de búsqueda. Para simular el entorno de múltiples motores de búsqueda, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: INQUERY [2], un modelo de lenguaje estadístico de unigrama con suavizado lineal [12,20] y un algoritmo de recuperación TFIDF con peso ltc [12,20]. Todos estos algoritmos fueron implementados con la herramienta Lemur [12]. Estos tres tipos de motores de búsqueda fueron asignados a las bases de datos entre los cuatro bancos de pruebas de manera round-robin. 5. RESULTADOS: SELECCIÓN DE RECURSOS DE LA RECOMENDACIÓN DE BASES DE DATOS Todos los cuatro bancos de pruebas descritos en la Sección 4 fueron utilizados en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de bases de datos. Las descripciones de los recursos fueron creadas utilizando muestreo basado en consultas. Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos. Las estadísticas del tamaño de la base de datos fueron estimadas mediante el método de muestra y remuestra [21]. Cincuenta consultas (101-150) se utilizaron como consultas de entrenamiento para construir el modelo logístico relevante y ajustar las funciones exponenciales de las curvas de puntuación de documentos centralizados para bases de datos de gran proporción (detalles en la Sección 3.1). Otros 50 consultas (51-100) se utilizaron como datos de prueba. Los algoritmos de selección de recursos de los sistemas de recomendación de bases de datos suelen compararse utilizando la métrica de recuperación nR [1,17,18,21]. Que B denote una clasificación base, que a menudo es la RBR (clasificación basada en relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos. Y que Bi y Ei denoten el número de documentos relevantes en la base de datos clasificada i-ésima de B o E. Entonces, Rn se define de la siguiente manera: = = = k i i k i i k B E R 1 1 (17) Por lo general, el objetivo es buscar solo algunas bases de datos, por lo que nuestras cifras solo muestran resultados para la selección de hasta 20 bases de datos. Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, CORI, ReDDE y UUM/HR. El algoritmo UUM/HR se describe en la Sección 3.3. Se puede observar en la Figura 3 que los algoritmos ReDDE y UUM/HR son más efectivos (en los conjuntos de pruebas representativos, relevantes y no relevantes) o igual de efectivos (en el conjunto de pruebas Trec123-100Col) que el algoritmo de selección de recursos CORI. El algoritmo UUM/HR es más efectivo que el algoritmo ReDDE en los conjuntos de pruebas representativos y relevantes y es aproximadamente igual que el algoritmo ReDDE en los conjuntos de pruebas Trec123100Col y no relevantes. Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo ReDDE. Se puede observar que al seleccionar solo algunas bases de datos en el Trec123-100Col o en los conjuntos de pruebas no relevantes, el algoritmo ReDEE tiene una pequeña ventaja sobre el algoritmo UUM/HR. Atribuimos esto a dos causas: i) El algoritmo ReDDE fue ajustado en el banco de pruebas Trec123-100Col; y ii) Aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico para estimar probabilidades de relevancia no es lo suficientemente preciso. Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este pequeño rompecabezas. Colecciones seleccionadas. Colecciones seleccionadas. Plataforma de pruebas Trec123-100Col. Plataforma de pruebas representativa. Colección seleccionada. Colección seleccionada. Plataforma de pruebas relevante. Plataforma de pruebas no relevante. Figura 3. Experimentos de selección de recursos en los cuatro bancos de pruebas. 38 6. RESULTADOS: EFECTIVIDAD DE LA RECUPERACIÓN DE DOCUMENTOS Para la recuperación de documentos, se buscan en las bases de datos seleccionadas y los resultados devueltos se fusionan en una lista final única. En todos los experimentos discutidos en esta sección, los resultados obtenidos de bases de datos individuales fueron combinados por el algoritmo de fusión de resultados de aprendizaje semisupervisado. Esta versión del algoritmo SSL [22] tiene permitido descargar un pequeño número de textos de documentos devueltos sobre la marcha para crear datos de entrenamiento adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicos de la base de datos en puntuaciones de documentos centralizadas estimadas. Se ha demostrado ser muy efectivo en entornos donde solo se obtienen listas de resultados cortas de cada base de datos seleccionada [22]. Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos. La efectividad de la recuperación de documentos se midió mediante la Precisión en la parte superior de la lista final de documentos. Los experimentos en esta sección se llevaron a cabo para estudiar la efectividad de recuperación de documentos de cinco algoritmos de selección, a saber, los algoritmos CORI, ReDDE, UUM/HR, UUM/HP-FL y UUM/HP-VL. Los últimos tres algoritmos fueron propuestos en la Sección 3. Todos los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada. El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos a recuperar de cada base de datos seleccionada; el número recuperado estaba limitado a ser de 10 a 100, y un múltiplo de 10. El Trec123-100Col y los bancos de pruebas representativos fueron seleccionados para la recuperación de documentos, ya que representan dos casos extremos de efectividad en la selección de recursos; en un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante Tabla 5. Precisión en el banco de pruebas representativo cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 documentos 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 documentos 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 documentos 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 documentos 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Tabla 6. Precisión en el banco de pruebas representativo cuando se seleccionaron 5 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 documentos 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 documentos 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 documentos 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 documentos 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Tabla 3. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 documentos 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 documentos 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 documentos 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 documentos 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Tabla 4. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 5 bases de datos. (El primer punto de referencia es CORI; el segundo punto de referencia para los métodos UUM/HP es UUM/HR). La precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 documentos 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 documentos 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 documentos 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 documentos 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 mucho peor que los otros algoritmos. Las Tablas 3 y 4 muestran los resultados en el banco de pruebas Trec123-100Col, y las Tablas 5 y 6 muestran los resultados en el banco de pruebas representativo. En el banco de pruebas Trec123-100Col, la efectividad de recuperación de documentos del algoritmo de selección CORI es aproximadamente la misma o un poco mejor que el algoritmo ReDDE, pero ambos son peores que los otros tres algoritmos (Tablas 3 y 4). El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos CORI y ReDDE. Una de las principales diferencias entre el algoritmo UUM/HR y el algoritmo ReDDE fue señalada anteriormente: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntuación de documentos centralizadas, mientras que el algoritmo ReDDE [21] utiliza un método heurístico, asume que las curvas de puntuación de documentos centralizadas son funciones escalonadas y no hace distinción entre la parte superior de las curvas. Esta diferencia hace que UUM/HR sea mejor que el algoritmo ReDDE para distinguir documentos con altas probabilidades de relevancia de aquéllos con bajas probabilidades de relevancia. Por lo tanto, el UUM/HR refleja mejor el objetivo de recuperación de alta precisión que el algoritmo ReDDE y, por lo tanto, es más efectivo para la recuperación de documentos. El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, como lo hacen los algoritmos UUM/HP-FL y UUM/HP-VL. Se puede observar que en este banco de pruebas, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos. Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de recuperación de documentos en las Ecuaciones 14 y 16. En el banco de pruebas representativo, CORI es mucho menos efectivo que otros algoritmos para la recuperación distribuida de documentos (Tablas 5 y 6). Los resultados de recuperación de documentos del algoritmo ReDDE son mejores que los del algoritmo CORI pero aún peores que los resultados del algoritmo UUM/HR. En este banco de pruebas, los tres algoritmos de UUM son aproximadamente igual de efectivos. Un análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho mayor que los experimentos en el banco de pruebas Trec123-100Col, ya que todos tienden a seleccionar las dos bases de datos grandes. Esto explica por qué son igualmente efectivos para la recuperación de documentos. En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos y reportar solo listas clasificadas de resultados. Dado que el modelo unificado de maximización de utilidad solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos. El único ajuste es que el algoritmo SSL fusiona listas clasificadas sin puntuaciones de documentos asignando a los documentos puntuaciones de pseudo-documentos normalizadas por sus rangos (En una lista clasificada de 50 documentos, el primero tiene una puntuación de 1, el segundo tiene una puntuación de 0.98, etc.), lo cual ha sido estudiado en [22]. Los resultados del experimento en el banco de pruebas trec123-100Col-bysource con 3 bases de datos seleccionadas se muestran en la Tabla 7. La configuración del experimento fue la misma que antes, excepto que las puntuaciones de los documentos fueron eliminadas intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de identificadores de documentos. Se puede observar en los resultados que el UUM/HP-FL y el UUM/HP-VL funcionan bien con bases de datos que no devuelven puntuaciones de documentos y siguen siendo más efectivos que otras alternativas. Otros experimentos con bases de datos que no devuelven puntuaciones de documentos no se informan, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL. Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión de manera explícita en la recuperación de documentos. Los nuevos algoritmos basados en este principio logran resultados mejores o al menos tan buenos como los algoritmos previos de vanguardia en varios entornos. CONCLUSIÓN La recuperación distribuida de información resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e Internet. La mayoría de investigaciones previas utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de bases de datos para la aplicación de recuperación de documentos distribuidos. Sostenemos que el objetivo de alta recuperación de recursos en la recomendación de bases de datos y el objetivo de alta precisión en la recuperación de documentos están relacionados pero no son idénticos. Este tipo de inconsistencia también ha sido observada en trabajos anteriores, pero las soluciones previas utilizaron métodos heurísticos o asumieron la cooperación de bases de datos individuales (por ejemplo, que todas las bases de datos utilizaran el mismo tipo de motores de búsqueda), lo cual frecuentemente no es cierto en un entorno no cooperativo. En este trabajo proponemos un modelo unificado de maximización de utilidad para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado. En este marco, las decisiones de selección se obtienen optimizando diferentes funciones objetivo. Hasta donde sabemos, este es el primer trabajo que intenta visualizar y modelar teóricamente la tarea de recuperación de información distribuida de manera integrada. El nuevo marco continúa una tendencia reciente de investigación que estudia el uso de muestreo basado en consultas y una base de datos de muestras centralizada. Se entrenó un único modelo logístico en la Tabla 7 centralizada. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). (Los motores de búsqueda no devuelven puntajes de documentos) Precisión en la Clasificación de Documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 documentos 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 documentos 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 documentos 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 documentos 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 base de datos de muestra para estimar las probabilidades de relevancia de documentos por sus puntajes de recuperación centralizados, mientras que la base de datos de muestra centralizada sirve como puente para conectar las bases de datos individuales con el modelo logístico centralizado. Por lo tanto, las probabilidades de relevancia para todos los documentos en las bases de datos pueden ser estimadas con una cantidad muy pequeña de juicio de relevancia humano, lo cual es mucho más eficiente que los métodos anteriores que construyen un modelo separado para cada base de datos. Este marco no solo es más sólido teóricamente, sino también muy efectivo. Un algoritmo para la selección de recursos (UUM/HR) y dos algoritmos para la recuperación de documentos (UUM/HP-FL y UUM/HP-VL) se derivan de este marco. Se han realizado estudios empíricos en bancos de pruebas para simular las soluciones de búsqueda distribuida de grandes organizaciones (empresas) o la Web oculta específica de un dominio. Además, los algoritmos de selección de recursos UUM/HP-FL y UUM/HP-VL se amplían con una variante del algoritmo de fusión de resultados SSL para abordar la tarea de recuperación de documentos distribuidos cuando las bases de datos seleccionadas no devuelven puntuaciones de documentos. Los experimentos han demostrado que estos algoritmos logran resultados que son al menos tan buenos como el estado del arte previo, y a veces considerablemente mejores. Un análisis detallado indica que la ventaja de estos algoritmos proviene de optimizar explícitamente los objetivos de las tareas específicas. El marco unificado de maximización de utilidad está abierto a diferentes extensiones. Cuando el costo está asociado con la búsqueda en las bases de datos en línea, el marco de utilidad puede ajustarse para estimar automáticamente el mejor número de bases de datos a buscar, de modo que se puedan recuperar una gran cantidad de documentos relevantes con costos relativamente bajos. Otra extensión del marco es considerar la efectividad de la recuperación de información de las bases de datos en línea, lo cual es un tema importante en los entornos operativos. Todas estas son las direcciones de la investigación futura. AGRADECIMIENTO Esta investigación fue apoyada por las subvenciones de la NSF EIA-9983253 y IIS-0118767. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este documento son del autor y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] J. Callan. (2000). Recuperación de información distribuida. En W.B. Croft, editor, Avances en Recuperación de Información. Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. \n\nEditorial Kluwer Academic. (pp. 127-150). [2] J. Callan, W.B. Croft, y J. Broglio. (1995). Experimentos TREC y TIPSTER con INQUERY. Procesamiento y Gestión de la Información, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson y M. Meziou. (2002). Selección de base de datos utilizando recursos de colección lógica adquiridos y físicos reales en un entorno operativo masivo específico de dominio. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [4] N. Craswell. (2000). Métodos para la recuperación distribuida de información. I'm sorry, but the sentence \"Ph.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate to Spanish? Tesis doctoral, Universidad Nacional Australiana. [5] N. Craswell, D. Hawking y P. Thistlewaite. (1999). Combinando resultados de motores de búsqueda aislados. En Actas de la 10ª Conferencia de Bases de Datos Australasiana. [6] D. DSouza, J. Thom y J. Zobel. (2000). Una comparación de técnicas para seleccionar colecciones de texto. En Actas de la 11ª Conferencia de Bases de Datos Australasiana. [7] N. Fuhr. (1999). Un enfoque de Teoría de la Decisión para la selección de bases de datos en IR en red. ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina y A. Paepcke. (1997). Propuesta de Stanford para la metabusqueda en internet. En Actas de la 20ª Conferencia Internacional ACM-SIGMOD sobre Gestión de Datos. [9] L. Gravano, P. Ipeirotis y M. Sahami. (2003). QProber: Un sistema para la clasificación automática de bases de datos de la web oculta. ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis y L. Gravano. (2002). Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] El kit de herramientas lemur. http://www.cs.cmu.edu/~lemur [13] J. Lu y J. Callan. (2003). Recuperación de información basada en contenido en redes peer-to-peer. En Actas de la 12ª Conferencia Internacional sobre Información y Gestión del Conocimiento. [14] W. Meng, C.T. Yu y K.L. Liu. (2002) Construcción de motores de búsqueda eficientes y efectivos. ACM Comput. Surv. 34(1). [15] H. Nottelmann y N. Fuhr. (2003). Evaluando diferentes métodos para estimar la calidad de recuperación para la selección de recursos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [16] H., Nottelmann y N., Fuhr. (2003). La arquitectura MIND para bibliotecas digitales federadas de multimedia heterogénea. Taller ACM SIGIR 2003 sobre Recuperación de Información Distribuida. [17] A.L. Powell, J.C. French, J. Callan, M. Connell y C.L. Viles. (2000). \n\nViles. (2000). El impacto de la selección de bases de datos en la búsqueda distribuida. En Actas de la 23ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [18] A.L. Powell y J.C. French. (2003). Comparando el rendimiento de los algoritmos de selección de bases de datos. ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). \n\nACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). Busca en la web invisible. Guardian Unlimited. [20] L. Si y J. Callan. (2002). Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [21] L. Si y J. Callan. (2003). Método de estimación de distribución de documentos relevantes para la selección de recursos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [22] L. Si y J. Callan. (2003). Un método de aprendizaje semi-supervisado para fusionar los resultados de un motor de búsqueda. ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41\n\nACM Transactions on Information Systems, 21(4). (pp. 457-491). 41 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "semi-supervised learning": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the <br>semi-supervised learning</br> (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A <br>semi-supervised learning</br> method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "Furthermore, the <br>semi-supervised learning</br> (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "A <br>semi-supervised learning</br> method to merge search engine results."
            ],
            "translated_annotated_samples": [
                "Además, se aplica el algoritmo de <br>aprendizaje semisupervisado</br> (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada.",
                "Un método de <br>aprendizaje semi-supervisado</br> para fusionar los resultados de un motor de búsqueda."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La recuperación distribuida de documentos es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de recuperación de documentos distribuidos, tratándolos como objetivos de optimización diferentes. Primero, se construye una base de datos de muestra centralizada mediante el muestreo aleatorio de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas; también se estiman las estadísticas del tamaño de la base de datos. Un modelo de transformación logística se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia. Segundo, después de que se envía una nueva consulta, la consulta se puede utilizar para buscar en la base de datos de muestras centralizada que produce una puntuación para cada documento muestreado. La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada puede estimarse aplicando el modelo logístico al puntaje de cada documento. Entonces, las probabilidades de relevancia de todos los documentos (en su mayoría no vistos) entre las bases de datos disponibles pueden ser estimadas utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos. Para la tarea de selección de recursos para un sistema de recomendación de bases de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alto recall. Para la selección de recursos para un sistema distribuido de recuperación de documentos, se prefieren las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia. Este criterio de selección cumple con el objetivo de alta precisión de la aplicación de recuperación de documentos distribuidos. Además, se aplica el algoritmo de <br>aprendizaje semisupervisado</br> (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada. El marco de utilidad unificado hace muy pocas suposiciones y funciona en entornos no cooperativos. Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos correspondientes; y ii) Muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos. Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades correspondientes de relevancia y utiliza la base de datos de muestra centralizada como puente entre las bases de datos individuales y el modelo logístico. El esfuerzo humano (juicio de relevancia) necesario para entrenar el modelo logístico centralizado único no aumenta con el número de bases de datos. Esta es una gran ventaja sobre investigaciones anteriores, las cuales requerían que la cantidad de esfuerzo humano fuera lineal con el número de bases de datos [7,15]. El marco de utilidad unificada no solo es más sólido teóricamente, sino también muy efectivo. Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de vanguardia en una variedad de configuraciones. La siguiente sección discute el trabajo relacionado. La sección 3 describe el nuevo modelo unificado de maximización de utilidad. La sección 4 explica nuestra metodología experimental. Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos. La sección 7 concluye. 2. Investigación previa Ha habido una considerable investigación sobre todos los subproblemas de la recuperación de información distribuida. Exploramos los trabajos más relacionados en esta sección. El primer problema de la recuperación de información distribuida es la representación de recursos. El protocolo STARTS es una solución para adquirir descripciones de recursos en entornos cooperativos [8]. Sin embargo, en entornos no cooperativos, aunque las bases de datos estén dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no. Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí. Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en consultas, que genera y envía consultas de forma aleatoria a motores de búsqueda individuales y recupera algunos documentos para construir las descripciones. Dado que los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico. Los experimentos han demostrado que descripciones de recursos bastante precisas pueden ser construidas enviando alrededor de 80 consultas y descargando alrededor de 300 documentos [1]. Muchos algoritmos de selección de recursos como gGlOSS/vGlOSS [8] y CORI [1] han sido propuestos en la última década. El algoritmo CORI representa cada base de datos por sus términos, las frecuencias de los documentos y un pequeño número de estadísticas del corpus (detalles en [1]). Como investigaciones previas en diferentes conjuntos de datos han demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo utilizamos como algoritmo base en este trabajo. El algoritmo de selección de recursos de estimación de distribución de documentos relevantes (ReDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia. Aunque se ha demostrado que el algoritmo ReDDE es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21]. El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar puntuaciones de documentos específicas de la base de datos en puntuaciones de documentos independientes de la base de datos comparables. El algoritmo de fusión de resultados de aprendizaje semisupervisado (SSL) [20,22] utiliza los documentos adquiridos mediante muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos y de la consulta. Estos modelos lineales se utilizan para convertir las puntuaciones de documentos específicas de la base de datos en las puntuaciones de documentos centralizadas aproximadas. El algoritmo SSL ha demostrado ser efectivo [22]. Sirve como un componente importante de nuestro marco unificado de maximización de utilidad (Sección 3). Para lograr resultados precisos en la recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos en sistemas de recomendación de bases de datos. Pero como se señaló anteriormente, un algoritmo de selección de recursos optimizado para un alto recuerdo puede no funcionar bien para la recuperación de documentos, que tiene como objetivo la alta precisión. Este tipo de inconsistencia ha sido observada en investigaciones previas [4,21]. La investigación en [21] intentó resolver el problema con un método heurístico. La investigación más similar a lo que proponemos aquí es el marco teórico de la toma de decisiones (DTF) [7,15]. Este marco de trabajo calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y se han propuesto varios métodos [15] para estimar la calidad de recuperación. Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF. Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de bases de datos y recuperación distribuida de documentos) en el mismo marco unificado. Segundo, el DTF construye un modelo para cada base de datos para calcular las probabilidades de relevancia. Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos. Por el contrario, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada. La base de datos de muestra centralizada puede servir como puente para conectar las bases de datos individuales con el modelo logístico centralizado, de esta manera se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos. Esta estrategia puede ahorrar una gran cantidad de esfuerzo en juicio humano y es una gran ventaja del marco de maximización de utilidad unificada sobre el DTF, especialmente cuando hay un gran número de bases de datos. MARCO DE MAXIMIZACIÓN DE UTILIDAD UNIFICADA El marco de Maximización de Utilidad Unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría no vistos) disponibles en el entorno de búsqueda distribuida. En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el modelo de Maximización de Utilidad Unificado. También describimos cómo el modelo puede ser optimizado para el objetivo de alto recuerdo de un sistema de recomendación de base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido. 3.1 Estimación de Probabilidades de Relevancia Como se señaló anteriormente, el propósito de la selección de recursos es el alto recuerdo y el propósito de la recuperación de documentos es la alta precisión. Para cumplir con estos objetivos diversos, el problema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos. Este es un problema difícil porque solo podemos observar una muestra de los contenidos de cada base de datos utilizando muestreo basado en consultas. Nuestra estrategia es aprovechar al máximo toda la información disponible para calcular las estimaciones de probabilidad. 3.1.1 Aprendizaje de Probabilidades de Relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se construye mediante muestreo basado en consultas y los tamaños de la base de datos se estiman utilizando el método de muestreo y remuestreo [21]. Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (Inquery [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de entrenamiento. Para cada consulta de entrenamiento, se aplica el algoritmo de selección de recursos CORI [1] para seleccionar un cierto número (por ejemplo, 10) de bases de datos y recuperar 50 identificadores de documentos de cada base de datos. El algoritmo de fusión de resultados SSL [20,22] se utiliza para combinar los resultados. Luego, podemos descargar los 50 documentos principales de la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando Inquery y las estadísticas del corpus de la base de datos de muestra centralizada. Las puntuaciones centralizadas se normalizan aún más (dividiéndolas por la puntuación centralizada máxima para cada consulta), ya que este método ha sido sugerido para mejorar la precisión de la estimación en investigaciones anteriores [15]. El juicio humano se adquiere para esos documentos y se construye un modelo logístico para transformar las puntuaciones de documentos centralizados normalizados en probabilidades de relevancia de la siguiente manera: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) donde )( _ dSc es la puntuación de documento centralizada normalizada y ac y bc son los dos parámetros del modelo logístico. Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de entrenamiento. El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de las puntuaciones de documentos centralizadas. 3.1.2 Estimación de las puntuaciones de documentos centralizadas Cuando el usuario envía una nueva consulta, se calculan las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar las puntuaciones de los documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados. Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas del tamaño de la base de datos. Definimos el factor de escala de la base de datos para la base de datos i como la razón entre el tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: SF_i = ^N_db / _N_db_samp_i donde ^N_db es el tamaño estimado de la base de datos y _N_db_samp_i es el número de documentos de la base de datos i en la base de datos de muestra centralizada. La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podríamos suponer que hay alrededor de 50 documentos en esa base de datos que tienen puntuaciones de alrededor de 0.5. De hecho, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntuación del documento centralizado para cada base de datos. Formalmente, clasificamos todos los documentos muestreados de la base de datos i-ésima por sus puntajes de documento centralizado 34 para obtener la lista de puntajes de documento centralizado muestreado {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} para la base de datos i; asumimos que si pudiéramos calcular los puntajes de documento centralizado para todos los documentos en esta base de datos y obtener la lista completa de puntajes de documento centralizado, el documento superior en la lista muestreada tendría un rango de SFdbi/2, el segundo documento en la lista muestreada tendría un rango de SFdbi3/2, y así sucesivamente. Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. La interpolación lineal por tramos se aplica para estimar la curva de puntuación del documento centralizado, como se ilustra en la Figura 1. La lista completa de puntuaciones de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documentos centralizados como: ],1[,)(S ^^ c idbij Njd ∈ . Se puede observar en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntuación del documento centralizado. Sin embargo, para bases de datos con grandes proporciones de escala de base de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos mejor clasificados (por ejemplo, [1, SFdbi/2]). Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizados de los documentos mejor clasificados para bases de datos con ratios a gran escala (por ejemplo, mayores de 100). Específicamente, se construye un modelo logístico para cada una de estas bases de datos. El modelo logístico se utiliza para estimar la puntuación del documento centralizado superior 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con las puntuaciones centralizadas más altas. 0iα , 1iα y 2iα son los parámetros del modelo logístico. Para cada consulta de entrenamiento, se descarga el documento mejor recuperado de cada base de datos y se calcula la puntuación del documento centralizado correspondiente. Junto con las puntuaciones de los dos documentos muestreados principales, estos parámetros pueden ser estimados. Después de estimar la puntuación centralizada del documento principal, se ajusta una función exponencial para la parte superior ([1, SFdbi/2]) de la curva de puntuación del documento centralizado como: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) Los dos parámetros 0iβ y 1iβ se ajustan para asegurarse de que la función exponencial pase por los dos puntos (1, ^ 1)( ic dS ) y (SFdbi/2, Sc(dsi1)). La función exponencial se utiliza únicamente para ajustar la parte superior de la curva de puntuación del documento centralizado, mientras que la parte inferior de la curva sigue siendo ajustada con el método de interpolación lineal descrito anteriormente. El ajuste mediante la función exponencial de los documentos mejor clasificados ha demostrado empíricamente producir resultados más precisos. A partir de las curvas de puntuación de documentos centralizadas, podemos estimar las listas completas de puntuación de documentos centralizados correspondientes para todas las bases de datos disponibles. Después de que las puntuaciones estimadas de los documentos centralizados se normalizan, las listas completas de probabilidades de relevancia pueden ser construidas a partir de las listas completas de puntuaciones de documentos centralizados mediante la Ecuación 1. Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El Modelo Unificado de Maximización de Utilidad En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco. En la tarea de recomendación de bases de datos, el sistema necesita decidir cómo clasificar las bases de datos. En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada. Generalizamos el proceso de selección de recomendaciones de bases de datos, que implícitamente recomienda todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos. Formalmente, denotamos di como el número de documentos que nos gustaría recuperar de la base de datos i y ,.....},{ 21 ddd = como una acción de selección para todas las bases de datos. La decisión de selección de la base de datos se toma en base a las listas completas de probabilidades de relevancia para todas las bases de datos. Las listas completas de probabilidades de relevancia se infieren a partir de toda la información disponible, específicamente sR, que representa las descripciones de recursos adquiridas mediante muestreo basado en consultas y las estimaciones del tamaño de la base de datos adquiridas mediante muestreo-resampleo; cS representa las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Si el método de estimación de puntajes de documentos centralizados y probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈. El vector aleatorio   denota un conjunto arbitrario de listas completas de probabilidades de relevancia y ),|( cs SRP θ como la probabilidad de generar este conjunto de listas. Finalmente, a cada acción de selección d y un conjunto de listas completas de la Figura 1. Construcción de la lista completa de puntuación de documentos centralizada mediante interpolación lineal (el factor de escala de la base de datos es 50). Para 35 probabilidades de relevancia θ, asociamos una función de utilidad ),( dU θ que indica el beneficio de realizar la selección d cuando las verdaderas listas completas de probabilidades de relevancia son θ. Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7). Un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en los valores de parámetros más probables en lugar de calcular toda la expectativa. En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recomendación de bases de datos. El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de N bases de datos de Nsdb) que contengan tantos documentos relevantes como sea posible, lo cual puede definirse formalmente como: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) es la función indicadora, que es 1 cuando se selecciona la i-ésima base de datos y 0 en caso contrario. Inserta esta ecuación en el modelo básico de la Ecuación 8 y asocia la restricción del número de base de datos seleccionado para obtener lo siguiente: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) La solución de este problema de optimización es muy simple. Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) Las bases de datos Nsdb con el mayor número esperado de documentos relevantes pueden ser seleccionadas para cumplir con el objetivo de alto recall. Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recuperación distribuida de documentos. Se mide mediante la Precisión en la parte superior de la lista final de documentos fusionados. Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la Precisión de los documentos recuperados de las bases de datos seleccionadas. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Tenga en cuenta que la diferencia clave entre la Ecuación 12 y la Ecuación 9 es que la Ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos, mientras que la Ecuación 12 solo considera una parte mucho más pequeña de la clasificación. Específicamente, podemos calcular la decisión de selección óptima mediante: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Diferentes tipos de restricciones causadas por las diferentes características de las tareas de recuperación de documentos pueden estar asociadas con el problema de optimización anterior. La más común es seleccionar un número fijo (Nsdb) de bases de datos y recuperar un número fijo (Nrdoc) de documentos de cada base de datos seleccionada, definido formalmente como: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) Este problema de optimización puede resolverse fácilmente calculando el número de documentos relevantes esperados en la parte superior de la lista completa de probabilidades de relevancia de cada base de datos: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Luego, las bases de datos pueden ser clasificadas por estos valores y seleccionadas. Llamamos a este algoritmo UUM/HP-FL (Maximización Unificada de Utilidad para Alta Precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada). Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada. Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes cantidades de documentos. Para simplificar, se requiere que las longitudes de la lista de resultados sean múltiplos de un número base 10. (Este valor también puede variar, pero para simplificar se establece en 10 en este documento). Esta restricción está establecida para simular el comportamiento de los motores de búsqueda comerciales en la web. (Motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 identificadores de documentos por página de resultados). Este procedimiento ahorra tiempo de cálculo al calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea de 10 en lugar de 1 (más detalles se discuten posteriormente). Para una mayor simplificación, restringimos la selección a un máximo de 100 documentos de cada base de datos (di<=100). Luego, el problema de optimización de la selección se formaliza de la siguiente manera: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc es el número total de documentos a recuperar. Desafortunadamente, no hay una solución simple para este problema de optimización como la hay para las Ecuaciones 10 y 14. Sin embargo, se puede aplicar un algoritmo de programación dinámica de 36 para calcular la solución óptima. Los pasos básicos de este método de programación dinámica se describen en la Figura 2. Dado que este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se le llama algoritmo UUM/HP-VL. Después de que se toman las decisiones de selección, se buscan las bases de datos seleccionadas y se recuperan los identificadores de documentos correspondientes de cada base de datos. El paso final de la recuperación de documentos es fusionar los resultados devueltos en una única lista clasificada con el algoritmo de aprendizaje semisupervisado. Se señaló anteriormente que el algoritmo SSL mapea las puntuaciones específicas de la base de datos en las puntuaciones de documentos centralizadas y construye la lista clasificada final en consecuencia, lo cual es consistente con todos nuestros procedimientos de selección donde se seleccionan los documentos con mayores probabilidades de relevancia (y por ende, puntuaciones de documentos centralizadas más altas). 4. METODOLOGÍA EXPERIMENTAL 4.1 Bancos de pruebas Es deseable evaluar algoritmos de recuperación de información distribuida con bancos de pruebas que simulen de cerca las aplicaciones del mundo real. Las colecciones web TREC WT2g o WT10g proporcionan una forma de dividir los documentos por diferentes servidores web. De esta manera, se podrían crear un gran número (O(1000)) de bases de datos con contenidos bastante diversos, lo que podría convertir a este banco de pruebas en un buen candidato para simular entornos operativos como la web oculta de dominio abierto. Sin embargo, dos debilidades de este banco de pruebas son: i) Cada base de datos contiene solo una pequeña cantidad de documentos (259 documentos en promedio para WT2g) [4]; y ii) El contenido de WT2g o WT10g se extrae arbitrariamente de la web. No es probable que una base de datos web oculta proporcione páginas personales o páginas web que indiquen que las páginas están en construcción y no contengan información útil en absoluto. Estos tipos de páginas web están contenidos en los conjuntos de datos WT2g/WT10g. Por lo tanto, los datos ruidosos de la Web no son similares a los contenidos de alta calidad de las bases de datos ocultas de la Web, que generalmente están organizados por expertos en el dominio. Otra opción es los datos de noticias/gobierno de TREC [1,15,17,18,21]. Los datos gubernamentales/noticias de TREC se centran en temas relativamente específicos. Comparado con los datos web de TREC: i) Los documentos de noticias/gobierno son mucho más similares a los contenidos proporcionados por una base de datos orientada a temas que a una página web arbitraria, ii) Una base de datos en este banco de pruebas es más grande que la de los datos web de TREC. En promedio, una base de datos contiene miles de documentos, lo cual es más realista que una base de datos de datos web de TREC con alrededor de 250 documentos. Dado que los contenidos y tamaños de las bases de datos en el banco de pruebas de noticias/gobierno de TREC son más similares a los de una base de datos orientada a temas, es un buen candidato para simular los entornos de recuperación de información distribuida de grandes organizaciones (empresas) o sitios web ocultos específicos de dominio, como West, que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3]. Dado que la mayoría de los sistemas actuales de recuperación de información distribuida están desarrollados para entornos de grandes organizaciones (empresas) o para la Web oculta de dominios específicos en lugar de la Web oculta de dominio abierto, en este trabajo se eligió el banco de pruebas de noticias/gobierno de TREC. El banco de pruebas Trec123-100col-bysource es uno de los más utilizados en las pruebas de noticias y gobierno de TREC [1,15,17,21]. Fue elegido en este trabajo. Tres bancos de pruebas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes también se utilizaron para proporcionar una simulación más exhaustiva para entornos reales. Se crearon 100 bases de datos a partir de los CDs de TREC 1, 2 y 3. Fueron organizados por fuente y fecha de publicación [1]. Los tamaños de las bases de datos no están sesgados. Los detalles se encuentran en la Tabla 1. Tres bancos de pruebas construidos en [21] se basaron en el banco de pruebas trec123-100colbysource. Cada banco de pruebas contiene muchas bases de datos pequeñas y dos bases de datos grandes creadas al fusionar alrededor de 10 a 20 bases de datos pequeñas. Listas completas de probabilidades de relevancia para todas las bases de datos |DB|. Solución de selección óptima para la Ecuación 16. i) Crear el arreglo tridimensional: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Cada Sel (x, y, z) está asociado con una decisión de selección xyzd, que representa la mejor decisión de selección en la condición: solo se consideran bases de datos del número 1 al número x para la selección; se recuperarán un total de y*10 documentos; solo se seleccionan z bases de datos de los candidatos de la base de datos x. Y Sel (x, y, z) es el valor de utilidad correspondiente al elegir la mejor selección. ii) Inicializar Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) solo con la información de relevancia estimada de la 1ª base de datos. iii) Iterar el candidato actual de la base de datos i desde 2 hasta |DB| Para cada entrada Sel (i, y, z): Encontrar k tal que: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k∗ documentos de la base de datos i-ésima, de lo contrario no debemos seleccionar esta base de datos y se debe mantener la solución anterior mejor Sel (i-1, y, z). Luego establezca el valor de iyzd y Sel (i, y, z) en consecuencia. iv) La mejor solución de selección se da por _ /10| | Toral rdoc sdbDB N Nd y el valor de utilidad correspondiente es Sel (|DB|, NTotal_rdoc/10, Nsdb). Figura 2. El procedimiento de optimización de programación dinámica para la Ecuación 16. Tabla 1: Estadísticas del banco de pruebas. Número de documentos Tamaño (MB) Tamaño del banco de pruebas (GB) Mínimo Promedio Máximo Mínimo Promedio Máximo Trec123 3.2 752 10782 39713 28 32 42 Tabla 2: Estadísticas del conjunto de consultas. Nombre del conjunto de temas TREC Campo del tema TREC Longitud promedio (palabras) Trec123 51-150 Título 3.1 37 Trec123-2ldb-60col (representativo): Las bases de datos en el trec123-100col-bysource se ordenaron en orden alfabético. Dos grandes bases de datos fueron creadas al fusionar 20 bases de datos pequeñas con el método de round-robin. Por lo tanto, las dos bases de datos grandes tienen más documentos relevantes debido a sus tamaños grandes, aunque las densidades de documentos relevantes son aproximadamente iguales a las de las bases de datos pequeñas. Las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, APall y WSJall. Las otras 60 colecciones quedaron sin cambios. Las bases de datos APall y WSJall tienen una mayor densidad de documentos relevantes para las consultas de TREC que las bases de datos pequeñas. Por lo tanto, las dos bases de datos grandes tienen muchos más documentos relevantes que las bases de datos pequeñas. Las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, FRall y DOEall. Las otras 80 colecciones quedaron sin cambios. Las bases de datos FRall y DOEall tienen densidades más bajas de documentos relevantes para las consultas de TREC que las bases de datos pequeñas, a pesar de ser mucho más grandes. Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150. Las consultas 101-150 se utilizaron como consultas de entrenamiento y las consultas 51-100 se utilizaron como consultas de prueba (detalles en la Tabla 2). 4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o en la Web oculta específica de dominio, diferentes bases de datos pueden utilizar diferentes tipos de motores de búsqueda. Para simular el entorno de múltiples motores de búsqueda, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: INQUERY [2], un modelo de lenguaje estadístico de unigrama con suavizado lineal [12,20] y un algoritmo de recuperación TFIDF con peso ltc [12,20]. Todos estos algoritmos fueron implementados con la herramienta Lemur [12]. Estos tres tipos de motores de búsqueda fueron asignados a las bases de datos entre los cuatro bancos de pruebas de manera round-robin. 5. RESULTADOS: SELECCIÓN DE RECURSOS DE LA RECOMENDACIÓN DE BASES DE DATOS Todos los cuatro bancos de pruebas descritos en la Sección 4 fueron utilizados en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de bases de datos. Las descripciones de los recursos fueron creadas utilizando muestreo basado en consultas. Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos. Las estadísticas del tamaño de la base de datos fueron estimadas mediante el método de muestra y remuestra [21]. Cincuenta consultas (101-150) se utilizaron como consultas de entrenamiento para construir el modelo logístico relevante y ajustar las funciones exponenciales de las curvas de puntuación de documentos centralizados para bases de datos de gran proporción (detalles en la Sección 3.1). Otros 50 consultas (51-100) se utilizaron como datos de prueba. Los algoritmos de selección de recursos de los sistemas de recomendación de bases de datos suelen compararse utilizando la métrica de recuperación nR [1,17,18,21]. Que B denote una clasificación base, que a menudo es la RBR (clasificación basada en relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos. Y que Bi y Ei denoten el número de documentos relevantes en la base de datos clasificada i-ésima de B o E. Entonces, Rn se define de la siguiente manera: = = = k i i k i i k B E R 1 1 (17) Por lo general, el objetivo es buscar solo algunas bases de datos, por lo que nuestras cifras solo muestran resultados para la selección de hasta 20 bases de datos. Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, CORI, ReDDE y UUM/HR. El algoritmo UUM/HR se describe en la Sección 3.3. Se puede observar en la Figura 3 que los algoritmos ReDDE y UUM/HR son más efectivos (en los conjuntos de pruebas representativos, relevantes y no relevantes) o igual de efectivos (en el conjunto de pruebas Trec123-100Col) que el algoritmo de selección de recursos CORI. El algoritmo UUM/HR es más efectivo que el algoritmo ReDDE en los conjuntos de pruebas representativos y relevantes y es aproximadamente igual que el algoritmo ReDDE en los conjuntos de pruebas Trec123100Col y no relevantes. Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo ReDDE. Se puede observar que al seleccionar solo algunas bases de datos en el Trec123-100Col o en los conjuntos de pruebas no relevantes, el algoritmo ReDEE tiene una pequeña ventaja sobre el algoritmo UUM/HR. Atribuimos esto a dos causas: i) El algoritmo ReDDE fue ajustado en el banco de pruebas Trec123-100Col; y ii) Aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico para estimar probabilidades de relevancia no es lo suficientemente preciso. Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este pequeño rompecabezas. Colecciones seleccionadas. Colecciones seleccionadas. Plataforma de pruebas Trec123-100Col. Plataforma de pruebas representativa. Colección seleccionada. Colección seleccionada. Plataforma de pruebas relevante. Plataforma de pruebas no relevante. Figura 3. Experimentos de selección de recursos en los cuatro bancos de pruebas. 38 6. RESULTADOS: EFECTIVIDAD DE LA RECUPERACIÓN DE DOCUMENTOS Para la recuperación de documentos, se buscan en las bases de datos seleccionadas y los resultados devueltos se fusionan en una lista final única. En todos los experimentos discutidos en esta sección, los resultados obtenidos de bases de datos individuales fueron combinados por el algoritmo de fusión de resultados de aprendizaje semisupervisado. Esta versión del algoritmo SSL [22] tiene permitido descargar un pequeño número de textos de documentos devueltos sobre la marcha para crear datos de entrenamiento adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicos de la base de datos en puntuaciones de documentos centralizadas estimadas. Se ha demostrado ser muy efectivo en entornos donde solo se obtienen listas de resultados cortas de cada base de datos seleccionada [22]. Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos. La efectividad de la recuperación de documentos se midió mediante la Precisión en la parte superior de la lista final de documentos. Los experimentos en esta sección se llevaron a cabo para estudiar la efectividad de recuperación de documentos de cinco algoritmos de selección, a saber, los algoritmos CORI, ReDDE, UUM/HR, UUM/HP-FL y UUM/HP-VL. Los últimos tres algoritmos fueron propuestos en la Sección 3. Todos los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada. El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos a recuperar de cada base de datos seleccionada; el número recuperado estaba limitado a ser de 10 a 100, y un múltiplo de 10. El Trec123-100Col y los bancos de pruebas representativos fueron seleccionados para la recuperación de documentos, ya que representan dos casos extremos de efectividad en la selección de recursos; en un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante Tabla 5. Precisión en el banco de pruebas representativo cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 documentos 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 documentos 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 documentos 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 documentos 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Tabla 6. Precisión en el banco de pruebas representativo cuando se seleccionaron 5 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 documentos 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 documentos 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 documentos 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 documentos 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Tabla 3. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 documentos 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 documentos 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 documentos 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 documentos 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Tabla 4. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 5 bases de datos. (El primer punto de referencia es CORI; el segundo punto de referencia para los métodos UUM/HP es UUM/HR). La precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 documentos 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 documentos 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 documentos 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 documentos 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 mucho peor que los otros algoritmos. Las Tablas 3 y 4 muestran los resultados en el banco de pruebas Trec123-100Col, y las Tablas 5 y 6 muestran los resultados en el banco de pruebas representativo. En el banco de pruebas Trec123-100Col, la efectividad de recuperación de documentos del algoritmo de selección CORI es aproximadamente la misma o un poco mejor que el algoritmo ReDDE, pero ambos son peores que los otros tres algoritmos (Tablas 3 y 4). El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos CORI y ReDDE. Una de las principales diferencias entre el algoritmo UUM/HR y el algoritmo ReDDE fue señalada anteriormente: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntuación de documentos centralizadas, mientras que el algoritmo ReDDE [21] utiliza un método heurístico, asume que las curvas de puntuación de documentos centralizadas son funciones escalonadas y no hace distinción entre la parte superior de las curvas. Esta diferencia hace que UUM/HR sea mejor que el algoritmo ReDDE para distinguir documentos con altas probabilidades de relevancia de aquéllos con bajas probabilidades de relevancia. Por lo tanto, el UUM/HR refleja mejor el objetivo de recuperación de alta precisión que el algoritmo ReDDE y, por lo tanto, es más efectivo para la recuperación de documentos. El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, como lo hacen los algoritmos UUM/HP-FL y UUM/HP-VL. Se puede observar que en este banco de pruebas, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos. Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de recuperación de documentos en las Ecuaciones 14 y 16. En el banco de pruebas representativo, CORI es mucho menos efectivo que otros algoritmos para la recuperación distribuida de documentos (Tablas 5 y 6). Los resultados de recuperación de documentos del algoritmo ReDDE son mejores que los del algoritmo CORI pero aún peores que los resultados del algoritmo UUM/HR. En este banco de pruebas, los tres algoritmos de UUM son aproximadamente igual de efectivos. Un análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho mayor que los experimentos en el banco de pruebas Trec123-100Col, ya que todos tienden a seleccionar las dos bases de datos grandes. Esto explica por qué son igualmente efectivos para la recuperación de documentos. En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos y reportar solo listas clasificadas de resultados. Dado que el modelo unificado de maximización de utilidad solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos. El único ajuste es que el algoritmo SSL fusiona listas clasificadas sin puntuaciones de documentos asignando a los documentos puntuaciones de pseudo-documentos normalizadas por sus rangos (En una lista clasificada de 50 documentos, el primero tiene una puntuación de 1, el segundo tiene una puntuación de 0.98, etc.), lo cual ha sido estudiado en [22]. Los resultados del experimento en el banco de pruebas trec123-100Col-bysource con 3 bases de datos seleccionadas se muestran en la Tabla 7. La configuración del experimento fue la misma que antes, excepto que las puntuaciones de los documentos fueron eliminadas intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de identificadores de documentos. Se puede observar en los resultados que el UUM/HP-FL y el UUM/HP-VL funcionan bien con bases de datos que no devuelven puntuaciones de documentos y siguen siendo más efectivos que otras alternativas. Otros experimentos con bases de datos que no devuelven puntuaciones de documentos no se informan, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL. Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión de manera explícita en la recuperación de documentos. Los nuevos algoritmos basados en este principio logran resultados mejores o al menos tan buenos como los algoritmos previos de vanguardia en varios entornos. CONCLUSIÓN La recuperación distribuida de información resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e Internet. La mayoría de investigaciones previas utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de bases de datos para la aplicación de recuperación de documentos distribuidos. Sostenemos que el objetivo de alta recuperación de recursos en la recomendación de bases de datos y el objetivo de alta precisión en la recuperación de documentos están relacionados pero no son idénticos. Este tipo de inconsistencia también ha sido observada en trabajos anteriores, pero las soluciones previas utilizaron métodos heurísticos o asumieron la cooperación de bases de datos individuales (por ejemplo, que todas las bases de datos utilizaran el mismo tipo de motores de búsqueda), lo cual frecuentemente no es cierto en un entorno no cooperativo. En este trabajo proponemos un modelo unificado de maximización de utilidad para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado. En este marco, las decisiones de selección se obtienen optimizando diferentes funciones objetivo. Hasta donde sabemos, este es el primer trabajo que intenta visualizar y modelar teóricamente la tarea de recuperación de información distribuida de manera integrada. El nuevo marco continúa una tendencia reciente de investigación que estudia el uso de muestreo basado en consultas y una base de datos de muestras centralizada. Se entrenó un único modelo logístico en la Tabla 7 centralizada. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). (Los motores de búsqueda no devuelven puntajes de documentos) Precisión en la Clasificación de Documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 documentos 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 documentos 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 documentos 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 documentos 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 base de datos de muestra para estimar las probabilidades de relevancia de documentos por sus puntajes de recuperación centralizados, mientras que la base de datos de muestra centralizada sirve como puente para conectar las bases de datos individuales con el modelo logístico centralizado. Por lo tanto, las probabilidades de relevancia para todos los documentos en las bases de datos pueden ser estimadas con una cantidad muy pequeña de juicio de relevancia humano, lo cual es mucho más eficiente que los métodos anteriores que construyen un modelo separado para cada base de datos. Este marco no solo es más sólido teóricamente, sino también muy efectivo. Un algoritmo para la selección de recursos (UUM/HR) y dos algoritmos para la recuperación de documentos (UUM/HP-FL y UUM/HP-VL) se derivan de este marco. Se han realizado estudios empíricos en bancos de pruebas para simular las soluciones de búsqueda distribuida de grandes organizaciones (empresas) o la Web oculta específica de un dominio. Además, los algoritmos de selección de recursos UUM/HP-FL y UUM/HP-VL se amplían con una variante del algoritmo de fusión de resultados SSL para abordar la tarea de recuperación de documentos distribuidos cuando las bases de datos seleccionadas no devuelven puntuaciones de documentos. Los experimentos han demostrado que estos algoritmos logran resultados que son al menos tan buenos como el estado del arte previo, y a veces considerablemente mejores. Un análisis detallado indica que la ventaja de estos algoritmos proviene de optimizar explícitamente los objetivos de las tareas específicas. El marco unificado de maximización de utilidad está abierto a diferentes extensiones. Cuando el costo está asociado con la búsqueda en las bases de datos en línea, el marco de utilidad puede ajustarse para estimar automáticamente el mejor número de bases de datos a buscar, de modo que se puedan recuperar una gran cantidad de documentos relevantes con costos relativamente bajos. Otra extensión del marco es considerar la efectividad de la recuperación de información de las bases de datos en línea, lo cual es un tema importante en los entornos operativos. Todas estas son las direcciones de la investigación futura. AGRADECIMIENTO Esta investigación fue apoyada por las subvenciones de la NSF EIA-9983253 y IIS-0118767. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este documento son del autor y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] J. Callan. (2000). Recuperación de información distribuida. En W.B. Croft, editor, Avances en Recuperación de Información. Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. \n\nEditorial Kluwer Academic. (pp. 127-150). [2] J. Callan, W.B. Croft, y J. Broglio. (1995). Experimentos TREC y TIPSTER con INQUERY. Procesamiento y Gestión de la Información, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson y M. Meziou. (2002). Selección de base de datos utilizando recursos de colección lógica adquiridos y físicos reales en un entorno operativo masivo específico de dominio. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [4] N. Craswell. (2000). Métodos para la recuperación distribuida de información. I'm sorry, but the sentence \"Ph.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate to Spanish? Tesis doctoral, Universidad Nacional Australiana. [5] N. Craswell, D. Hawking y P. Thistlewaite. (1999). Combinando resultados de motores de búsqueda aislados. En Actas de la 10ª Conferencia de Bases de Datos Australasiana. [6] D. DSouza, J. Thom y J. Zobel. (2000). Una comparación de técnicas para seleccionar colecciones de texto. En Actas de la 11ª Conferencia de Bases de Datos Australasiana. [7] N. Fuhr. (1999). Un enfoque de Teoría de la Decisión para la selección de bases de datos en IR en red. ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina y A. Paepcke. (1997). Propuesta de Stanford para la metabusqueda en internet. En Actas de la 20ª Conferencia Internacional ACM-SIGMOD sobre Gestión de Datos. [9] L. Gravano, P. Ipeirotis y M. Sahami. (2003). QProber: Un sistema para la clasificación automática de bases de datos de la web oculta. ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis y L. Gravano. (2002). Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En Actas de la 28ª Conferencia Internacional sobre Bases de Datos Muy Grandes (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] El kit de herramientas lemur. http://www.cs.cmu.edu/~lemur [13] J. Lu y J. Callan. (2003). Recuperación de información basada en contenido en redes peer-to-peer. En Actas de la 12ª Conferencia Internacional sobre Información y Gestión del Conocimiento. [14] W. Meng, C.T. Yu y K.L. Liu. (2002) Construcción de motores de búsqueda eficientes y efectivos. ACM Comput. Surv. 34(1). [15] H. Nottelmann y N. Fuhr. (2003). Evaluando diferentes métodos para estimar la calidad de recuperación para la selección de recursos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [16] H., Nottelmann y N., Fuhr. (2003). La arquitectura MIND para bibliotecas digitales federadas de multimedia heterogénea. Taller ACM SIGIR 2003 sobre Recuperación de Información Distribuida. [17] A.L. Powell, J.C. French, J. Callan, M. Connell y C.L. Viles. (2000). \n\nViles. (2000). El impacto de la selección de bases de datos en la búsqueda distribuida. En Actas de la 23ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [18] A.L. Powell y J.C. French. (2003). Comparando el rendimiento de los algoritmos de selección de bases de datos. ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). \n\nACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001). Busca en la web invisible. Guardian Unlimited. [20] L. Si y J. Callan. (2002). Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [21] L. Si y J. Callan. (2003). Método de estimación de distribución de documentos relevantes para la selección de recursos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información. [22] L. Si y J. Callan. (2003). Un método de <br>aprendizaje semi-supervisado</br> para fusionar los resultados de un motor de búsqueda. ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41\n\nACM Transactions on Information Systems, 21(4). (pp. 457-491). 41 ",
            "candidates": [],
            "error": [
                [
                    "aprendizaje semisupervisado",
                    "aprendizaje semi-supervisado"
                ]
            ]
        },
        "unified utility maximization model": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new <br>unified utility maximization model</br>.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the <br>unified utility maximization model</br>.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The <br>unified utility maximization model</br> In this section, we formally define the new <br>unified utility maximization model</br>, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the <br>unified utility maximization model</br> only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a <br>unified utility maximization model</br> to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [
                "Section 3 describes the new <br>unified utility maximization model</br>.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the <br>unified utility maximization model</br>.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The <br>unified utility maximization model</br> In this section, we formally define the new <br>unified utility maximization model</br>, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "As the <br>unified utility maximization model</br> only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "In this work we propose a <br>unified utility maximization model</br> to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework."
            ],
            "translated_annotated_samples": [
                "La sección 3 describe el nuevo <br>modelo unificado de maximización de utilidad</br>.",
                "En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el <br>modelo de Maximización de Utilidad Unificado</br>.",
                "Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El <br>Modelo Unificado de Maximización de Utilidad</br> En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco.",
                "Dado que el <br>modelo unificado de maximización de utilidad</br> solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos.",
                "En este trabajo proponemos un <br>modelo unificado de maximización de utilidad</br> para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado."
            ],
            "translated_text": "Marco unificado de maximización de utilidad para la selección de recursos en el Instituto de Tecnología del Lenguaje Luo Si. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Instituto de Tecnología del Lenguaje. Escuela de Ciencias de la Computación de la Universidad Carnegie Mellon, Pittsburgh, PA 15213 callan@cs.cmu.edu RESUMEN Este artículo presenta un marco de utilidad unificado para la selección de recursos de recuperación de información textual distribuida. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos puede realizarse optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se utiliza para la recomendación de bases de datos, la selección se optimiza para el objetivo de alta recuperación (incluyendo tantos documentos relevantes como sea posible en las bases de datos seleccionadas); cuando se utiliza para la recuperación distribuida de documentos, la selección apunta al objetivo de alta precisión (alta precisión en la lista final combinada de documentos). Este nuevo modelo proporciona un marco más sólido para la recuperación distribuida de información. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de vanguardia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Términos Generales Algoritmos 1. INTRODUCCIÓN Los motores de búsqueda convencionales como Google o AltaVista utilizan una solución de recuperación de información ad-hoc al asumir que todos los documentos buscables pueden ser copiados en una base de datos centralizada única con el propósito de indexarlos. La recuperación de información distribuida, también conocida como búsqueda federada, es diferente de la recuperación de información ad-hoc, ya que aborda los casos en los que los documentos no pueden ser adquiridos y almacenados en una sola base de datos. Por ejemplo, los contenidos de la Web oculta (también llamados contenidos invisibles o de la Web profunda) son información en la Web que no puede ser accedida por los motores de búsqueda convencionales. Se estima que el contenido web oculto es de 2 a 50 veces más grande que el contenido que puede ser buscado por los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red local, como en entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o utilizar el mismo tipo de motores de búsqueda. La investigación temprana en recuperación de información distribuida se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia como entornos corporativos muy grandes o en la Web hay muchos tipos de motores de búsqueda y es difícil asumir que todos los proveedores de información puedan cooperar como se requiere. Aunque estén dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una única solución para todos los proveedores de información o detectar si las fuentes de información proporcionan la información correcta según lo requerido. Muchas aplicaciones caen en el último tipo de entornos no cooperativos, como el proyecto Mind [16], que integra bibliotecas digitales no cooperativas, o el sistema QProber [9], que admite la navegación y búsqueda de bases de datos ocultas en la Web no cooperativas. En este artículo, nos enfocamos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. Segundo, dado una consulta, se debe seleccionar un conjunto de recursos para realizar la búsqueda (selección de recursos) [5,7,21]. Tercero, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una lista final única antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net proporciona navegación guiada de bases de datos web ocultas al recopilar las descripciones de recursos de estas bases de datos y construir jerarquías de clases que las agrupan por temas similares. Un sistema de recomendación de bases de datos va un paso más allá que un sistema de navegación como Invisible-web.net al recomendar las fuentes de información más relevantes para las consultas de los usuarios. Está compuesto por la descripción del recurso y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedir al sistema que recupere documentos relevantes automáticamente. La recuperación distribuida de documentos es una tarea más sofisticada. Selecciona fuentes de información relevantes para las consultas de los usuarios, al igual que lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se envían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de bases de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, lo cual llamamos un objetivo de alto recuerdo. Por otro lado, la efectividad de la recuperación distribuida de documentos suele medirse por la Precisión de la lista de resultados finales de documentos fusionados, a la que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no son idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo de selección de recursos efectivo del sistema de recomendación de bases de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco unificado de maximización de utilidad para integrar el problema de selección de recursos tanto de recomendación de bases de datos como de recuperación de documentos distribuidos, tratándolos como objetivos de optimización diferentes. Primero, se construye una base de datos de muestra centralizada mediante el muestreo aleatorio de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas; también se estiman las estadísticas del tamaño de la base de datos. Un modelo de transformación logística se aprende fuera de línea con una pequeña cantidad de consultas de entrenamiento para mapear las puntuaciones de documentos centralizadas en la base de datos de muestra centralizada a las probabilidades correspondientes de relevancia. Segundo, después de que se envía una nueva consulta, la consulta se puede utilizar para buscar en la base de datos de muestras centralizada que produce una puntuación para cada documento muestreado. La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada puede estimarse aplicando el modelo logístico al puntaje de cada documento. Entonces, las probabilidades de relevancia de todos los documentos (en su mayoría no vistos) entre las bases de datos disponibles pueden ser estimadas utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos. Para la tarea de selección de recursos para un sistema de recomendación de bases de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alto recall. Para la selección de recursos para un sistema distribuido de recuperación de documentos, se prefieren las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia. Este criterio de selección cumple con el objetivo de alta precisión de la aplicación de recuperación de documentos distribuidos. Además, se aplica el algoritmo de aprendizaje semisupervisado (SSL) [20,22] para fusionar los documentos devueltos en una lista final clasificada. El marco de utilidad unificado hace muy pocas suposiciones y funciona en entornos no cooperativos. Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos correspondientes; y ii) Muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos. Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades correspondientes de relevancia y utiliza la base de datos de muestra centralizada como puente entre las bases de datos individuales y el modelo logístico. El esfuerzo humano (juicio de relevancia) necesario para entrenar el modelo logístico centralizado único no aumenta con el número de bases de datos. Esta es una gran ventaja sobre investigaciones anteriores, las cuales requerían que la cantidad de esfuerzo humano fuera lineal con el número de bases de datos [7,15]. El marco de utilidad unificada no solo es más sólido teóricamente, sino también muy efectivo. Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de vanguardia en una variedad de configuraciones. La siguiente sección discute el trabajo relacionado. La sección 3 describe el nuevo <br>modelo unificado de maximización de utilidad</br>. La sección 4 explica nuestra metodología experimental. Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos. La sección 7 concluye. 2. Investigación previa Ha habido una considerable investigación sobre todos los subproblemas de la recuperación de información distribuida. Exploramos los trabajos más relacionados en esta sección. El primer problema de la recuperación de información distribuida es la representación de recursos. El protocolo STARTS es una solución para adquirir descripciones de recursos en entornos cooperativos [8]. Sin embargo, en entornos no cooperativos, aunque las bases de datos estén dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no. Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí. Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en consultas, que genera y envía consultas de forma aleatoria a motores de búsqueda individuales y recupera algunos documentos para construir las descripciones. Dado que los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico. Los experimentos han demostrado que descripciones de recursos bastante precisas pueden ser construidas enviando alrededor de 80 consultas y descargando alrededor de 300 documentos [1]. Muchos algoritmos de selección de recursos como gGlOSS/vGlOSS [8] y CORI [1] han sido propuestos en la última década. El algoritmo CORI representa cada base de datos por sus términos, las frecuencias de los documentos y un pequeño número de estadísticas del corpus (detalles en [1]). Como investigaciones previas en diferentes conjuntos de datos han demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo utilizamos como algoritmo base en este trabajo. El algoritmo de selección de recursos de estimación de distribución de documentos relevantes (ReDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia. Aunque se ha demostrado que el algoritmo ReDDE es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21]. El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar puntuaciones de documentos específicas de la base de datos en puntuaciones de documentos independientes de la base de datos comparables. El algoritmo de fusión de resultados de aprendizaje semisupervisado (SSL) [20,22] utiliza los documentos adquiridos mediante muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos y de la consulta. Estos modelos lineales se utilizan para convertir las puntuaciones de documentos específicas de la base de datos en las puntuaciones de documentos centralizadas aproximadas. El algoritmo SSL ha demostrado ser efectivo [22]. Sirve como un componente importante de nuestro marco unificado de maximización de utilidad (Sección 3). Para lograr resultados precisos en la recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos en sistemas de recomendación de bases de datos. Pero como se señaló anteriormente, un algoritmo de selección de recursos optimizado para un alto recuerdo puede no funcionar bien para la recuperación de documentos, que tiene como objetivo la alta precisión. Este tipo de inconsistencia ha sido observada en investigaciones previas [4,21]. La investigación en [21] intentó resolver el problema con un método heurístico. La investigación más similar a lo que proponemos aquí es el marco teórico de la toma de decisiones (DTF) [7,15]. Este marco de trabajo calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y se han propuesto varios métodos [15] para estimar la calidad de recuperación. Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF. Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de bases de datos y recuperación distribuida de documentos) en el mismo marco unificado. Segundo, el DTF construye un modelo para cada base de datos para calcular las probabilidades de relevancia. Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos. Por el contrario, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada. La base de datos de muestra centralizada puede servir como puente para conectar las bases de datos individuales con el modelo logístico centralizado, de esta manera se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos. Esta estrategia puede ahorrar una gran cantidad de esfuerzo en juicio humano y es una gran ventaja del marco de maximización de utilidad unificada sobre el DTF, especialmente cuando hay un gran número de bases de datos. MARCO DE MAXIMIZACIÓN DE UTILIDAD UNIFICADA El marco de Maximización de Utilidad Unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría no vistos) disponibles en el entorno de búsqueda distribuida. En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo son utilizadas por el <br>modelo de Maximización de Utilidad Unificado</br>. También describimos cómo el modelo puede ser optimizado para el objetivo de alto recuerdo de un sistema de recomendación de base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido. 3.1 Estimación de Probabilidades de Relevancia Como se señaló anteriormente, el propósito de la selección de recursos es el alto recuerdo y el propósito de la recuperación de documentos es la alta precisión. Para cumplir con estos objetivos diversos, el problema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos. Este es un problema difícil porque solo podemos observar una muestra de los contenidos de cada base de datos utilizando muestreo basado en consultas. Nuestra estrategia es aprovechar al máximo toda la información disponible para calcular las estimaciones de probabilidad. 3.1.1 Aprendizaje de Probabilidades de Relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se construye mediante muestreo basado en consultas y los tamaños de la base de datos se estiman utilizando el método de muestreo y remuestreo [21]. Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (Inquery [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de entrenamiento. Para cada consulta de entrenamiento, se aplica el algoritmo de selección de recursos CORI [1] para seleccionar un cierto número (por ejemplo, 10) de bases de datos y recuperar 50 identificadores de documentos de cada base de datos. El algoritmo de fusión de resultados SSL [20,22] se utiliza para combinar los resultados. Luego, podemos descargar los 50 documentos principales de la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando Inquery y las estadísticas del corpus de la base de datos de muestra centralizada. Las puntuaciones centralizadas se normalizan aún más (dividiéndolas por la puntuación centralizada máxima para cada consulta), ya que este método ha sido sugerido para mejorar la precisión de la estimación en investigaciones anteriores [15]. El juicio humano se adquiere para esos documentos y se construye un modelo logístico para transformar las puntuaciones de documentos centralizados normalizados en probabilidades de relevancia de la siguiente manera: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) donde )( _ dSc es la puntuación de documento centralizada normalizada y ac y bc son los dos parámetros del modelo logístico. Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de entrenamiento. El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de las puntuaciones de documentos centralizadas. 3.1.2 Estimación de las puntuaciones de documentos centralizadas Cuando el usuario envía una nueva consulta, se calculan las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar las puntuaciones de los documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados. Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas del tamaño de la base de datos. Definimos el factor de escala de la base de datos para la base de datos i como la razón entre el tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: SF_i = ^N_db / _N_db_samp_i donde ^N_db es el tamaño estimado de la base de datos y _N_db_samp_i es el número de documentos de la base de datos i en la base de datos de muestra centralizada. La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podríamos suponer que hay alrededor de 50 documentos en esa base de datos que tienen puntuaciones de alrededor de 0.5. De hecho, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntuación del documento centralizado para cada base de datos. Formalmente, clasificamos todos los documentos muestreados de la base de datos i-ésima por sus puntajes de documento centralizado 34 para obtener la lista de puntajes de documento centralizado muestreado {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} para la base de datos i; asumimos que si pudiéramos calcular los puntajes de documento centralizado para todos los documentos en esta base de datos y obtener la lista completa de puntajes de documento centralizado, el documento superior en la lista muestreada tendría un rango de SFdbi/2, el segundo documento en la lista muestreada tendría un rango de SFdbi3/2, y así sucesivamente. Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. La interpolación lineal por tramos se aplica para estimar la curva de puntuación del documento centralizado, como se ilustra en la Figura 1. La lista completa de puntuaciones de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documentos centralizados como: ],1[,)(S ^^ c idbij Njd ∈ . Se puede observar en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntuación del documento centralizado. Sin embargo, para bases de datos con grandes proporciones de escala de base de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos mejor clasificados (por ejemplo, [1, SFdbi/2]). Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizados de los documentos mejor clasificados para bases de datos con ratios a gran escala (por ejemplo, mayores de 100). Específicamente, se construye un modelo logístico para cada una de estas bases de datos. El modelo logístico se utiliza para estimar la puntuación del documento centralizado superior 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con las puntuaciones centralizadas más altas. 0iα , 1iα y 2iα son los parámetros del modelo logístico. Para cada consulta de entrenamiento, se descarga el documento mejor recuperado de cada base de datos y se calcula la puntuación del documento centralizado correspondiente. Junto con las puntuaciones de los dos documentos muestreados principales, estos parámetros pueden ser estimados. Después de estimar la puntuación centralizada del documento principal, se ajusta una función exponencial para la parte superior ([1, SFdbi/2]) de la curva de puntuación del documento centralizado como: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) Los dos parámetros 0iβ y 1iβ se ajustan para asegurarse de que la función exponencial pase por los dos puntos (1, ^ 1)( ic dS ) y (SFdbi/2, Sc(dsi1)). La función exponencial se utiliza únicamente para ajustar la parte superior de la curva de puntuación del documento centralizado, mientras que la parte inferior de la curva sigue siendo ajustada con el método de interpolación lineal descrito anteriormente. El ajuste mediante la función exponencial de los documentos mejor clasificados ha demostrado empíricamente producir resultados más precisos. A partir de las curvas de puntuación de documentos centralizadas, podemos estimar las listas completas de puntuación de documentos centralizados correspondientes para todas las bases de datos disponibles. Después de que las puntuaciones estimadas de los documentos centralizados se normalizan, las listas completas de probabilidades de relevancia pueden ser construidas a partir de las listas completas de puntuaciones de documentos centralizados mediante la Ecuación 1. Formalmente, para la i-ésima base de datos, la lista completa de probabilidades de relevancia es: ],1[,)(R ^^ idbij Njd ∈. 3.2 El <br>Modelo Unificado de Maximización de Utilidad</br> En esta sección, definimos formalmente el nuevo modelo unificado de maximización de utilidad, que optimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de bases de datos) y alta precisión (recuperación de documentos distribuidos) en el mismo marco. En la tarea de recomendación de bases de datos, el sistema necesita decidir cómo clasificar las bases de datos. En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada. Generalizamos el proceso de selección de recomendaciones de bases de datos, que implícitamente recomienda todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos. Formalmente, denotamos di como el número de documentos que nos gustaría recuperar de la base de datos i y ,.....},{ 21 ddd = como una acción de selección para todas las bases de datos. La decisión de selección de la base de datos se toma en base a las listas completas de probabilidades de relevancia para todas las bases de datos. Las listas completas de probabilidades de relevancia se infieren a partir de toda la información disponible, específicamente sR, que representa las descripciones de recursos adquiridas mediante muestreo basado en consultas y las estimaciones del tamaño de la base de datos adquiridas mediante muestreo-resampleo; cS representa las puntuaciones de documentos centralizadas de los documentos en la base de datos de muestra centralizada. Si el método de estimación de puntajes de documentos centralizados y probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈. El vector aleatorio   denota un conjunto arbitrario de listas completas de probabilidades de relevancia y ),|( cs SRP θ como la probabilidad de generar este conjunto de listas. Finalmente, a cada acción de selección d y un conjunto de listas completas de la Figura 1. Construcción de la lista completa de puntuación de documentos centralizada mediante interpolación lineal (el factor de escala de la base de datos es 50). Para 35 probabilidades de relevancia θ, asociamos una función de utilidad ),( dU θ que indica el beneficio de realizar la selección d cuando las verdaderas listas completas de probabilidades de relevancia son θ. Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7). Un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en los valores de parámetros más probables en lugar de calcular toda la expectativa. En otras palabras, solo necesitamos calcular ),( * dU θ y la Ecuación 7 se simplifica de la siguiente manera: ),(maxarg * * θdUd d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de bases de datos como para el sistema de recuperación de documentos. 3.3 Selección de Recursos para Alto Recuerdo Alto recuerdo es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recomendación de bases de datos. El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de N bases de datos de Nsdb) que contengan tantos documentos relevantes como sea posible, lo cual puede definirse formalmente como: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) es la función indicadora, que es 1 cuando se selecciona la i-ésima base de datos y 0 en caso contrario. Inserta esta ecuación en el modelo básico de la Ecuación 8 y asocia la restricción del número de base de datos seleccionado para obtener lo siguiente: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) La solución de este problema de optimización es muy simple. Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) Las bases de datos Nsdb con el mayor número esperado de documentos relevantes pueden ser seleccionadas para cumplir con el objetivo de alto recall. Llamamos a esto el algoritmo UUM/HR (Maximización Unificada de Utilidad para Alta Recuperación). 3.4 Selección de Recursos para Alta Precisión La alta precisión es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada como la recuperación distribuida de documentos. Se mide mediante la Precisión en la parte superior de la lista final de documentos fusionados. Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la Precisión de los documentos recuperados de las bases de datos seleccionadas. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Tenga en cuenta que la diferencia clave entre la Ecuación 12 y la Ecuación 9 es que la Ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos, mientras que la Ecuación 12 solo considera una parte mucho más pequeña de la clasificación. Específicamente, podemos calcular la decisión de selección óptima mediante: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Diferentes tipos de restricciones causadas por las diferentes características de las tareas de recuperación de documentos pueden estar asociadas con el problema de optimización anterior. La más común es seleccionar un número fijo (Nsdb) de bases de datos y recuperar un número fijo (Nrdoc) de documentos de cada base de datos seleccionada, definido formalmente como: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) Este problema de optimización puede resolverse fácilmente calculando el número de documentos relevantes esperados en la parte superior de la lista completa de probabilidades de relevancia de cada base de datos: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Luego, las bases de datos pueden ser clasificadas por estos valores y seleccionadas. Llamamos a este algoritmo UUM/HP-FL (Maximización Unificada de Utilidad para Alta Precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada). Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada. Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes cantidades de documentos. Para simplificar, se requiere que las longitudes de la lista de resultados sean múltiplos de un número base 10. (Este valor también puede variar, pero para simplificar se establece en 10 en este documento). Esta restricción está establecida para simular el comportamiento de los motores de búsqueda comerciales en la web. (Motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 identificadores de documentos por página de resultados). Este procedimiento ahorra tiempo de cálculo al calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea de 10 en lugar de 1 (más detalles se discuten posteriormente). Para una mayor simplificación, restringimos la selección a un máximo de 100 documentos de cada base de datos (di<=100). Luego, el problema de optimización de la selección se formaliza de la siguiente manera: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc es el número total de documentos a recuperar. Desafortunadamente, no hay una solución simple para este problema de optimización como la hay para las Ecuaciones 10 y 14. Sin embargo, se puede aplicar un algoritmo de programación dinámica de 36 para calcular la solución óptima. Los pasos básicos de este método de programación dinámica se describen en la Figura 2. Dado que este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se le llama algoritmo UUM/HP-VL. Después de que se toman las decisiones de selección, se buscan las bases de datos seleccionadas y se recuperan los identificadores de documentos correspondientes de cada base de datos. El paso final de la recuperación de documentos es fusionar los resultados devueltos en una única lista clasificada con el algoritmo de aprendizaje semisupervisado. Se señaló anteriormente que el algoritmo SSL mapea las puntuaciones específicas de la base de datos en las puntuaciones de documentos centralizadas y construye la lista clasificada final en consecuencia, lo cual es consistente con todos nuestros procedimientos de selección donde se seleccionan los documentos con mayores probabilidades de relevancia (y por ende, puntuaciones de documentos centralizadas más altas). 4. METODOLOGÍA EXPERIMENTAL 4.1 Bancos de pruebas Es deseable evaluar algoritmos de recuperación de información distribuida con bancos de pruebas que simulen de cerca las aplicaciones del mundo real. Las colecciones web TREC WT2g o WT10g proporcionan una forma de dividir los documentos por diferentes servidores web. De esta manera, se podrían crear un gran número (O(1000)) de bases de datos con contenidos bastante diversos, lo que podría convertir a este banco de pruebas en un buen candidato para simular entornos operativos como la web oculta de dominio abierto. Sin embargo, dos debilidades de este banco de pruebas son: i) Cada base de datos contiene solo una pequeña cantidad de documentos (259 documentos en promedio para WT2g) [4]; y ii) El contenido de WT2g o WT10g se extrae arbitrariamente de la web. No es probable que una base de datos web oculta proporcione páginas personales o páginas web que indiquen que las páginas están en construcción y no contengan información útil en absoluto. Estos tipos de páginas web están contenidos en los conjuntos de datos WT2g/WT10g. Por lo tanto, los datos ruidosos de la Web no son similares a los contenidos de alta calidad de las bases de datos ocultas de la Web, que generalmente están organizados por expertos en el dominio. Otra opción es los datos de noticias/gobierno de TREC [1,15,17,18,21]. Los datos gubernamentales/noticias de TREC se centran en temas relativamente específicos. Comparado con los datos web de TREC: i) Los documentos de noticias/gobierno son mucho más similares a los contenidos proporcionados por una base de datos orientada a temas que a una página web arbitraria, ii) Una base de datos en este banco de pruebas es más grande que la de los datos web de TREC. En promedio, una base de datos contiene miles de documentos, lo cual es más realista que una base de datos de datos web de TREC con alrededor de 250 documentos. Dado que los contenidos y tamaños de las bases de datos en el banco de pruebas de noticias/gobierno de TREC son más similares a los de una base de datos orientada a temas, es un buen candidato para simular los entornos de recuperación de información distribuida de grandes organizaciones (empresas) o sitios web ocultos específicos de dominio, como West, que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3]. Dado que la mayoría de los sistemas actuales de recuperación de información distribuida están desarrollados para entornos de grandes organizaciones (empresas) o para la Web oculta de dominios específicos en lugar de la Web oculta de dominio abierto, en este trabajo se eligió el banco de pruebas de noticias/gobierno de TREC. El banco de pruebas Trec123-100col-bysource es uno de los más utilizados en las pruebas de noticias y gobierno de TREC [1,15,17,21]. Fue elegido en este trabajo. Tres bancos de pruebas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes también se utilizaron para proporcionar una simulación más exhaustiva para entornos reales. Se crearon 100 bases de datos a partir de los CDs de TREC 1, 2 y 3. Fueron organizados por fuente y fecha de publicación [1]. Los tamaños de las bases de datos no están sesgados. Los detalles se encuentran en la Tabla 1. Tres bancos de pruebas construidos en [21] se basaron en el banco de pruebas trec123-100colbysource. Cada banco de pruebas contiene muchas bases de datos pequeñas y dos bases de datos grandes creadas al fusionar alrededor de 10 a 20 bases de datos pequeñas. Listas completas de probabilidades de relevancia para todas las bases de datos |DB|. Solución de selección óptima para la Ecuación 16. i) Crear el arreglo tridimensional: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Cada Sel (x, y, z) está asociado con una decisión de selección xyzd, que representa la mejor decisión de selección en la condición: solo se consideran bases de datos del número 1 al número x para la selección; se recuperarán un total de y*10 documentos; solo se seleccionan z bases de datos de los candidatos de la base de datos x. Y Sel (x, y, z) es el valor de utilidad correspondiente al elegir la mejor selección. ii) Inicializar Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) solo con la información de relevancia estimada de la 1ª base de datos. iii) Iterar el candidato actual de la base de datos i desde 2 hasta |DB| Para cada entrada Sel (i, y, z): Encontrar k tal que: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k∗ documentos de la base de datos i-ésima, de lo contrario no debemos seleccionar esta base de datos y se debe mantener la solución anterior mejor Sel (i-1, y, z). Luego establezca el valor de iyzd y Sel (i, y, z) en consecuencia. iv) La mejor solución de selección se da por _ /10| | Toral rdoc sdbDB N Nd y el valor de utilidad correspondiente es Sel (|DB|, NTotal_rdoc/10, Nsdb). Figura 2. El procedimiento de optimización de programación dinámica para la Ecuación 16. Tabla 1: Estadísticas del banco de pruebas. Número de documentos Tamaño (MB) Tamaño del banco de pruebas (GB) Mínimo Promedio Máximo Mínimo Promedio Máximo Trec123 3.2 752 10782 39713 28 32 42 Tabla 2: Estadísticas del conjunto de consultas. Nombre del conjunto de temas TREC Campo del tema TREC Longitud promedio (palabras) Trec123 51-150 Título 3.1 37 Trec123-2ldb-60col (representativo): Las bases de datos en el trec123-100col-bysource se ordenaron en orden alfabético. Dos grandes bases de datos fueron creadas al fusionar 20 bases de datos pequeñas con el método de round-robin. Por lo tanto, las dos bases de datos grandes tienen más documentos relevantes debido a sus tamaños grandes, aunque las densidades de documentos relevantes son aproximadamente iguales a las de las bases de datos pequeñas. Las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, APall y WSJall. Las otras 60 colecciones quedaron sin cambios. Las bases de datos APall y WSJall tienen una mayor densidad de documentos relevantes para las consultas de TREC que las bases de datos pequeñas. Por lo tanto, las dos bases de datos grandes tienen muchos más documentos relevantes que las bases de datos pequeñas. Las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el banco de pruebas trec123-100col-bysource se fusionaron en dos grandes bases de datos, FRall y DOEall. Las otras 80 colecciones quedaron sin cambios. Las bases de datos FRall y DOEall tienen densidades más bajas de documentos relevantes para las consultas de TREC que las bases de datos pequeñas, a pesar de ser mucho más grandes. Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150. Las consultas 101-150 se utilizaron como consultas de entrenamiento y las consultas 51-100 se utilizaron como consultas de prueba (detalles en la Tabla 2). 4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o en la Web oculta específica de dominio, diferentes bases de datos pueden utilizar diferentes tipos de motores de búsqueda. Para simular el entorno de múltiples motores de búsqueda, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: INQUERY [2], un modelo de lenguaje estadístico de unigrama con suavizado lineal [12,20] y un algoritmo de recuperación TFIDF con peso ltc [12,20]. Todos estos algoritmos fueron implementados con la herramienta Lemur [12]. Estos tres tipos de motores de búsqueda fueron asignados a las bases de datos entre los cuatro bancos de pruebas de manera round-robin. 5. RESULTADOS: SELECCIÓN DE RECURSOS DE LA RECOMENDACIÓN DE BASES DE DATOS Todos los cuatro bancos de pruebas descritos en la Sección 4 fueron utilizados en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de bases de datos. Las descripciones de los recursos fueron creadas utilizando muestreo basado en consultas. Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos. Las estadísticas del tamaño de la base de datos fueron estimadas mediante el método de muestra y remuestra [21]. Cincuenta consultas (101-150) se utilizaron como consultas de entrenamiento para construir el modelo logístico relevante y ajustar las funciones exponenciales de las curvas de puntuación de documentos centralizados para bases de datos de gran proporción (detalles en la Sección 3.1). Otros 50 consultas (51-100) se utilizaron como datos de prueba. Los algoritmos de selección de recursos de los sistemas de recomendación de bases de datos suelen compararse utilizando la métrica de recuperación nR [1,17,18,21]. Que B denote una clasificación base, que a menudo es la RBR (clasificación basada en relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos. Y que Bi y Ei denoten el número de documentos relevantes en la base de datos clasificada i-ésima de B o E. Entonces, Rn se define de la siguiente manera: = = = k i i k i i k B E R 1 1 (17) Por lo general, el objetivo es buscar solo algunas bases de datos, por lo que nuestras cifras solo muestran resultados para la selección de hasta 20 bases de datos. Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, CORI, ReDDE y UUM/HR. El algoritmo UUM/HR se describe en la Sección 3.3. Se puede observar en la Figura 3 que los algoritmos ReDDE y UUM/HR son más efectivos (en los conjuntos de pruebas representativos, relevantes y no relevantes) o igual de efectivos (en el conjunto de pruebas Trec123-100Col) que el algoritmo de selección de recursos CORI. El algoritmo UUM/HR es más efectivo que el algoritmo ReDDE en los conjuntos de pruebas representativos y relevantes y es aproximadamente igual que el algoritmo ReDDE en los conjuntos de pruebas Trec123100Col y no relevantes. Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo ReDDE. Se puede observar que al seleccionar solo algunas bases de datos en el Trec123-100Col o en los conjuntos de pruebas no relevantes, el algoritmo ReDEE tiene una pequeña ventaja sobre el algoritmo UUM/HR. Atribuimos esto a dos causas: i) El algoritmo ReDDE fue ajustado en el banco de pruebas Trec123-100Col; y ii) Aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico para estimar probabilidades de relevancia no es lo suficientemente preciso. Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este pequeño rompecabezas. Colecciones seleccionadas. Colecciones seleccionadas. Plataforma de pruebas Trec123-100Col. Plataforma de pruebas representativa. Colección seleccionada. Colección seleccionada. Plataforma de pruebas relevante. Plataforma de pruebas no relevante. Figura 3. Experimentos de selección de recursos en los cuatro bancos de pruebas. 38 6. RESULTADOS: EFECTIVIDAD DE LA RECUPERACIÓN DE DOCUMENTOS Para la recuperación de documentos, se buscan en las bases de datos seleccionadas y los resultados devueltos se fusionan en una lista final única. En todos los experimentos discutidos en esta sección, los resultados obtenidos de bases de datos individuales fueron combinados por el algoritmo de fusión de resultados de aprendizaje semisupervisado. Esta versión del algoritmo SSL [22] tiene permitido descargar un pequeño número de textos de documentos devueltos sobre la marcha para crear datos de entrenamiento adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicos de la base de datos en puntuaciones de documentos centralizadas estimadas. Se ha demostrado ser muy efectivo en entornos donde solo se obtienen listas de resultados cortas de cada base de datos seleccionada [22]. Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos. La efectividad de la recuperación de documentos se midió mediante la Precisión en la parte superior de la lista final de documentos. Los experimentos en esta sección se llevaron a cabo para estudiar la efectividad de recuperación de documentos de cinco algoritmos de selección, a saber, los algoritmos CORI, ReDDE, UUM/HR, UUM/HP-FL y UUM/HP-VL. Los últimos tres algoritmos fueron propuestos en la Sección 3. Todos los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada. El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos a recuperar de cada base de datos seleccionada; el número recuperado estaba limitado a ser de 10 a 100, y un múltiplo de 10. El Trec123-100Col y los bancos de pruebas representativos fueron seleccionados para la recuperación de documentos, ya que representan dos casos extremos de efectividad en la selección de recursos; en un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante Tabla 5. Precisión en el banco de pruebas representativo cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 documentos 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 documentos 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 documentos 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 documentos 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Tabla 6. Precisión en el banco de pruebas representativo cuando se seleccionaron 5 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 documentos 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 documentos 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 documentos 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 documentos 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Tabla 3. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 3 bases de datos. (La primera línea base es CORI; la segunda línea base para los métodos UUM/HP es UUM/HR). Precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 documentos 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 documentos 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 documentos 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 documentos 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Tabla 4. Precisión en el banco de pruebas trec123-100col-bysource cuando se seleccionaron 5 bases de datos. (El primer punto de referencia es CORI; el segundo punto de referencia para los métodos UUM/HP es UUM/HR). La precisión en la clasificación de documentos CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 documentos 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 documentos 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 documentos 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 documentos 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 documentos 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 mucho peor que los otros algoritmos. Las Tablas 3 y 4 muestran los resultados en el banco de pruebas Trec123-100Col, y las Tablas 5 y 6 muestran los resultados en el banco de pruebas representativo. En el banco de pruebas Trec123-100Col, la efectividad de recuperación de documentos del algoritmo de selección CORI es aproximadamente la misma o un poco mejor que el algoritmo ReDDE, pero ambos son peores que los otros tres algoritmos (Tablas 3 y 4). El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos CORI y ReDDE. Una de las principales diferencias entre el algoritmo UUM/HR y el algoritmo ReDDE fue señalada anteriormente: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntuación de documentos centralizadas, mientras que el algoritmo ReDDE [21] utiliza un método heurístico, asume que las curvas de puntuación de documentos centralizadas son funciones escalonadas y no hace distinción entre la parte superior de las curvas. Esta diferencia hace que UUM/HR sea mejor que el algoritmo ReDDE para distinguir documentos con altas probabilidades de relevancia de aquéllos con bajas probabilidades de relevancia. Por lo tanto, el UUM/HR refleja mejor el objetivo de recuperación de alta precisión que el algoritmo ReDDE y, por lo tanto, es más efectivo para la recuperación de documentos. El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, como lo hacen los algoritmos UUM/HP-FL y UUM/HP-VL. Se puede observar que en este banco de pruebas, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos. Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de recuperación de documentos en las Ecuaciones 14 y 16. En el banco de pruebas representativo, CORI es mucho menos efectivo que otros algoritmos para la recuperación distribuida de documentos (Tablas 5 y 6). Los resultados de recuperación de documentos del algoritmo ReDDE son mejores que los del algoritmo CORI pero aún peores que los resultados del algoritmo UUM/HR. En este banco de pruebas, los tres algoritmos de UUM son aproximadamente igual de efectivos. Un análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho mayor que los experimentos en el banco de pruebas Trec123-100Col, ya que todos tienden a seleccionar las dos bases de datos grandes. Esto explica por qué son igualmente efectivos para la recuperación de documentos. En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos y reportar solo listas clasificadas de resultados. Dado que el <br>modelo unificado de maximización de utilidad</br> solo utiliza las puntuaciones de recuperación de los documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin hacer referencia a las puntuaciones de los documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de clasificación sin puntuaciones de documentos. El único ajuste es que el algoritmo SSL fusiona listas clasificadas sin puntuaciones de documentos asignando a los documentos puntuaciones de pseudo-documentos normalizadas por sus rangos (En una lista clasificada de 50 documentos, el primero tiene una puntuación de 1, el segundo tiene una puntuación de 0.98, etc.), lo cual ha sido estudiado en [22]. Los resultados del experimento en el banco de pruebas trec123-100Col-bysource con 3 bases de datos seleccionadas se muestran en la Tabla 7. La configuración del experimento fue la misma que antes, excepto que las puntuaciones de los documentos fueron eliminadas intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de identificadores de documentos. Se puede observar en los resultados que el UUM/HP-FL y el UUM/HP-VL funcionan bien con bases de datos que no devuelven puntuaciones de documentos y siguen siendo más efectivos que otras alternativas. Otros experimentos con bases de datos que no devuelven puntuaciones de documentos no se informan, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL. Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión de manera explícita en la recuperación de documentos. Los nuevos algoritmos basados en este principio logran resultados mejores o al menos tan buenos como los algoritmos previos de vanguardia en varios entornos. CONCLUSIÓN La recuperación distribuida de información resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e Internet. La mayoría de investigaciones previas utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de bases de datos para la aplicación de recuperación de documentos distribuidos. Sostenemos que el objetivo de alta recuperación de recursos en la recomendación de bases de datos y el objetivo de alta precisión en la recuperación de documentos están relacionados pero no son idénticos. Este tipo de inconsistencia también ha sido observada en trabajos anteriores, pero las soluciones previas utilizaron métodos heurísticos o asumieron la cooperación de bases de datos individuales (por ejemplo, que todas las bases de datos utilizaran el mismo tipo de motores de búsqueda), lo cual frecuentemente no es cierto en un entorno no cooperativo. En este trabajo proponemos un <br>modelo unificado de maximización de utilidad</br> para integrar la selección de recursos de recomendación de bases de datos y tareas de recuperación de documentos en un marco unificado. ",
            "candidates": [],
            "error": [
                [
                    "modelo unificado de maximización de utilidad",
                    "modelo de Maximización de Utilidad Unificado",
                    "Modelo Unificado de Maximización de Utilidad",
                    "modelo unificado de maximización de utilidad",
                    "modelo unificado de maximización de utilidad"
                ]
            ]
        },
        "distribute information retrieval": {
            "translated_key": "distribución de recuperación de información",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Unified Utility Maximization Framework for Resource Selection Luo Si Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst.",
                "School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu ABSTRACT This paper presents a unified utility framework for resource selection of distributed text information retrieval.",
                "This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.",
                "With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.",
                "Specifically, when used for database recommendation, the selection is optimized for the goal of highrecall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents).",
                "This new model provides a more solid framework for distributed information retrieval.",
                "Empirical studies show that it is at least as effective as other state-of-the-art algorithms.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: General Terms Algorithms 1.",
                "INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing.",
                "Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database.",
                "For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.",
                "Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines.",
                "Therefore, it is very important to search this type of valuable information.",
                "The architecture of distributed search solution is highly influenced by different environmental characteristics.",
                "In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines.",
                "Early distributed information retrieval research focused on this type of cooperative environments [1,8].",
                "On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required.",
                "Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.",
                "Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases.",
                "In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.",
                "There are three important sub-problems in distributed information retrieval.",
                "First, information about the contents of each individual database must be acquired (resource representation) [1,8,21].",
                "Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21].",
                "Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].",
                "Many types of solutions exist for distributed information retrieval.",
                "Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics.",
                "A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users queries.",
                "It is composed of the resource description and the resource selection components.",
                "This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically.",
                "Distributed document retrieval is a more sophisticated task.",
                "It selects relevant information sources for users queries as the database recommendation system does.",
                "Furthermore, users queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.",
                "The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal.",
                "On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal.",
                "Prior research indicated that these two goals are related but not identical [4,21].",
                "However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].",
                "This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.",
                "First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21].",
                "A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.",
                "Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document.",
                "The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each documents score.",
                "Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.",
                "For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal.",
                "For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance.",
                "This selection criterion meets the high-precision goal of distributed document retrieval application.",
                "Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.",
                "The unified utility framework makes very few assumptions and works in uncooperative environments.",
                "Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases.",
                "Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model.",
                "The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases.",
                "This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].",
                "The unified utility framework is not only more theoretically solid but also very effective.",
                "Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.",
                "The next section discusses related work.",
                "Section 3 describes the new unified utility maximization model.",
                "Section 4 explains our experimental methodology.",
                "Sections 5 and 6 present our experimental results for resource selection and document retrieval.",
                "Section 7 concludes. 2.",
                "PRIOR RESEARCH There has been considerable research on all the sub-problems of distributed information retrieval.",
                "We survey the most related work in this section.",
                "The first problem of distributed information retrieval is resource representation.",
                "The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].",
                "However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not.",
                "Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other.",
                "Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions.",
                "As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.",
                "Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].",
                "Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade.",
                "The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]).",
                "As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work.",
                "The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly.",
                "Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].",
                "The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores.",
                "The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models.",
                "These linear models are used to convert the database-specific document scores into the approximated centralized document scores.",
                "The SSL algorithm has been shown to be effective [22].",
                "It serves as an important component of our unified utility maximization framework (Section 3).",
                "In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system.",
                "But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal.",
                "This type of inconsistency has been observed in previous research [4,21].",
                "The research in [21] tried to solve the problem with a heuristic method.",
                "The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15].",
                "This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality.",
                "However, two points distinguish our research from the DTF model.",
                "First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework.",
                "Second, the DTF builds a model for each database to calculate the probabilities of relevance.",
                "This requires human relevance judgments for the results retrieved from each database.",
                "In contrast, our approach only builds one logistic model for the centralized sample database.",
                "The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated.",
                "This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases. 3.",
                "UNIFIED UTILITY MAXIMIZATION FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment.",
                "In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model.",
                "We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system. 3.1 Estimating Probabilities of Relevance As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision.",
                "In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.",
                "This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling.",
                "Our strategy is to make full use of all the available information to calculate the probability estimates. 3.1.1 Learning Probabilities of Relevance In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21].",
                "At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries.",
                "For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database.",
                "The SSL results merging algorithm [20,22] is used to merge the results.",
                "Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database.",
                "The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15].",
                "Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model.",
                "These two parameters are estimated by maximizing the probabilities of relevance of the training queries.",
                "The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores. 3.1.2 Estimating Centralized Document Scores When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated.",
                "However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents.",
                "This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.",
                "We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database.",
                "The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about 0.5.",
                "Actually, we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database.",
                "Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on.",
                "Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}.",
                "Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1.",
                "The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .",
                "It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves.",
                "However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.",
                "Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).",
                "Specifically, a logistic model is built for each of these databases.",
                "The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model.",
                "For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated.",
                "Together with the scores of the top two sampled documents, these parameters can be estimated.",
                "After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^ 0 1 1log( ( ))i c i iS dβ β= − (5) )12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)).",
                "The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above.",
                "The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.",
                "From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases.",
                "After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1.",
                "Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ . 3.2 The Unified Utility Maximization Model In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.",
                "In the task of database recommendation, the system needs to decide how to rank databases.",
                "In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database.",
                "We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task.",
                "Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.",
                "The database selection decision is made based on the complete lists of probabilities of relevance for all the databases.",
                "The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.",
                "If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ .",
                "Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.",
                "Finally, to each selection action d and a set of complete lists of Figure 1.",
                "Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .",
                "Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation.",
                "In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system. 3.3 Resource Selection for High-Recall High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation.",
                "The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise.",
                "Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple.",
                "We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal.",
                "We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall). 3.4 Resource Selection for High-Precision High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval.",
                "It is measured by the Precision at the top part of the final merged document list.",
                "This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database, while Equation 12 only considers a much smaller part of the ranking.",
                "Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem.",
                "The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each databases complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.",
                "We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).",
                "A more complex situation is to vary the number of retrieved documents from each selected database.",
                "More specifically, we allow different selected databases to return different numbers of documents.",
                "For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.)",
                "This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.)",
                "This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly).",
                "For further simplification, we restrict to select at most 100 documents from each database (di<=100) Then, the selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.",
                "Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14.",
                "However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution.",
                "The basic steps of this dynamic programming method are described in Figure 2.",
                "As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.",
                "After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database.",
                "The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm.",
                "It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected. 4.",
                "EXPERIMENTAL METHODOLOGY 4.1 Testbeds It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.",
                "The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers.",
                "In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web.",
                "However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web.",
                "It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all.",
                "These types of web pages are contained in the WT2g/WT10g datasets.",
                "Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.",
                "Another choice is the TREC news/government data [1,15,17, 18,21].",
                "TREC news/government data is concentrated on relatively narrow topics.",
                "Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data.",
                "By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents.",
                "As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3].",
                "As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web, TREC news/government testbed was chosen in this work.",
                "Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21].",
                "It was chosen in this work.",
                "Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.",
                "Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3.",
                "They were organized by source and publication date [1].",
                "The sizes of the databases are not skewed.",
                "Details are in Table 1.",
                "Three testbeds built in [21] were based on the trec123-100colbysource testbed.",
                "Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.",
                "Input: Complete lists of probabilities of relevance for all the |DB| databases.",
                "Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates.",
                "And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve * 10 k∗ documents from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.",
                "Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|, NTotal_rdoc/10, Nsdb).",
                "Figure 2.",
                "The dynamic programming optimization procedure for Equation 16.",
                "Table1: Testbed statistics.",
                "Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.",
                "Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.",
                "Two large databases were created by merging 20 small databases with the round-robin method.",
                "Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.",
                "Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall.",
                "The other 60 collections were left unchanged.",
                "The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases.",
                "Thus, the two large databases have many more relevant documents than the small databases.",
                "Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall.",
                "The other 80 collections were left unchanged.",
                "The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger. 100 queries were created from the title fields of TREC topics 51-150.",
                "The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2). 4.2 Search Engines In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine.",
                "To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20].",
                "All these algorithms were implemented with the Lemur toolkit [12].",
                "These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner. 5.",
                "RESULTS: RESOURCE SELECTION OF DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.",
                "The resource descriptions were created using query-based sampling.",
                "About 80 queries were sent to each database to download 300 unique documents.",
                "The database size statistics were estimated by the sample-resample method [21].",
                "Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).",
                "Another 50 queries (51-100) were used as test data.",
                "Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].",
                "Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.",
                "And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.",
                "The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.",
                "The UUM/HR algorithm is described in Section 3.3.",
                "It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.",
                "The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds.",
                "This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.",
                "It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.",
                "We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.",
                "More training data or a more sophisticated model may help to solve this minor puzzle.",
                "Collections Selected.",
                "Collections Selected.",
                "Trec123-100Col Testbed.",
                "Representative Testbed.",
                "Collection Selected.",
                "Collection Selected.",
                "Relevant Testbed.",
                "Nonrelevant Testbed.",
                "Figure 3.",
                "Resource selection experiments on the four testbeds. 38 6.",
                "RESULTS: DOCUMENT RETRIEVAL EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list.",
                "In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm.",
                "This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores.",
                "It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22].",
                "This is a common scenario in operational environments and was the case for our experiments.",
                "Document retrieval effectiveness was measured by Precision at the top part of the final document list.",
                "The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI, ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.",
                "The last three algorithms were proposed in Section 3.",
                "All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database.",
                "The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.",
                "The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5.",
                "Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%) 10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%) 15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2) 20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%) 30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%) Table 6.",
                "Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%) 10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%) 15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%) 20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%) 30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%) Table 3.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%) 10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%) 15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%) 20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%) 30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%) Table 4.",
                "Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.)",
                "Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%) 10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%) 15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%) 20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%) 30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%) 39 a lot worse than the other algorithms.",
                "Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.",
                "On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).",
                "The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms.",
                "One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves.",
                "This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance.",
                "Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.",
                "The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.",
                "It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms.",
                "This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.",
                "On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6).",
                "The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm.",
                "On this testbed the three UUM algorithms are about equally effective.",
                "Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases.",
                "This explains why they are about equally effective for document retrieval.",
                "In real operational environments, databases may return no document scores and report only ranked lists of results.",
                "As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores.",
                "The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22].",
                "The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7.",
                "The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids.",
                "It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives.",
                "Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.",
                "The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval.",
                "The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments. 7.",
                "CONCLUSION Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets.",
                "Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application.",
                "We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical.",
                "This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.",
                "In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework.",
                "In this framework, the selection decisions are obtained by optimizing different objective functions.",
                "As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.",
                "The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database.",
                "A single logistic model was trained on the centralized Table 7.",
                "Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL 5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8) 10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%) 15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%) 20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%) 30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%) 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.",
                "Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.",
                "This framework is not only more theoretically solid but also very effective.",
                "One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework.",
                "Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web.",
                "Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores.",
                "Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better.",
                "Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.",
                "The unified utility maximization framework is open for different extensions.",
                "When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs.",
                "Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments.",
                "All of these are the directions of future research.",
                "ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767.",
                "Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsor.",
                "REFERENCES [1] J. Callan. (2000).",
                "Distributed information retrieval.",
                "In W.B.",
                "Croft, editor, Advances in Information Retrieval.",
                "Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B.",
                "Croft, and J. Broglio. (1995).",
                "TREC and TIPSTER experiments with INQUERY.",
                "Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002).",
                "Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000).",
                "Methods for distributed information retrieval.",
                "Ph.",
                "D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).",
                "Merging results from isolated search engines.",
                "In Proceedings of 10th Australasian Database Conference. [6] D. DSouza, J. Thom, and J. Zobel. (2000).",
                "A comparison of techniques for selecting text collections.",
                "In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999).",
                "A Decision-Theoretic approach to database selection in networked IR.",
                "ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997).",
                "STARTS: Stanford proposal for internet metasearching.",
                "In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003).",
                "QProber: A System for Automatic Classification of Hidden-Web Databases.",
                "ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002).",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003).",
                "Content-based information retrieval in peer-to-peer networks.",
                "In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T.",
                "Yu and K.L.",
                "Liu. (2002) Building efficient and effective metasearch engines.",
                "ACM Comput.",
                "Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003).",
                "Evaluating different method of estimating retrieval quality for resource selection.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003).",
                "The MIND architecture for heterogeneous multimedia federated digital libraries.",
                "ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L.",
                "Powell, J.C. French, J. Callan, M. Connell, and C.L.",
                "Viles. (2000).",
                "The impact of database selection on distributed searching.",
                "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [18] A.L.",
                "Powell and J.C. French. (2003).",
                "Comparing the performance of database selection algorithms.",
                "ACM Transactions on Information Systems, 21(4). (pp. 412-456). [19] C. Sherman (2001).",
                "Search for the invisible web.",
                "Guardian Unlimited. [20] L. Si and J. Callan. (2002).",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [21] L. Si and J. Callan. (2003).",
                "Relevant document distribution estimation method for resource selection.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [22] L. Si and J. Callan. (2003).",
                "A Semi-Supervised learning method to merge search engine results.",
                "ACM Transactions on Information Systems, 21(4). (pp. 457-491). 41"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}